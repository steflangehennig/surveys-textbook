[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Survey Methods for Social Scientists",
    "section": "",
    "text": "Welcome\nThis is the github repository for the work-in-progress first edition of Survey Methods for Social Scientists.\nThe goal of this book is to teach you how to create, conduct, and analyze surveys with R. While the book is primarily geared towards undergraduate students in the social sciences (political science, psychology, sociology, economics, etc.), the practical uses for this book can be extended to any discipline.\nThe book is divided into three sections:\n\nThe Science Behind Survey Methods\nBest Practices of Survey Design\nConducting Survey Analyses\n\nEach section comprises multiple chapters that succinctly explain the why behind the method, as well as how to apply the method. Built into each chapter are comprehensive R code tutorials alongside practical explanations."
  },
  {
    "objectID": "ch01intro.html",
    "href": "ch01intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "This book is geared toward undergraduates learning survey methodologies. This includes, but is not limited to, the psychology of the survey response process; creating and disseminating survey questions; survey modes; sampling and weighting; ethics; and survey analysis and reporting."
  },
  {
    "objectID": "ch01intro.html#learning-objectives",
    "href": "ch01intro.html#learning-objectives",
    "title": "1  Introduction",
    "section": "1.1 Learning Objectives",
    "text": "1.1 Learning Objectives"
  },
  {
    "objectID": "ch02psych_response.html",
    "href": "ch02psych_response.html",
    "title": "3  Psychology of Survey Response",
    "section": "",
    "text": "4 Psychology of Survey Response"
  },
  {
    "objectID": "ch02psych_response.html#learning-objectives",
    "href": "ch02psych_response.html#learning-objectives",
    "title": "2  Psychology of Survey Response Response",
    "section": "2.1 Learning Objectives",
    "text": "2.1 Learning Objectives"
  },
  {
    "objectID": "ch07weights.html",
    "href": "ch07weights.html",
    "title": "7  Survey Weighting",
    "section": "",
    "text": "Note: This tutorial uses the anesrake package to calculate the survey weights. They are many other packages to calculate weights so this is just one possible approach that could successfully be used to create survey weights.\nSurvey weights are widely used in survey research for a variety of purposes. In this tutorial, we will be focusing on one specific form of survey weights called a “rake weight”. Rake weights are typically used to make the survey sample match the target population on a set of demographic, and sometimes attitudinal, measures. They are used to ensure the sample’s demographics match the target population’s demographics. This numerical correction will change how much each individual case in your dataset is contributing to the overall, or sub-group, mean values across your sample data.\nFirst, we load necessary packages to compute and analyze the weights. If a package is not installed on your machine, you must first install it before this chunk of code will run."
  },
  {
    "objectID": "ch07weights.html#import-your-survey-data-into-r",
    "href": "ch07weights.html#import-your-survey-data-into-r",
    "title": "8  Survey Weighting",
    "section": "8.2 Import your survey data into R",
    "text": "8.2 Import your survey data into R\nWe need to import our survey data into R. The way we do this will vary by the format of your data, but in this case the data is saved as a “.dta” file so we will use the haven package to import it.\nYou should always examine your data and the base R function head shows the first 5 cases along with all of your column labels.\n\n\n [1] \"caseid\"         \"pid_4\"          \"ideo5\"          \"gov_choice\"    \n [5] \"prop_111\"       \"prop_112\"       \"trump_app\"      \"hick_app\"      \n [9] \"gardner_app\"    \"cong_app\"       \"scotus_app\"     \"pot_law\"       \n[13] \"gambling\"       \"fracking\"       \"gun_control\"    \"anger\"         \n[17] \"pride\"          \"hope\"           \"disgust\"        \"worry\"         \n[21] \"trump_app2\"     \"hick_app2\"      \"gardner_app2\"   \"cong_app2\"     \n[25] \"scotus_app2\"    \"pot_law2\"       \"gambling2\"      \"fracking2\"     \n[29] \"gun_control2\"   \"old_weight_old\" \"pid_x\"          \"sex\"           \n[33] \"race_4\"         \"speakspanish\"   \"marstat\"        \"child18\"       \n[37] \"employ\"         \"faminc_new\"     \"casscd\"         \"religiosity\"   \n[41] \"age_group\"      \"educ\""
  },
  {
    "objectID": "ch07weights.html#save-your-target-population-demographic-parameters",
    "href": "ch07weights.html#save-your-target-population-demographic-parameters",
    "title": "8  Survey Weighting",
    "section": "8.3 Save Your Target Population Demographic Parameters",
    "text": "8.3 Save Your Target Population Demographic Parameters\nYou will need to know the target population proportion for each of the variables you wish to weight your sample data on. How easy it will be to find your population values will be based on your specific target population.\nSome populations will be relatively easy to find (e.g. think adult demographic proportions in the United States from the Census, CPS, or ACS results and all the sub-geographic levels that accompany them), but others won’t be as easy. Sometimes, you cannot know your target population proportions so in those cases you will not be able to weight your survey sample data.\nIn this chunk of R code, we are creating the target population parameters for two specific demographic variables measured in our sample political poll data. This was a political poll conducted in October 2018 with the sample consisting of likely Colorado voters in the then upcoming 2018 election. This gubernatorial election year poll measured multiple things including: - 2018 Colorado Gubernatorial Preference - Jared Polis or Walker Stapleton - Policy Questions: Marijuana Legalization, Fracking, Gun Control laws - Approval ratings: President Donald Trump, Governor John Hickenlooper (at the time), US Congress - Demographic questions for survey weighting purposes\n\n8.3.1 Saving New Vectors With Target Population Demographic Values\nUsing this data, let’s create some survey weights. To illustrate the principle, we will start with basic weights using just two demographic variables commonly used in calculating survey weights: sex (unfortunately only biological sex was collected in this survey) and age (split into 5 categories). We must save a vector of data with the target population demographic proportions, so in this case we will save two vectors one called sex and one called age_group.\nThere are two critical things to get correct in this step.\n1. Matching Names The names we give these vectors matter and must match the names of the appropriate demographic variable in your sample data. Since the vector names we chose were sex and age_group, the variable names in the sample data must be exactly sex and age_group. Otherwise, the code will not be able to match the two and will fail.\n2. Matching Orders The second critical thing to get correct is the order the proportion values are entered into the vector must match the order the proportion values are stored in the sample data. In this example, the order of proportions stored in the sex variable in the sample data is (female, male) so the values we give the sex vector must be in that exact order as well. The same is true for the age_group variable, which has 5 groups in the sample data: 18-29, 30-39, 40-49, 50-64, 65+. The proportion order in our vector for the age_group must match that exactly as well otherwise you are creating incorrect weights or best-case scenario getting an error message.\n\nsex &lt;- c(.525, .475)  #Target values for females and males; label order (female, male)\nsum(sex) #proportions should = 1 so this checks that it does\n\n[1] 1\n\nage_group  &lt;- c(.182, .203, .17, .218, .227)  #Target values for 5 age groups; 18-29, 30-39, 40-49, 50-64, 65+\nsum(age_group) #proportions should = 1 so this checks that it does\n\n[1] 1\n\n\nFirst, let’s look at the unweighted values in both the age and sex variables.\n\n#Shows the unweighted size of the age_group demographics\nsample %&gt;%\n  group_by(age_group) %&gt;%\n  summarise(n = n()) %&gt;% \n  mutate(proportion = n /sum(n))\n\n# A tibble: 5 × 3\n  age_group     n proportion\n  &lt;dbl+lbl&gt; &lt;int&gt;      &lt;dbl&gt;\n1 1 [18-29]    33     0.0412\n2 2 [30-39]   101     0.126 \n3 3 [40-49]   118     0.148 \n4 4 [50-64]   306     0.382 \n5 5 [65+]     242     0.302 \n\n#Shows the unweighted size of the sex demographics\nsample %&gt;%\n  group_by(sex) %&gt;%\n  summarise(n = n()) %&gt;% \n  mutate(proportion = n /sum(n))\n\n# A tibble: 2 × 3\n  sex            n proportion\n  &lt;dbl+lbl&gt;  &lt;int&gt;      &lt;dbl&gt;\n1 1 [Female]   423      0.529\n2 2 [Male]     377      0.471"
  },
  {
    "objectID": "ch07weights.html#calculating-your-rake-weights",
    "href": "ch07weights.html#calculating-your-rake-weights",
    "title": "8  Survey Weighting",
    "section": "8.4 Calculating Your Rake Weights",
    "text": "8.4 Calculating Your Rake Weights\nNow that we have our target population parameters saved in vectors for each demographic variable we plan to weight our survey data on and our sample data with matching names and orders, we can begin to create our survey weights.\n1. Create List First, we create a list that merges the two demographic vectors for use in the raking process. Remember, this names must match the column names in the sample data. We give this list the name of targets to reflect this is the target population parameters we want to match the sample data to. We then give the column names to match with the sample data.\n2. Calculate the Weights Now, it is time to create some survey weights using the anesrake function. This function has many possible items that could be used, with all the possible items listed in the following R chunk. You should view the R documentation for all possible things it can do.\nFor our purposes, we will be focusing on a few things that will be noted. We will calculate a new dataframe called myweights where we input the targets list, the name of our sample data cpc, a caseid value that uniquely identifies each case, the cap item tells the function to cape the size of the survey weights at 8 and not allow any case to have a weight larger than that value. The type item tells the function how it should handle, if at all, a target population demographic that is very close to the sample value for that same demographic.\nYou’ll see in the output once you run the anesrake function how many iterations it took for the raking to converge on this specific set of weights. Here, it took 15 iterations across the two target demographic variables.\n3. Save Weights in Sample Data Next, we save that newly created weight as a new variable in our existing sample data, and now we have a weight variable that we can use in our analysis of the data.\n\n#Now we save these values as a list and call the list targets\n#Step 1: Save the target list \ntargets &lt;- list(sex, age_group)\n# remember, these names will have to match\nnames(targets) &lt;- c(\"sex\", \"age_group\")\n\n#anesrake(targets, dataframe, caseid, weightvec = NULL, cap = 5,\n#verbose = FALSE, maxit = 1000, type = \"pctlim\", pctlim = 5,\n#nlim = 5, filter = 1, choosemethod = \"total\", iterate = TRUE)\n\n#Step 2 - Calculate the Rake Weight\nset.seed(1599385) #Set the seed for replication  \nmyweights &lt;- anesrake(targets, sample, \n                      caseid = sample$caseid, cap = 8, type = \"nolim\", pctlim=.05)\n\n[1] \"Raking converged in 12 iterations\"\n\n#Step 3 - Save the Rake Weight at the end of your sample data\nsample$weight  &lt;- unlist(myweights[1])"
  },
  {
    "objectID": "ch07weights.html#reviewing-the-newly-created-survey-weights",
    "href": "ch07weights.html#reviewing-the-newly-created-survey-weights",
    "title": "8  Survey Weighting",
    "section": "8.5 Reviewing the Newly Created Survey Weights",
    "text": "8.5 Reviewing the Newly Created Survey Weights\nBefore we start the analysis of the weighted data, let’s examine the newly created survey weights saved in our sample data.\nWith only 2 target weighting variables with 10 total categories combined between them, we can examine the weights individually by group. To do this, we will use the srvyr package to examine the weight size by the target groups.\n\n#Displays summary of the weight size to see range\nsummary(sample$weight)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.5347  0.6086  0.6997  1.0000  1.0912  4.8008 \n\n#Shows the weight size by demographic groups used in the weighting scheme\nsample %&gt;% \n  as_survey(weights = c(weight)) %&gt;%\n  group_by(sex, age_group) %&gt;% \n  summarise(weight = survey_mean(weight, na.rm = T))\n\n# A tibble: 10 × 4\n# Groups:   sex [2]\n   sex        age_group weight weight_se\n   &lt;dbl+lbl&gt;  &lt;dbl+lbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;\n 1 1 [Female] 1 [18-29]  4.22   0       \n 2 1 [Female] 2 [30-39]  1.51   2.86e-17\n 3 1 [Female] 3 [40-49]  1.09   0       \n 4 1 [Female] 4 [50-64]  0.535  0       \n 5 1 [Female] 5 [65+]    0.700  0       \n 6 2 [Male]   1 [18-29]  4.80   0       \n 7 2 [Male]   2 [30-39]  1.72   3.22e-17\n 8 2 [Male]   3 [40-49]  1.24   0       \n 9 2 [Male]   4 [50-64]  0.609  0       \n10 2 [Male]   5 [65+]    0.796  0       \n\n\nNow we see the weight size for each of the 10 groups that we weighted our sample data on. Obviously, with more demographic variables including in the weighting scheme this list would get much more cumbersome but for pedagogical purposes it is important to look at these values to understand their meaning.\nFor females between the age of 18-29 the weight equals 4.21. This means that females between the ages of 18-29 are under-represented in the sample data since the value is over 1. Fundamentally what this means is that for each female between the ages of 18-29 in the sample data, they are “speaking” for 4.21 females between the age of 18-29 from the target population. Compare this value to females between the ages of 50 and 64 (group 4) who have a weight value of .53. This means that females in this age group were over-represented in the sample data since the weight value is under 1.\nWe should also look at the weighted demographic values to ensure the weights worked as we hope they do - i.e. the weighted sample demographic values match the target population values.\n\n#Shows the unweighted size of the age_group demographics\nsample %&gt;%\n  group_by(age_group) %&gt;%\n  summarise(n = n()) %&gt;% \n  mutate(proportion = n /sum(n))\n\n# A tibble: 5 × 3\n  age_group     n proportion\n  &lt;dbl+lbl&gt; &lt;int&gt;      &lt;dbl&gt;\n1 1 [18-29]    33     0.0412\n2 2 [30-39]   101     0.126 \n3 3 [40-49]   118     0.148 \n4 4 [50-64]   306     0.382 \n5 5 [65+]     242     0.302 \n\n#Shows the weighted size of the age_group demographics\nsample %&gt;%\n  as_survey(weights = c(weight)) %&gt;%\n  group_by(age_group) %&gt;%\n  summarise(n = survey_total()) %&gt;% \n  mutate(proportion = n /sum(n))\n\n# A tibble: 5 × 4\n  age_group     n  n_se proportion\n  &lt;dbl+lbl&gt; &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt;\n1 1 [18-29]  146. 24.9       0.182\n2 2 [30-39]  162. 15.2       0.203\n3 3 [40-49]  136  11.6       0.17 \n4 4 [50-64]  174.  7.87      0.218\n5 5 [65+]    182.  9.78      0.227\n\n#Saves the weighted & unweighted size of the age_group demographics\nag_w&lt;-sample %&gt;%\n  as_survey(weights = c(weight)) %&gt;%\n  group_by(age_group) %&gt;%\n  summarise(n = survey_total()) %&gt;% \n  mutate(weighted_sample = n /sum(n))\n\nag_uw&lt;- sample %&gt;%\n  group_by(age_group) %&gt;%\n  summarise(n = n()) %&gt;% \n  mutate(unweighted_sample = n /sum(n))\nag_combo&lt;-left_join(ag_w, ag_uw, by = \"age_group\", suffix = c(\"\", \"_pop\")) %&gt;%\n  group_by(age_group)\n\nag_combo$ag_diff_per&lt;- 100*(ag_combo$weighted_sample-ag_combo$unweighted_sample)\nag_combo\n\n# A tibble: 5 × 7\n# Groups:   age_group [5]\n  age_group     n  n_se weighted_sample n_pop unweighted_sample ag_diff_per\n  &lt;dbl+lbl&gt; &lt;dbl&gt; &lt;dbl&gt;           &lt;dbl&gt; &lt;int&gt;             &lt;dbl&gt;       &lt;dbl&gt;\n1 1 [18-29]  146. 24.9            0.182    33            0.0412       14.1 \n2 2 [30-39]  162. 15.2            0.203   101            0.126         7.68\n3 3 [40-49]  136  11.6            0.17    118            0.148         2.25\n4 4 [50-64]  174.  7.87           0.218   306            0.382       -16.4 \n5 5 [65+]    182.  9.78           0.227   242            0.302        -7.55\n\nprint(ag_combo$weighted_sample)\n\n[1] 0.182 0.203 0.170 0.218 0.227\n\nprint(targets)\n\n$sex\n[1] 0.525 0.475\n\n$age_group\n[1] 0.182 0.203 0.170 0.218 0.227\n\n\nWe see that the weighted age_group values match the target population values we inputted earlier so this weighting scheme seems to be working in the way that we hoped it would.\n\n#Shows the unweighted size of the sex demographics\nsample %&gt;%\n  group_by(sex) %&gt;%\n  summarise(n = n()) %&gt;% \n  mutate(proportion = n /sum(n))\n\n# A tibble: 2 × 3\n  sex            n proportion\n  &lt;dbl+lbl&gt;  &lt;int&gt;      &lt;dbl&gt;\n1 1 [Female]   423      0.529\n2 2 [Male]     377      0.471\n\n#Shows the weighted size of the sex demographics\nsample %&gt;%\n  as_survey(weights = c(weight)) %&gt;%\n  group_by(sex) %&gt;%\n  summarise(n = survey_total()) %&gt;% \n  mutate(proportion = n /sum(n))\n\n# A tibble: 2 × 4\n  sex            n  n_se proportion\n  &lt;dbl+lbl&gt;  &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt;\n1 1 [Female]   420  22.0      0.525\n2 2 [Male]     380  20.4      0.475\n\n\nFor the sex measure, the unweighted and weighted values matched nearly identically, but that is because the unweighted sample nearly matched the target population identically without any statistical correction. In these instances, it is common to drop the demographic variable that does not need much adjustment. The standard limit is 5% or less should not get an adjustment meaning that you should only apply weights with the target population values and the sample values are 5% points or more different."
  },
  {
    "objectID": "ch07weights.html#evaluating-influence-of-weights-on-reported-mean-values-in-the-sample-data---smallish-weights",
    "href": "ch07weights.html#evaluating-influence-of-weights-on-reported-mean-values-in-the-sample-data---smallish-weights",
    "title": "8  Survey Weighting",
    "section": "8.6 Evaluating Influence of Weights on Reported Mean Values in the Sample Data - Smallish Weights",
    "text": "8.6 Evaluating Influence of Weights on Reported Mean Values in the Sample Data - Smallish Weights\nNow let’s see what impact these weights have on our sample values. First, we use the surveys package must create a new dataframe that incorporates the survey weights. Let’s call it sample.weighted to signal that this is the weighted version of the sample data.\nWe need to calculate the weighted and unweighted means of the same variable. Using the fracking2 variable which measures support for fracking in the Colorado which various safety measures, we can compare the influence of the survey weights on the conclusions we would draw about support for fracking in Colorado.\nOnce we run the following R chunk, we see that there is virtually no difference between the weighted and unweighted estimates of how supportive Coloradoans are of fracking. Why is this? This occurs sometimes when the weights that applied simply do not change the sample composition enough to have an influence on the overall sample mean.\n\nfracking_uw&lt;-sample %&gt;% #Looks at the unweighted support for fracking in CO\n  summarise(unweight_support = mean(fracking2, na.rm = T))\n\nfracking_w&lt;-sample %&gt;% #Looks at the weighted support for fracking in CO\n  as_survey(weights = c(weight)) %&gt;%\n   summarise(weight_support = survey_mean(fracking2, na.rm = T))\n\n\nfracking_combo&lt;-cbind(fracking_uw, fracking_w ) \n\nfracking_combo &lt;- mutate(fracking_combo, difference = weight_support - unweight_support)\nfracking_combo\n\n  unweight_support weight_support weight_support_se  difference\n1        0.5277778      0.5025514         0.0229558 -0.02522642\n\n#Gubernatorial Vote Choice - Weighted & Unweighted \ngov_w&lt;-sample %&gt;%\n  as_survey(weights = c(weight)) %&gt;%\n  filter(!is.na(gov_choice)) %&gt;% \n  group_by(gov_choice) %&gt;%\n  summarise(n = survey_total()) %&gt;% \n  mutate(weight_support = n /sum(n)) \n\ngov_uw&lt;-sample %&gt;%\n  group_by(gov_choice) %&gt;%\n  filter(!is.na(gov_choice)) %&gt;% \n  summarise(n = n()) %&gt;% \n  mutate(unweight_support = n /sum(n))\n\ngov_combo&lt;-cbind(gov_uw, gov_w) \n\n\ngov_combo$diff &lt;- gov_combo$weight_support - gov_combo$unweight_support\ngov_combo\n\n  gov_choice   n unweight_support gov_choice         n      n_se weight_support\n1          1 413          0.51625          1 438.30863 22.858613     0.54788579\n2          2 346          0.43250          2 312.66152 17.843921     0.39082691\n3          3  30          0.03750          3  31.10659  6.029889     0.03888324\n4          4  11          0.01375          4  17.92325  7.817047     0.02240406\n          diff\n1  0.031635792\n2 -0.041673095\n3  0.001383243\n4  0.008654059"
  },
  {
    "objectID": "ch07weights.html#create-new-weighting-scheme-that-incorporates-more-demographic-variables",
    "href": "ch07weights.html#create-new-weighting-scheme-that-incorporates-more-demographic-variables",
    "title": "8  Survey Weighting",
    "section": "8.7 Create New Weighting Scheme That Incorporates More Demographic Variables",
    "text": "8.7 Create New Weighting Scheme That Incorporates More Demographic Variables\nTypically, when creating survey weights you will include more than just 2 demographic variables into your weighting scheme. Here, we use 5 variables to create a new weight: sex, age, race/ethnicity, education, and partisanship.\n\n#Save new vectors with target population values for weights \nsex &lt;- c(.525, .475)  ##Target values for females and males; label order (female, male)\nage_group  &lt;- c(.132, .183, .15, .248, .287)   #Target values for 5 age groups \nrace_4 &lt;-c(.7143, .0501, .1768, .0588) #Target values race/ethnic identities - white, black, Hispanic, all others\neduc &lt;-c(.2075, .2445, .0828, .2398, .2254) #Target values education - HS or less, Some college, AA, BA, Graduate degree\npid_4 &lt;-c(.3375, .2838, .335, .0437) #Target values Party Registration - (Democrats, Independents, Republicans, All 3rd Parties)  \n\n#Combine the demographic vectors into a list\ntargets &lt;- list(sex, age_group, race_4, educ, pid_4)\n# remember, these names will have to match the column names & order in the sample data \nnames(targets) &lt;- c(\"sex\", \"age_group\", \"race_4\", \"educ\", \"pid_4\")\n\nset.seed(1984)\nmyweights &lt;- anesrake(targets, sample, \n                      caseid = sample$caseid, cap = 8, type = \"pctlim\", pctlim=.05)    \n\n[1] \"Raking converged in 20 iterations\"\n[1] \"Raking converged in 21 iterations\"\n\nsample$full_weight  &lt;- unlist(myweights[1])\n\nsummary(sample$full_weight)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.3218  0.5685  0.7672  1.0000  1.0729  7.9999 \n\n\nLet’s look at how well the weights worked to match the sample data to the target population values for the race4 and educ5 measures.\n\n#Shows the weighted size of the educ5 demographics\n##We can also bind the two values together to calculate their differences \ned_w&lt;-sample %&gt;%\n  as_survey(weights = c(full_weight)) %&gt;%\n  group_by(educ) %&gt;%\n  summarise(n = survey_total()) %&gt;% \n  mutate(weighted_sample = n /sum(n))\n\ned_uw&lt;- sample %&gt;%\n  group_by(educ) %&gt;%\n  summarise(n = n()) %&gt;% \n  mutate(unweighted_sample = n /sum(n))\ned_combo&lt;-left_join(ed_w, ed_uw, by = \"educ\") %&gt;%\n  group_by(educ)\n\ned_combo$ed_diff_per&lt;- 100*(ed_combo$weighted_sample-ed_combo$unweighted_sample)\ned_combo\n\n# A tibble: 5 × 7\n# Groups:   educ [5]\n  educ             n.x  n_se weighted_sample   n.y unweighted_sample ed_diff_per\n  &lt;dbl+lbl&gt;      &lt;dbl&gt; &lt;dbl&gt;           &lt;dbl&gt; &lt;int&gt;             &lt;dbl&gt;       &lt;dbl&gt;\n1 1 [HS grad or… 166.  22.1           0.208     92             0.115       9.25 \n2 2 [Some colle… 196.  17.2           0.244    187             0.234       1.07 \n3 3 [AA degree]   66.2  7.29          0.0828    96             0.12       -3.72 \n4 4 [BA]         192.  14.3           0.240    249             0.311      -7.15 \n5 5 [Post-grad]  180.  14.3           0.225    176             0.22        0.540\n\n#Shows the weighted size of the race4 demographics\n##We can also bind the two values together to calculate their differences \nr_w&lt;-sample %&gt;%\n  as_survey(weights = c(full_weight)) %&gt;%\n  group_by(race_4) %&gt;%\n  summarise(n = survey_total()) %&gt;% \n  mutate(weighted_sample = n /sum(n))\n\nr_uw&lt;- sample %&gt;%\n  group_by(race_4) %&gt;%\n  summarise(n = n()) %&gt;% \n  mutate(unweighted_sample = n /sum(n))\nr_combo&lt;-left_join(r_w, r_uw, by = \"race_4\") %&gt;%\n  group_by(race_4)\n\nr_combo$r_diff_per&lt;- 100*(r_combo$weighted_sample-r_combo$unweighted_sample)\nr_combo\n\n# A tibble: 4 × 7\n# Groups:   race_4 [4]\n  race_4          n.x  n_se weighted_sample   n.y unweighted_sample r_diff_per\n  &lt;dbl+lbl&gt;     &lt;dbl&gt; &lt;dbl&gt;           &lt;dbl&gt; &lt;int&gt;             &lt;dbl&gt;      &lt;dbl&gt;\n1 1 [White]     571.  14.0           0.714    692            0.865     -15.1  \n2 2 [Black]      40.1  9.97          0.0501    22            0.0275      2.26 \n3 3 [Hispanic]  141.  25.1           0.177     41            0.0512     12.6  \n4 4 [Other POC]  47.0  7.56          0.0588    45            0.0562      0.255\n\n#Shows the weighted size of the pid_4 registered voter variable \n##We can also bind the two values together to calculate their differences \npid_w&lt;-sample %&gt;%\n  as_survey(weights = c(full_weight)) %&gt;%\n  group_by(pid_4) %&gt;%\n  summarise(n = survey_total()) %&gt;% \n  mutate(weighted_sample = n /sum(n))\n\npid_uw&lt;- sample %&gt;%\n  group_by(pid_4) %&gt;%\n  summarise(n = n()) %&gt;% \n  mutate(unweighted_sample = n /sum(n))\npid_combo&lt;-left_join(pid_w, pid_uw, by = \"pid_4\") %&gt;%\n  group_by(pid_4)\n\npid_combo$pid_diff_per&lt;- 100*(pid_combo$weighted_sample-pid_combo$unweighted_sample)\npid_combo\n\n# A tibble: 4 × 7\n# Groups:   pid_4 [4]\n  pid_4           n.x  n_se weighted_sample   n.y unweighted_sample pid_diff_per\n  &lt;dbl+lbl&gt;     &lt;dbl&gt; &lt;dbl&gt;           &lt;dbl&gt; &lt;int&gt;             &lt;dbl&gt;        &lt;dbl&gt;\n1 1 [Democrat]  270.  21.1           0.338    283            0.354        -1.62 \n2 2 [Independe… 227.  17.2           0.284    214            0.268         1.63 \n3 3 [Republica… 268.  19.8           0.335    266            0.332         0.250\n4 4 [Other]      35.0  6.55          0.0437    37            0.0462       -0.255\n\n\n\n#Gubernatorial Vote Choice - Weighted & Unweighted \ngov_w&lt;-sample %&gt;%\n  as_survey(weights = c(full_weight)) %&gt;%\n  filter(!is.na(gov_choice)) %&gt;% \n  group_by(gov_choice) %&gt;%\n  summarise(n = survey_total()) %&gt;% \n  mutate(weight_support = n /sum(n)) \n\ngov_uw&lt;-sample %&gt;%\n  group_by(gov_choice) %&gt;%\n  filter(!is.na(gov_choice)) %&gt;% \n  summarise(n = n()) %&gt;% \n  mutate(unweight_support = n /sum(n))\n\ngov_combo&lt;-cbind(gov_uw, gov_w) \n\n\ngov_combo$diff &lt;- gov_combo$weight_support - gov_combo$unweight_support\n\ngov_outcome&lt;-cbind(gov_combo$gov_choice, gov_combo$weight_support, gov_combo$unweight_support, gov_combo$diff) \n\ncolnames(gov_outcome) &lt;- c(\"candidate\", \"weighted support\", \"unewighted support\", \"diff\") \ngov_outcome\n\n     candidate weighted support unewighted support         diff\n[1,]         1       0.50298793            0.51625 -0.013262068\n[2,]         2       0.43459577            0.43250  0.002095774\n[3,]         3       0.04328807            0.03750  0.005788066\n[4,]         4       0.01912823            0.01375  0.005378227\n\n####Comparing weighted to unweighted fracking support in Colorado \n\nfracking_uw&lt;-sample %&gt;% #Looks at the unweighted support for fracking in CO\n  summarise(unweight_support = mean(fracking2, na.rm = T))\n\nfracking_w&lt;-sample %&gt;% #Looks at the weighted support for fracking in CO\n  as_survey(weights = c(full_weight)) %&gt;%\n   summarise(weight_support = survey_mean(fracking2, na.rm = T))\n\n\nfracking_combo&lt;-cbind(fracking_uw, fracking_w ) \n\nfracking_combo &lt;- mutate(fracking_combo, frack_diff = weight_support - unweight_support)\n\n#Do the same analysis for gun control then pot laws\ngc_uw&lt;-sample %&gt;% #Looks at the unweighted support for fracking in CO\n  summarise(unweight_support = mean(gun_control2, na.rm = T))\n\ngc_w&lt;-sample %&gt;% #Looks at the weighted support for fracking in CO\n  as_survey(weights = c(full_weight)) %&gt;%\n   summarise(weight_support = survey_mean(gun_control2, na.rm = T))\n\n\ngc_combo&lt;-cbind(gc_uw, gc_w ) \n\ngc_combo &lt;- mutate(gc_combo, gun_control_diff = weight_support - unweight_support)\n\npl_uw&lt;-sample %&gt;% #Looks at the unweighted support for fracking in CO\n  summarise(unweight_support = mean(pot_law2, na.rm = T))\n\npl_w&lt;-sample %&gt;% #Looks at the weighted support for fracking in CO\n  as_survey(weights = c(full_weight)) %&gt;%\n   summarise(weight_support = survey_mean(pot_law2, na.rm = T))\n\n\npl_combo&lt;-cbind(pl_uw, pl_w ) \n\npl_combo &lt;- mutate(pl_combo, pot_law_diff = weight_support - unweight_support)\n\n\ncombo&lt;-cbind(100*(fracking_combo$frack_diff),100*(gc_combo$gun_control_diff), 100*(pl_combo$pot_law_diff)) \n\ncolnames(combo) &lt;- c(\"fracking_diff\", \"gun_control_diff\", \"pot_law_diff\") \ncombo&lt;-round(combo,3)\ncombo\n\n     fracking_diff gun_control_diff pot_law_diff\n[1,]         2.651            0.531       -0.303\n\n\nAbove, shows the impact that the new weighting scheme had on the differences in support for fracking, new gun control policies, support for marijuana legalization and a tax revenue proposition on the upcoming ballot. For fracking, the weighted sample supported fracking 3.3 percentage points higher than the unweighted sample. While that might not seem like a large difference, in a polarized American electorate 3.3 percentage points easily be the differences between winning an election or going down in defeat.\nThe other three items all saw a decrease in support in the weighted data. Overall, the results showed increased support for a generally conservative supported issue, fracking, while revealing decreased support for 3 more liberal supported issues, gun control, marijuana legalization, and increased governmental spending. This is likely caused by weighting older voters to be\nWe can also use the newly created survey weights in regression analyses. To do so, you first must create a new weighted survey dataset that you then conduct the analysis on.\n\n#Using srvyr package to create a new weighted dataset for analysis purposes\n#Step 1: Create Weighted Survey Data for Analysis\nsample.weighted &lt;- sample %&gt;% \n  as_survey_design(ids = 1, # 1 for no cluster ids; use this for a simple random sample \n                   weights = full_weight, # No weight added\n                   strata = NULL # sampling was simple (no strata) \n                  )\n\nnonweighted &lt;-lm(gambling ~ pid_x + ideo5 + sex , data=sample)\nweighted &lt;-lm(gambling ~ pid_x + ideo5 + sex, data=sample, weights=sample$weight)\nweighted2 &lt;-lm(gambling ~ pid_x + ideo5 + sex, data=sample, weights=sample$full_weight)\n\nstargazer(nonweighted, weighted,  weighted2, type=\"text\")\n\n\n============================================================\n                                    Dependent variable:     \n                               -----------------------------\n                                         gambling           \n                                  (1)       (2)       (3)   \n------------------------------------------------------------\npid_x                           -0.017    -0.010    -0.013  \n                                (0.031)   (0.032)   (0.029) \n                                                            \nideo5                           -0.059    -0.065   -0.096** \n                                (0.051)   (0.051)   (0.047) \n                                                            \nsex                             0.184**  0.215***   0.188** \n                                (0.080)   (0.080)   (0.079) \n                                                            \nConstant                       2.391***  2.441***  2.591*** \n                                (0.153)   (0.154)   (0.157) \n                                                            \n------------------------------------------------------------\nObservations                      669       669       669   \nR2                               0.015     0.018     0.025  \nAdjusted R2                      0.011     0.013     0.021  \nResidual Std. Error (df = 665)   1.030     1.020     0.998  \nF Statistic (df = 3; 665)       3.481**  4.032***  5.678*** \n============================================================\nNote:                            *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01\n\n\nResults are largely similar across the regression models, but there are some slight differences between the unweighted model and the two weighted ones. This happens because certain respondent’s opinions are being given more or less weight to the overall average relationship which can cause the conclusions you draw from your analysis to differ. This is one reason why it is critically important to create your survey weights using defensible target population values."
  },
  {
    "objectID": "ch07weights.html#concluding-thoughts",
    "href": "ch07weights.html#concluding-thoughts",
    "title": "8  Survey Weighting",
    "section": "8.8 Concluding Thoughts",
    "text": "8.8 Concluding Thoughts\nThis is an important lesson for the application of the survey weights. The target population values that you weight your survey sample data to match can have profound implications on the conclusions you and others draw from your survey results. In the case, the decision to weight the survey to give more voice to the Republican members of the sample influenced the conclusions drawn about support for various policies being debated in the public realm. This makes it critically important to make sure that the target population values that are chosen are as accurate as possible and publicly defensible.\nOverall, this tutorial has taken you through how to calculate survey weights using the anesrake package. Using a sample political poll, you hopefully learned how to create target demographic population vectors, which then merge with our sample demographic values. Following this, you learned how to calculate directly survey weights, evaluate the success/failure of the survey weighting process, and compare the impact of using the survey weight on the conclusions drawn from the results.\n\nsessionInfo()\n\nR version 4.3.0 (2023-04-21 ucrt)\nPlatform: x86_64-w64-mingw32/x64 (64-bit)\nRunning under: Windows 10 x64 (build 19044)\n\nMatrix products: default\n\n\nlocale:\n[1] LC_COLLATE=English_United Kingdom.utf8 \n[2] LC_CTYPE=English_United Kingdom.utf8   \n[3] LC_MONETARY=English_United Kingdom.utf8\n[4] LC_NUMERIC=C                           \n[5] LC_TIME=English_United Kingdom.utf8    \n\ntime zone: America/Denver\ntzcode source: internal\n\nattached base packages:\n[1] grid      stats     graphics  grDevices utils     datasets  methods  \n[8] base     \n\nother attached packages:\n [1] stargazer_5.2.3   data.table_1.14.8 anesrake_0.80     weights_1.0.4    \n [5] Hmisc_5.1-0       survey_4.2-1      survival_3.5-5    Matrix_1.5-4     \n [9] srvyr_1.2.0       poliscidata_2.3.0 skimr_2.1.5       lubridate_1.9.2  \n[13] forcats_1.0.0     stringr_1.5.0     purrr_1.0.1       readr_2.1.4      \n[17] tibble_3.2.1      tidyverse_2.0.0   knitr_1.43        tidyr_1.3.0      \n[21] dplyr_1.1.2       ggplot2_3.4.2     haven_2.5.2      \n\nloaded via a namespace (and not attached):\n [1] tidyselect_1.2.0   bitops_1.0-7       fastmap_1.1.1      digest_0.6.32     \n [5] rpart_4.1.19       timechange_0.2.0   lifecycle_1.0.3    cluster_2.1.4     \n [9] gdata_2.19.0       magrittr_2.0.3     compiler_4.3.0     rlang_1.1.1       \n[13] tools_4.3.0        plotrix_3.8-2      utf8_1.2.3         htmlwidgets_1.6.2 \n[17] plyr_1.8.8         repr_1.1.6         abind_1.4-5        KernSmooth_2.23-20\n[21] withr_2.5.0        foreign_0.8-84     nnet_7.3-18        fansi_1.0.4       \n[25] jomo_2.7-6         caTools_1.18.2     xtable_1.8-4       colorspace_2.1-0  \n[29] mice_3.16.0        MASS_7.3-58.4      scales_1.2.1       gtools_3.9.4      \n[33] iterators_1.0.14   cli_3.6.1          crayon_1.5.2       rmarkdown_2.23    \n[37] generics_0.1.3     rstudioapi_0.14    tzdb_0.4.0         minqa_1.2.5       \n[41] DBI_1.1.3          splines_4.3.0      base64enc_0.1-3    mitools_2.4       \n[45] vctrs_0.6.3        boot_1.3-28.1      glmnet_4.1-7       jsonlite_1.8.5    \n[49] carData_3.0-5      car_3.1-2          hms_1.1.3          mitml_0.4-5       \n[53] Formula_1.2-5      htmlTable_2.4.1    foreach_1.5.2      glue_1.6.2        \n[57] pan_1.8            nloptr_2.0.3       codetools_0.2-19   stringi_1.7.12    \n[61] gtable_0.3.3       shape_1.4.6        lme4_1.1-33        munsell_0.5.0     \n[65] pillar_1.9.0       htmltools_0.5.5    gplots_3.1.3       R6_2.5.1          \n[69] evaluate_0.21      lattice_0.21-8     backports_1.4.1    broom_1.0.5       \n[73] descr_1.1.7        Rcpp_1.0.10        nlme_3.1-162       gridExtra_2.3     \n[77] checkmate_2.2.0    xfun_0.39          pkgconfig_2.0.3"
  },
  {
    "objectID": "ch09hyp_analysis_rep.html#hypothesis-testing",
    "href": "ch09hyp_analysis_rep.html#hypothesis-testing",
    "title": "9  Hypothesis Testing, Analysis, & Reporting",
    "section": "9.2 Hypothesis Testing",
    "text": "9.2 Hypothesis Testing"
  },
  {
    "objectID": "ch09hyp_analysis_rep.html#analysis",
    "href": "ch09hyp_analysis_rep.html#analysis",
    "title": "9  Hypothesis Testing, Analysis, & Reporting",
    "section": "9.3 Analysis",
    "text": "9.3 Analysis"
  },
  {
    "objectID": "ch09hyp_analysis_rep.html#reporting",
    "href": "ch09hyp_analysis_rep.html#reporting",
    "title": "9  Hypothesis Testing, Analysis, & Reporting",
    "section": "9.4 Reporting",
    "text": "9.4 Reporting"
  },
  {
    "objectID": "ch07weights.html#step-by-step-guide-to-creating-basic-rake-weights-in-r",
    "href": "ch07weights.html#step-by-step-guide-to-creating-basic-rake-weights-in-r",
    "title": "8  Survey Weighting",
    "section": "8.1 Step-By-Step Guide to Creating Basic Rake Weights in R",
    "text": "8.1 Step-By-Step Guide to Creating Basic Rake Weights in R\nNote: This tutorial uses the anesrake package to calculate the survey weights. They are many other packages to calculate weights so this is just one possible approach that could successfully be used to create survey weights.\nSurvey weights are widely used in survey research for a variety of purposes. In this tutorial, we will be focusing on one specific form of survey weights called a “rake weight”. Rake weights are typically used to make the survey sample match the target population on a set of demographic, and sometimes attitudinal, measures. They are used to ensure the sample’s demographics match the target population’s demographics. This numerical correction will change how much each individual case in your dataset is contributing to the overall, or sub-group, mean values across your sample data.\nFirst, we load necessary packages to compute and analyze the weights. If a package is not installed on your machine, you must first install it before this chunk of code will run."
  },
  {
    "objectID": "part_intro.html#section-outline",
    "href": "part_intro.html#section-outline",
    "title": "The Science of Survey Methods",
    "section": "Section Outline",
    "text": "Section Outline\nIn addition to getting our working environment set up to conduct survey analyses, this section introduces students to the science behind survey methods, including how individuals approach and answer surveys, as well as the ethics behind designing, administering, and synthesizing surveys.\n\nChapter 1: Introduction to Survey Methods\nChapter 2: Setting Up our Environment for Survey Methods\nChapter 3: Psychology of Survey Response\nChapter 4: Survey Ethics"
  },
  {
    "objectID": "ch00setup.html#cleaning-and-recoding-data-prior-to-analysis",
    "href": "ch00setup.html#cleaning-and-recoding-data-prior-to-analysis",
    "title": "2  Data Cleaning and Recoding",
    "section": "2.1 Cleaning and Recoding Data Prior to Analysis",
    "text": "2.1 Cleaning and Recoding Data Prior to Analysis\nAnytime you import a survey, or really any kind of data into R, you should always ask yourself what you need to do to get it prepared for analysis. With survey data specifically, there are usually a few things that should always be checked and changed prior to conducting any type of analysis.\nWe will cover several of them here, including the following:\n\nReordering a survey scale so higher values = more of something\nReorder incorrectly coded ordinal scales\nCombine multiple questions into one response\nChange the name of a variable to indicate what it measures\nCombine multiple options into fewer groups\nSetting specific values to NAs\nApplying survey weights"
  },
  {
    "objectID": "ch00setup.html#importing-data",
    "href": "ch00setup.html#importing-data",
    "title": "2  Data Cleaning and Recoding",
    "section": "2.2 Importing Data",
    "text": "2.2 Importing Data\nWe will be importing data from a Stata file downloaded from the American National Election Survey website for the 2020 presidential election in the United States. This survey interviewed over 8,000 US residents asking over 1,000 questions combined between a pre and post survey.\n\n\n\n\n\n\nStarting a New Analysis\n\n\n\nYou should always start a new analysis with a clean data set, meaning one that has not been previously altered. This allows for easier replication by other researchers and ensures that all of the changes made to variables and cases in your data set can be followed.\n\n\nSurvey data is unique to other types of data because most variables are a combination of numbers and variable labels. Because survey questions require respondents to answer with pre-written scale options, for which there is a corresponding number and value label, it adds an additional complication to simply importing data. The haven package handles this unique aspect well by reading both the number and the variable label into R. This gives us more information about our data that we can use to make informed decisions on how to clean prior to analysis.\n\n\n\n\n\n\nDownloading Codebooks\n\n\n\nYou should always download the codebook for any available survey data set so that you have access to how each variable was asked along with how the variables are coded.\n\n\n\nanes &lt;- read_dta('C:/Users/Stefani.Langehennig/OneDrive - University of Denver/Documents/research/surveys-textbook-data/anes_timeseries_2020_stata_20220210.dta')"
  },
  {
    "objectID": "ch00setup.html#recoding-variables",
    "href": "ch00setup.html#recoding-variables",
    "title": "2  Data Cleaning and Recoding",
    "section": "2.3 Recoding Variables",
    "text": "2.3 Recoding Variables\n\n2.3.0.1 Naming Conventions\nIt is important when you start creating new variables to keep a few best practices in mind. First, names should always be lowercase. It is okay to have multiple words in a variable name but you should use ‘snake_case’, which means connecting words with an underscore ‘_’. Next, keep names short but as informative as possible. The name should reflect what the new variable is measuring. So if the new variables measures presidential approval, the name should convey that with something like pres_app where pres = presidential and app = approval. This follows all of the best practices: lowercase, snake_case, and short but informative.\nKeep this in mind as we work through this code.\n\n\n2.3.0.2 Flipping Order of Scale\nTo simplify, we will focus only on a few variables in the 2020 ANES data. Let’s start with examining how the ANES survey coded approval ratings of the president of the United States. When we examine the codebook, we find that the approval rating question is V201129x, and it is coded where 1 = Strongly Approve and 4 = Strongly Disapprove. This is a classic example of when we would want to flip the scale so that higher values equal approval rather than disapproval. This makes it easier to discuss the variable as we are used to talking about the approval of something rather than its negation, disapproval.\nFirst, let’s examine the attributes of the variable so that we can learn more about it before the transformations.\n\nattributes(anes$V201129x) #This gives you the variable label, class, and value labels for a survey object\n\n$label\n[1] \"PRE: SUMMARY: Approve or disapprove President handling job\"\n\n$format.stata\n[1] \"%12.0g\"\n\n$class\n[1] \"haven_labelled\" \"vctrs_vctr\"     \"double\"        \n\n$labels\n-2. DK/RF in V201127 or V201128             1. Approve strongly \n                             -2                               1 \n        2. Approve not strongly      3. Disapprove not strongly \n                              2                               3 \n         4. Disapprove strongly \n                              4 \n\n\nHere, we confirm that the variable we are analyzing is the one we want to analyze per the codebook. The label tells us the question is the presidential job approval one, the class tells us it is in haven_labelled format, and the labels tell us the value label for each specific value. We also see that -2 values are non-substantive responses that should be removed from the analysis.\nNext, we will get a frequency distribution for the presidential approval rating question to understand its distribution using the tidyverse.\n\nanes %&gt;%     #Data we are using             \n  count(V201129x) %&gt;%                            # Variable we want the distribution for \n  mutate(percent = scales::percent(n / sum(n))) # calculate percent  \n\n# A tibble: 5 × 3\n  V201129x                                 n percent\n  &lt;dbl+lbl&gt;                            &lt;int&gt; &lt;chr&gt;  \n1 -2 [-2. DK/RF in V201127 or V201128]    56 0.7%   \n2  1 [1. Approve strongly]              2603 31.4%  \n3  2 [2. Approve not strongly]           759 9.2%   \n4  3 [3. Disapprove not strongly]        502 6.1%   \n5  4 [4. Disapprove strongly]           4360 52.7%  \n\n\nFrom this output, we see an important fact: missing data is currently coded as -2 and if we tried to analyze these data without cleaning, we would be including that -2 in all calculations. That would lead to bias in our results, causing us to draw incorrect conclusions.\n\n\n\n\n\n\nImportant\n\n\n\nMissing data that is coded in a particular way can bias your results. Always check the frequency distribution of your data prior to analysis.\n\n\nWe also know from above that 1 = Strongly Approve while 4 = Strongly Disapprove. Next, we will flip the scale order while removing the missing data and saving a new variable called pres_app for presidential approval.\nNote, that we will use case_when here for recoding rather than the explicit recode function. This is because case_when is more flexible at handling labelled variables like our survey data but does require slightly more code. We also need case_when for more complex transformations such as combining two variables into one.\n\nanes &lt;- anes %&gt;% #This creates new variable called 'pres_app' where 4 = strongly approve and 1 = strongly disapprove while setting -2 to NA \n  mutate(pres_app = case_when(\n    V201129x ==1 ~ 4,\n    V201129x ==2 ~ 3,\n    V201129x ==3 ~ 2,\n    V201129x ==4 ~ 1, \n  V201129x ==-2 ~ NA_real_)) #This code makes all values of -2 = NA for analysis purposes. \n\n# Define value labels\n\nvalue_labels &lt;- c(\"Strongly Disapprove\", \"Disapprove\", \"Approve\", \"Strongly Approve\") #Add value labels to your new measure. Only use this if you need to as it changes the variable type to 'factor' which influences the types of analysis you can do with it. Previously, it was 'numeric'. \n\n# Assign value labels to the Response variable\nanes$pres_app &lt;- factor(anes$pres_app, levels = 1:4, labels = value_labels)\n\nanes %&gt;%     #Data we are using             \n  count(pres_app) %&gt;%                            # Variable we want the distribution for \n  mutate(percent = scales::percent(n / sum(n))) # calculate percent \n\n# A tibble: 5 × 3\n  pres_app                n percent\n  &lt;fct&gt;               &lt;int&gt; &lt;chr&gt;  \n1 Strongly Disapprove  4360 52.7%  \n2 Disapprove            502 6.1%   \n3 Approve               759 9.2%   \n4 Strongly Approve     2603 31.4%  \n5 &lt;NA&gt;                   56 0.7%   \n\nCrossTable(anes$V201129x, anes$pres_app, expected = FALSE, chisq=FALSE,  prop.c=TRUE, prop.r=FALSE, prop.t=FALSE, prop.chisq = FALSE)\n\n   Cell Contents \n|-------------------------|\n|                       N | \n|           N / Col Total | \n|-------------------------|\n\n===============================================================================\n                 anes$pres_app\nanes$V201129x    Strngly Dspprv   Disapprove   Approve   Strongly Apprv   Total\n-------------------------------------------------------------------------------\n-2                            0            0         0                0       0\n                              0            0         0                0        \n-------------------------------------------------------------------------------\n1                             0            0         0             2603    2603\n                              0            0         0                1        \n-------------------------------------------------------------------------------\n2                             0            0       759                0     759\n                              0            0         1                0        \n-------------------------------------------------------------------------------\n3                             0          502         0                0     502\n                              0            1         0                0        \n-------------------------------------------------------------------------------\n4                          4360            0         0                0    4360\n                              1            0         0                0        \n-------------------------------------------------------------------------------\nTotal                      4360          502       759             2603    8224\n                          0.530        0.061     0.092            0.317        \n===============================================================================\n\n\nThis example illustrates how to flip the order of a question scale so that higher values indicate more of whatever the question measures, in this case presidential approval. We named the new variable pres_app to keep the label short but also informative of what the question measures. We then added value labels to ensure we know what each value represents.\nIt is important to examine the distribution of your new variable to ensure nothing went wrong in the recoding process. We did two checks:\n\nWe checked the frequency distribution of the new variable to ensure we see what we expect to see.\nWe run a crosstab between our new variable and the original. We should expect to see a mirror image in the crosstab, which we do. This means our recode was successful and 4 = Strongly Approve while 1 = Strongly Disapprove.\n\nWe then used the haven package to label the new pres_app with more informative information to ensure we remember what the variable is measuring.\nNext, we will look at one additional way to flip your scale. This is least amount of code but also offers the largest possibility of error. Here we will the if_else command along with mutate to create a new presidential approval variable pres_app3, which is the inverse of the original presidential approval variable V201129x. We multiply the original value by -1 and then add one more than the total number of scale points. Here, we had 4 total scale points - strongly approve, approve, disapprove, strongly disapprove - so we add 4+1 (5) to the scale. This mathematically flips the scale so that the original values of 1=4, 2=3, 3=2, and 4=1. It will always work provided you add the appropriate number of points.\n\n#Additional way to flip your scale \nanes &lt;- anes %&gt;% \n  mutate(pres_app3 = if_else(V201129x&gt;=1, (V201129x*-1)+5, NA)) #When original variable &gt;= 1, we will multiply the original variable by -1 and then add 1 more than the total scale points. Since there were 4 scale points in the original scale, we add 5. We also recode anything that was &gt;=1 originally as NA since that reflects a non-substantive response. \n\n#1 becomes 4 b/c (1*-1)=-1+5=4\n#2 becomes 3 b/c (2*-1)=-2+5=3\n#3 becomes 2 b/c (3*-1)=-3+5=2\n#4 becomes 1 b/c (4*-1)=-4+5=1\n\nanes %&gt;%     #Data we are using             \n  group_by(V201129x)  %&gt;% #Original variable\n  count(pres_app3)         # New variable \n\n# A tibble: 5 × 3\n# Groups:   V201129x [5]\n  V201129x                             pres_app3     n\n  &lt;dbl+lbl&gt;                                &lt;dbl&gt; &lt;int&gt;\n1 -2 [-2. DK/RF in V201127 or V201128]        NA    56\n2  1 [1. Approve strongly]                     4  2603\n3  2 [2. Approve not strongly]                 3   759\n4  3 [3. Disapprove not strongly]              2   502\n5  4 [4. Disapprove strongly]                  1  4360\n\n\n\n\n2.3.0.3 Combining Two Variables into One\nNow, let’s combine two variables into one. Oftentimes, surveys will ask branching questions that need to be combined for analysis purposes. In fact, the presidential approval measure we just analyzed is the combination of two questions. Most survey firms will not pre-combine these two questions into one so it is important to learn how to do so.\nThere are two variables to combine:\n\nV201127 (1= approve & 2=disapprove) #Approve/Disapprove of performance\nV201128 (1=strongly & 2 = not strongly) #How strongly approve/disapprove of performance\n\nRemember, higher values should equal more approval so we want 4 = Strongly Approve and 1 = Strongly Disapprove.\n\n#First run a crosstab between your two existing variables to get the distribution across cells. \n\nanes %&gt;%     #Data we are using             \n  group_by(V201128 )  %&gt;% #X Variable in Crosstab \n  count(V201127)         # Y Variable in Crosstab \n\n# A tibble: 8 × 3\n# Groups:   V201128 [4]\n  V201128               V201127                 n\n  &lt;dbl+lbl&gt;             &lt;dbl+lbl&gt;           &lt;int&gt;\n1 -9 [-9. Refused]       1 [1. Approve]         3\n2 -9 [-9. Refused]       2 [2. Disapprove]      1\n3 -1 [-1. Inapplicable] -9 [-9. Refused]       44\n4 -1 [-1. Inapplicable] -8 [-8. Don't know]     8\n5  1 [1. Strongly]       1 [1. Approve]      2603\n6  1 [1. Strongly]       2 [2. Disapprove]   4360\n7  2 [2. Not strongly]   1 [1. Approve]       759\n8  2 [2. Not strongly]   2 [2. Disapprove]    502\n\n#Next, create your new variable based on the 4 possible combinations using a series of & statements. Be careful.\n\nanes &lt;- anes %&gt;% #This creates new variable called 'pid_x' where higher values=more republican \n  mutate(pres_app2 = case_when(\n    V201127==1 & V201128 ==1 ~ 4,\n    V201127==1 & V201128 ==2 ~ 3,\n    V201127==2 & V201128 ==2 ~ 2,\n    V201127==2 & V201128 ==1 ~ 1))\n\n#Now, if we want we can make a new factor variable that saves the labels. \n\nanes &lt;- anes %&gt;% #This creates new variable called 'pres_app2_f' which is a factorized version of the above pres_app2 measure.  \n  mutate(pres_app2_f = case_when(\n    V201127==1 & V201128 ==1 ~ 'Strong Approve',\n    V201127==1 & V201128 ==2 ~ 'Approve',\n    V201127==2 & V201128 ==2 ~ 'Disapprove',\n    V201127==2 & V201128 ==1 ~ 'Strong Disapprove'))\n\n#Check our work against the original variables.\n\nanes %&gt;%     #Data we are using             \n  group_by(V201128, V201127)  %&gt;% #X Variable in Crosstab \n  count(pres_app2)         # Y Variable in Crosstab  \n\n# A tibble: 8 × 4\n# Groups:   V201128, V201127 [8]\n  V201128               V201127             pres_app2     n\n  &lt;dbl+lbl&gt;             &lt;dbl+lbl&gt;               &lt;dbl&gt; &lt;int&gt;\n1 -9 [-9. Refused]       1 [1. Approve]            NA     3\n2 -9 [-9. Refused]       2 [2. Disapprove]         NA     1\n3 -1 [-1. Inapplicable] -9 [-9. Refused]           NA    44\n4 -1 [-1. Inapplicable] -8 [-8. Don't know]        NA     8\n5  1 [1. Strongly]       1 [1. Approve]             4  2603\n6  1 [1. Strongly]       2 [2. Disapprove]          1  4360\n7  2 [2. Not strongly]   1 [1. Approve]             3   759\n8  2 [2. Not strongly]   2 [2. Disapprove]          2   502\n\n#Check our work against our previously created variable.\n\nanes %&gt;%     #Data we are using             \n  group_by(pres_app)  %&gt;% #X Variable in Crosstab \n  count(pres_app2)         # Y Variable in Crosstab  \n\n# A tibble: 5 × 3\n# Groups:   pres_app [5]\n  pres_app            pres_app2     n\n  &lt;fct&gt;                   &lt;dbl&gt; &lt;int&gt;\n1 Strongly Disapprove         1  4360\n2 Disapprove                  2   502\n3 Approve                     3   759\n4 Strongly Approve            4  2603\n5 &lt;NA&gt;                       NA    56\n\n\nOnce we run the above code, we see that we successfully created our new variable combining two separate variables. This is a flexible approach that can be applied to any two or more variables provided you are careful in coding the correct values. For more complex combinations, it can be useful to map it on a whiteboard or piece of paper first.\n\n\n2.3.0.4 Collapsing Variables into Smaller Groups\nOftentimes, we want to collapse the number of groups in a variable into fewer groups or even into a dummy, otherwise known as dichotomous, variable. Using education (V201507x) as the example, let’s look at several ways to collapse groups into fewer options. Remember, since we are doing data transformations, we want to keep the original variable untransformed and save a new one.\nStart by examining the frequency distribution for the variable you want to transform. For education, we see that less than HS degree is only selected 376 times out of the 8,000+ cases. Because it is selected so infrequently, it should definitely be combined with the next option, HS degree. We also see that some college but no degree option is the modal response for the scale. This suggests that, depending on our theory and planned analysis, we should create a few different educational attainment variables.\nFirst, we will keep the original categories the same but combine the less than HS degree with HS degree options.\nThen, we will create a dummy variable for college degree or not (0 or 1):\n\nCollege Degree = People with Bachelor’s degree or graduate degree\nNo College Degree = People with some college but no degree, HS degree, or no HS degree\n\nLastly, we will create a 3-point scale splitting the no college degree into Some college or No College at all:\n\nCollege Degree = People with Bachelor’s degree or graduate degree\nSome College = People with some college but no degree\nHigh school degree or less = HS degree or no HS degree\n\n\n#First look at the age distribution\nanes %&gt;%     #Data we are using             \n  count(V201511x)         # Variable to analyze \n\n# A tibble: 8 × 2\n  V201511x                                                              n\n  &lt;dbl+lbl&gt;                                                         &lt;int&gt;\n1 -9 [-9. Refused]                                                     33\n2 -8 [-8. Don't know]                                                   1\n3 -2 [-2. Missing, other specify not coded for preliminary release]    97\n4  1 [1. Less than high school credential]                            376\n5  2 [2. High school credential]                                     1336\n6  3 [3. Some post-high school, no bachelor's degree]                2790\n7  4 [4. Bachelor's degree]                                          2055\n8  5 [5. Graduate degree]                                            1592\n\nanes &lt;- anes %&gt;%\n  mutate(college = ifelse(V201511x %in% c(1, 2), 1, V201511x-1)) #Recodes all values from the original education variables that are 1 or 2 to be = 1 and then sets all other values to their original value - 1 t0 keep the order in tact of 1, 2, 3, 4. \n\nvalue_labels &lt;- c(\"HS or Less\", \"Some College\", \"College Degree\", \"Graduate Degree\") #Add value labels to your new measure. Only use this if you need to as it changes the variable type to 'factor' which influences the types of analysis you can do with it. Previously, it was 'numeric'. \n\n# Assign value labels to the Response variable\nanes$college &lt;- factor(anes$college, levels = 1:4, labels = value_labels)\n\n#You should notice that -9 = missing data. -9 will automatically become NA in the new variable if you simply do nothing with it in this code. \n\n##Second way to recode our college variable; this time to create a dummy variable. \nanes &lt;- anes %&gt;% # \n  mutate(college2 = case_when(\n    V201511x==1 ~ 'No College Degree',\n    V201511x==2  ~ 'No College Degree',\n    V201511x==3  ~ 'No College Degree',\n    V201511x==4  ~ 'College Degree',\n    V201511x==5  ~ 'College Degree'))\n\nanes &lt;- anes %&gt;% # \n  mutate(college2 = case_when(\n    V201511x==1 ~ 0,\n    V201511x==2  ~ 0,\n    V201511x==3  ~ 0,\n    V201511x==4  ~ 1,\n    V201511x==5  ~ 1))\n\nclass(anes$college)\n\n[1] \"factor\"\n\n#Checks distribution of new variable \nanes %&gt;%     #Data we are using             \n  count(college)         #  New Variable \n\n# A tibble: 5 × 2\n  college             n\n  &lt;fct&gt;           &lt;int&gt;\n1 HS or Less       1712\n2 Some College     2790\n3 College Degree   2055\n4 Graduate Degree  1592\n5 &lt;NA&gt;              131\n\n#Crosstab between original and new variable to ensure recode success\nanes %&gt;%     #Data we are using    \n  group_by(V201511x) %&gt;% #Original Variable \n  count(college)         # New Variable \n\n# A tibble: 8 × 3\n# Groups:   V201511x [8]\n  V201511x                                                         college     n\n  &lt;dbl+lbl&gt;                                                        &lt;fct&gt;   &lt;int&gt;\n1 -9 [-9. Refused]                                                 &lt;NA&gt;       33\n2 -8 [-8. Don't know]                                              &lt;NA&gt;        1\n3 -2 [-2. Missing, other specify not coded for preliminary releas… &lt;NA&gt;       97\n4  1 [1. Less than high school credential]                         HS or …   376\n5  2 [2. High school credential]                                   HS or …  1336\n6  3 [3. Some post-high school, no bachelor's degree]              Some C…  2790\n7  4 [4. Bachelor's degree]                                        Colleg…  2055\n8  5 [5. Graduate degree]                                          Gradua…  1592\n\n###Different approach to creating a dichotomous variable. Here, you have to explicitly make missing data NA \nanes &lt;- anes %&gt;% \n  mutate(college2 = if_else(V201511x&gt;3, 1, 0))\n\nanes &lt;- anes %&gt;% #Explicitly making the values -1/-9 NA \n  mutate(college2 = replace(college2, V201511x &lt;= -1, NA)) #Removes NA from new college2 variable  \n\n#Crosstab between original and new variable to ensure recode success\nanes %&gt;%     #Data we are using    \n  group_by(V201511x) %&gt;% #Original Variable \n  count(college2)         # New Variable \n\n# A tibble: 8 × 3\n# Groups:   V201511x [8]\n  V201511x                                                        college2     n\n  &lt;dbl+lbl&gt;                                                          &lt;dbl&gt; &lt;int&gt;\n1 -9 [-9. Refused]                                                      NA    33\n2 -8 [-8. Don't know]                                                   NA     1\n3 -2 [-2. Missing, other specify not coded for preliminary relea…       NA    97\n4  1 [1. Less than high school credential]                               0   376\n5  2 [2. High school credential]                                         0  1336\n6  3 [3. Some post-high school, no bachelor's degree]                    0  2790\n7  4 [4. Bachelor's degree]                                              1  2055\n8  5 [5. Graduate degree]                                                1  1592\n\n#Create our third new education variable; this one with 3 options\nanes &lt;- anes %&gt;% # \n  mutate(college3 = case_when(\n    V201511x==1 ~ 'HS Degree or Less',\n    V201511x==2  ~ 'HS Degree or Less',\n    V201511x==3  ~ 'Some College',\n    V201511x==4  ~ 'College Degree',\n    V201511x==5  ~ 'College Degree'))"
  },
  {
    "objectID": "ch00setup.html#working-with-missing-datanon-substantive-responses",
    "href": "ch00setup.html#working-with-missing-datanon-substantive-responses",
    "title": "2  Data Cleaning and Recoding",
    "section": "2.4 Working with Missing Data/non-Substantive Responses",
    "text": "2.4 Working with Missing Data/non-Substantive Responses\nNearly all survey data will include missing data that should be investigated. Oftentimes, missing data is coded as a real value in your survey such as ‘-2’ or ‘99’. If this is the case, you must ensure that R knows what values should not be included in your analysis otherwise you will introduce bias into your calculations.\nIf we only want to change a specific value or values to NA without recoding the entire variable, we can use the replace function from tidyverse. We still want to save a new variable since we are transforming the orginal variable in some way but we will not need to do anything else to the variable. Here we recode ‘-9’ to system missing for a feeling thermometer measure rating how much Americans liked Donald Trump in 2020.\nThis code easily handles more complexity as the second recode changes any values &lt;= -1 or &gt;= 101 to system missing since the codebook identifies all these values as non-valid answers. This is another reminder to always work from an up-to-date codebook so that you can catch any potential issues.\n\nanes &lt;- anes %&gt;%\n  mutate(trump_feel = replace(V201152, V201152 == -9, NA)) #Create new variable called 'trump_feel' which equals the original variable 'V201151' but replace all -9 values as NA\n\nanes &lt;- anes %&gt;%\n  mutate(biden_feel = replace(V201151, (V201151 &lt;= -1 | V201151 &gt;= 101), NA)) #Create new variable called 'biden_feel' which equals the original variable 'V201151' but replace all -9 values as NA\n\n\n\n\n\n\n\nNote\n\n\n\nRemember, we already looked at how to make a value system missing using case_when when recoding the entire variable.\n\n\n\nanes &lt;- anes %&gt;% #This creates new variable called 'pres_app' where 4 = strongly approve and 1 = strongly disapprove while setting -2 to NA \n  mutate(pres_app = case_when(\n    V201129x ==1 ~ 4,\n    V201129x ==2 ~ 3,\n    V201129x ==3 ~ 2,\n    V201129x ==4 ~ 1, \n  V201129x ==-2 ~ NA_real_)) #This code makes all values of -2 = NA for analysis purposes."
  },
  {
    "objectID": "ch00setup.html#merging-external-data-into-survey-data",
    "href": "ch00setup.html#merging-external-data-into-survey-data",
    "title": "2  Data Cleaning and Recoding",
    "section": "2.5 Merging External Data into Survey Data",
    "text": "2.5 Merging External Data into Survey Data\nDepending on the sample, sometimes it is possible to link external data - i.e. data not collected in the survey but from some source external to the interview - to individual survey records. This can include things like a student’s grades if you work for a school system or voting records for political poll respondents. In these cases, researchers can utilize these external files to do a variety of interesting analyses. However, the first step is to merge the two files, which is not always easy to do.\nHere, we are going to merge the original 2020 American Election Study Survey with validated voting information collected by a third party vendor. Validated vote data essential is just a public record of if an individual tied their vote to their survey responses, all while keeping their identify confidential to researchers. By merging these files, researchers can then answer important questions around who actually turns out to vote versus who says they turned out to vote.\nLet’s go through that process here.\nTo begin, we need to identify the case_id, which is the column which should provide a unique identification number for all individual survey responses. This will be the critical variable that needs to be matched between your main file - for us the 2020 ANES - and the file to be merged - the validated vote file. We need to examine both files to ensure they have the same structure for their case_id variable and ideally that the variable names match. If these two columns do not match exactly, you will only get partial to zero matches.\n\n\n\n\n\n\nImportant\n\n\n\nAlways closely examine what variables are in your columns before trying to merge, as this will influence the quality of your matches.\n\n\n\nvvote &lt;- read_dta('C:/Users/Stefani.Langehennig/OneDrive - University of Denver/Documents/research/surveys-textbook-data/anes_timeseries_2020_stata_VoterValidation.dta')\n\nhead(anes$V200001)\n\n[1] 200015 200022 200039 200046 200053 200060\n\nhead(vvote$V200001)\n\n[1] 200015 200022 200039 200046 200053 200060\n\nanes_vv &lt;- merge(anes, vvote, by = \"V200001\", all = FALSE)\n\nWe’ll start by looking at the codebook for the validated vote data which tells us that V200001 is the name for the case_id variable and that it should match the original 2020 ANES name exactly. Using the head function, we can quickly verify that indeed these two variables have the same structure and same values for the first few cases. This indicates that it is safe to move forward with merging the two files.\nNote, there are 18,430 cases in the vvote data frame while there are only 8,280 in the anes data. That means the vvote data has cases that do not exist in the anes data, which is fine. We do not want to merge data that is not included in the ANES file. By including all = FALSE, we tell R to only include variables that are in the main file in the merge. This is one reason why it is important to pay attention to which file you are treating as the main file, which will be listed first in the merge code.\n\nanes_vv &lt;- merge(anes, vvote, by = \"V200001\")\nview(anes_vv)\nnames(vvote)\n\n [1] \"version\"           \"V200001\"           \"val1_addmatch\"    \n [4] \"val1_number\"       \"val1_matchprob\"    \"val1_turnout16\"   \n [7] \"val1_turnout20\"    \"val1_hh_turnout16\" \"val1_hh_turnout20\"\n[10] \"val2_addmatch\"     \"val2_number\"       \"val2_matchprob\"   \n[13] \"val2_match\"        \"val2_turnout16\"    \"val2_turnout20\"   \n[16] \"val2_hh_turnout16\" \"val2_hh_turnout20\" \"val2_c1_minor\"    \n[19] \"val2_c1_major\"     \"val2_c1_match\"     \"val2_c2_minor\"    \n[22] \"val2_c2_major\"     \"val2_c2_match\"    \n\n#Distribution of responses\nanes_vv %&gt;%     #Data we are using             \n  count(val1_turnout20)         # Y Variable in Crosstab \n\n  val1_turnout20    n\n1              2  261\n2              3 1605\n3              4 2965\n4              5  933\n5              6 2516\n\nanes_vv %&gt;%     #Data we are using             \n  count(val2_turnout20)         # Y Variable in Crosstab \n\n  val2_turnout20    n\n1              2  424\n2              3 1034\n3              7 6822\n\nCrossTable(anes_vv$val1_turnout20, anes_vv$val2_turnout20, expected = FALSE, chisq=TRUE,  prop.c=TRUE, prop.r=FALSE, prop.t=FALSE, prop.chisq = FALSE)\n\n   Cell Contents \n|-------------------------|\n|                       N | \n|           N / Col Total | \n|-------------------------|\n\n=======================================================\n                          anes_vv$val2_turnout20\nanes_vv$val1_turnout20        2       3       7   Total\n-------------------------------------------------------\n2                            67      55     139     261\n                          0.158   0.053   0.020        \n-------------------------------------------------------\n3                           187     710     708    1605\n                          0.441   0.687   0.104        \n-------------------------------------------------------\n4                            82     115    2768    2965\n                          0.193   0.111   0.406        \n-------------------------------------------------------\n5                            22      41     870     933\n                          0.052   0.040   0.128        \n-------------------------------------------------------\n6                            66     113    2337    2516\n                          0.156   0.109   0.343        \n-------------------------------------------------------\nTotal                       424    1034    6822    8280\n                          0.051   0.125   0.824        \n=======================================================\n\nStatistics for All Table Factors\n\nPearson's Chi-squared test \n------------------------------------------------------------\nChi^2 = 2492.817      d.f. = 8      p &lt;2e-16 \n\nattributes(anes_vv$V201529)\n\n$label\n[1] \"PRE: Describe R's employment\"\n\n$format.stata\n[1] \"%12.0g\"\n\n$labels\n                                                                     -9. Refused \n                                                                              -9 \n                                                                -1. Inapplicable \n                                                                              -1 \n                                           1. For-profit company or organization \n                                                                               1 \n  2. Non-profit organization (including tax-exempt and charitable organizations) \n                                                                               2 \n               3. Local government (for example: city or county school district) \n                                                                               3 \n                     4. State government (including state colleges/universities) \n                                                                               4 \n                          5. Active duty U.S. Armed Forces or Commissioned Corps \n                                                                               5 \n                                         6. Federal government civilian employee \n                                                                               6 \n           7. Owner of non-incorporated business, professional practice, or farm \n                                                                               7 \n               8. Owner of incorporated business, professional practice, or farm \n                                                                               8 \n9. Worked without pay in a for-profit family business or farm for 15 hours or mo \n                                                                               9 \n\n$class\n[1] \"haven_labelled\" \"vctrs_vctr\"     \"double\"        \n\nanes_vv %&gt;% \n  count(V201529)\n\n   V201529    n\n1       -9  181\n2       -1  234\n3        1 4141\n4        2  920\n5        3  772\n6        4  401\n7        5   49\n8        6  330\n9        7  683\n10       8  423\n11       9  146\n\n\nNow that the files are merged, we can quickly see that in the original anes file we had 1,775 variables while in the new anes_vv merged file we have 1,797 variables, or 22 additional. A quick visual inspection of the end of the anes_vv file reveals that the variables that were in the vvote file are now appended to the end of our original anes file. This means our merge was successful, and we can now start to analyze the newly created file."
  },
  {
    "objectID": "ch00setup.html#applying-survey-weights-in-the-analysis",
    "href": "ch00setup.html#applying-survey-weights-in-the-analysis",
    "title": "2  Data Cleaning and Recoding",
    "section": "2.6 Applying Survey Weights in the Analysis",
    "text": "2.6 Applying Survey Weights in the Analysis\nSurvey weights make the survey sample’s demographic profile look more like its population while also sometimes accounting for things such as unequal likelihood of being selected to participate. Many publicly accessible large-n surveys will come with associated survey weights that should be applied when conducting analysis. For the most part, provided weights should always be used when analyzing survey data.\nThe first step to using survey weights is to review the codebook and find what it has to say about its survey weights. While some survey data sets will come with only 1 survey weight, many, like the ANES which has 14 unique weights, will have multiple and utilizing the codebook is imperative in this instance.\nBy reviewing the 2020 ANES codebook, we see that we want to use V200010b as our primary weight since we are analyzing data collected in the post-election survey wave. In this case, we also want to include a strata weight per the instructions. Not all surveys will include both types of weights but when they do each should be utilized.\nThe fact that we are required to use the post-election survey weight will pose an additional problem that not every respondent who took the pre-election survey returned for the post-election wave. This code requires no missing data in the weighting variable otherwise it will not run. So we first will remove the cases that do not have values in the post-election weight variable.\nWe will use the svydesign function from the survey package to create new weighted data sets for analysis purposes. The new weighted file can then be used in analysis.\n\nanes_post &lt;- anes_vv[complete.cases(anes_vv[, c(\"V200010b\")]), ]\nanes_weighted &lt;- svydesign(ids = ~1, weights =~V200010b, data = anes_post) #Creates new weighted data for analysis using the population weights only \nanes_weighted2 &lt;- svydesign(ids = ~V200010c, weights =~V200010b, strata=~V200010d, nest=TRUE, data = anes_post) #For more complex survey designs that includes additional weights including strata and PSUs. PSU weight = 'ids' while 'strata' = strata weight as specified in code book   \n\n#Creates new variable for use in analysis \nanes_post &lt;- anes_post %&gt;%\n  mutate(trump_feel = replace(V201152, V201152 == -9, NA)) #Create new variable called 'trump_feel' which equals the original variable 'V201152' but replace all -9 values as NA\n\n###Analyze the mean of the Trump feeling thermometer measure for the different files \nsvymean(~trump_feel, anes_weighted, na.rm = TRUE) #Get weighted mean for simple weighting scheme\n\n             mean     SE\ntrump_feel 41.289 0.6584\n\nsvymean(~trump_feel, anes_weighted2, na.rm = TRUE) #Get weighted mean for complex weighting scheme\n\n             mean     SE\ntrump_feel 41.289 0.6973\n\nmean(anes_post$trump_feel, na.rm=TRUE) #Get unweighted mean for sample \n\n[1] 39.97184\n\n\nBy examining the means across the weighted and unweighted files, we see that the two weighted files return identical means, with slightly different standard errors, but the unweighted mean being 1.3 points lower. The difference between the weighted and unweighted means is why we want to use the survey weights as the weighted data should provide more accurate point estimates."
  },
  {
    "objectID": "part_design.html#section-outline",
    "href": "part_design.html#section-outline",
    "title": "Introduction to Survey Design",
    "section": "Section Outline",
    "text": "Section Outline"
  },
  {
    "objectID": "part_analysis.html#section-outline",
    "href": "part_analysis.html#section-outline",
    "title": "Introduction to Survey Analysis",
    "section": "Section Outline",
    "text": "Section Outline\nOnce we have completed the design of our survey, we are ready to launch the instrument! This means we will have data for which we can analyze. When we conduct a survey, we usually have a research or business objective in mind. Often, we will start with a hypothesis, collect our data, analyze it, and report back on what we found. This section will cover each of those things in depth:\n\nChapter 10: Hypothesis Testing\nChapter 11: Analyzing Survey Data\nChapter 12: Reporting Survey Data"
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "Survey Methods for Social Scientists",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nMore to come!"
  },
  {
    "objectID": "ch10conclusion.html#benefits-of-survey-design-analysis-in-r",
    "href": "ch10conclusion.html#benefits-of-survey-design-analysis-in-r",
    "title": "11  Conclusion",
    "section": "11.1 Benefits of Survey Design & Analysis in R",
    "text": "11.1 Benefits of Survey Design & Analysis in R"
  },
  {
    "objectID": "ch09hypothesis.html#learning-objectives",
    "href": "ch09hypothesis.html#learning-objectives",
    "title": "10  Hypothesis Testing in Survey Analysis",
    "section": "10.1 Learning Objectives",
    "text": "10.1 Learning Objectives"
  },
  {
    "objectID": "ch10analysis.html#learning-objectives",
    "href": "ch10analysis.html#learning-objectives",
    "title": "11  Analyzing Survey Data",
    "section": "11.1 Learning Objectives",
    "text": "11.1 Learning Objectives"
  },
  {
    "objectID": "ch11report.html#learning-objectives",
    "href": "ch11report.html#learning-objectives",
    "title": "12  Reporting Survey Data",
    "section": "12.1 Learning Objectives",
    "text": "12.1 Learning Objectives"
  },
  {
    "objectID": "ch12conclusion.html#benefits-of-survey-design-analysis-in-r",
    "href": "ch12conclusion.html#benefits-of-survey-design-analysis-in-r",
    "title": "Conclusion",
    "section": "Benefits of Survey Design & Analysis in R",
    "text": "Benefits of Survey Design & Analysis in R"
  },
  {
    "objectID": "ch02psych_response.html#factor-analysis-to-understan-political-emotions",
    "href": "ch02psych_response.html#factor-analysis-to-understan-political-emotions",
    "title": "3  Psychology of Survey Response",
    "section": "3.1 Factor Analysis to Understan Political Emotions",
    "text": "3.1 Factor Analysis to Understan Political Emotions\nWe will conduct both exploratory and confirmatory factor analysis using the nationally representative 2020 American National Election Survey for our data. In this survey, 9 unique political emotions variables were asked of respondents, and we will use these 9 variables to understand how political emotions are structured. Currently, it is widely believed that three latent emotional factors best explain the structure of political emotions (Marcus et al. 2006) with three primary latent factors:\n\nAversion to Politics: Measures include anger, outrage, and irritation\nWorry about Politics: Measures include fear, worry, and nervousness\nEnthusiasm about Politics: Measures include happiness, hope, and pride\n\nWe will use factor analysis to test whether a three factor solution really is the best way to explain these political emotions.\nTo start, we read in our data, in this case the 2020 American National Election study. Then we save a new data frame that includes only the nine political emotions variables we want to include in our analysis. We will use this data frame throughout the code. Once we have saved the political emotions variable as a new data frame, we recode all the negative values to NA as these are non-substantive responses which should be removed (as indicated in the associated codebook).\nNext, we change the variables names to something that is more informative. This will aid in interpretation of the factor analysis results and should not be skipped. Lastly, we use the skimr package to quickly skim the variables in our dataset. We want to see the minimum value = 1 with no negative values, since negative values should be treated as missing data.\n\nlibrary(haven) #Imports stata file\nlibrary(skimr) #For data evaluation \nlibrary(psych) #For exploratory factor analysis\nlibrary(corrplot) #To graph correlations \nlibrary(lavaan) #For confirmatory factor analysis\nlibrary(semPlot) #For graphing CFA results \n\nanes &lt;- read_dta('C:/Users/Stefani.Langehennig/OneDrive - University of Denver/Documents/research/surveys-textbook-data/anes_timeseries_2020_stata_20220210.dta')\n\ndf &lt;- data.frame(anes$V201115, anes$V201116, anes$V201117, anes$V201118, anes$V201119, anes$V201120, anes$V201121, anes$V201122, anes$V201123  ) #Save the variables you want to include in your factor analysis into a new data frame\n\ndf[df &lt;= -1] &lt;- NA #Set missing values to NA to not include in analysis\n\nnew_names &lt;- c(\"hope\", \"afraid\", \"outrage\", \"angry\", \"happy\", \"worried\", \"proud\", \"irritated\", \"nervous\") #Give your variables new informative names \n\n# Update column names\ncolnames(df) &lt;- new_names #Apply new names to your data frame\n\nskim(df) #Checks the variables in your data frame; evaluate for missing data\n\n\nData summary\n\n\nName\ndf\n\n\nNumber of rows\n8280\n\n\nNumber of columns\n9\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n9\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nhope\n23\n1\n2.53\n1.17\n1\n2\n3\n3\n5\n▆▆▇▃▂\n\n\nafraid\n15\n1\n3.39\n1.23\n1\n3\n3\n4\n5\n▂▅▇▇▆\n\n\noutrage\n18\n1\n3.60\n1.25\n1\n3\n4\n5\n5\n▂▃▆▇▇\n\n\nangry\n17\n1\n3.58\n1.20\n1\n3\n4\n5\n5\n▂▃▇▇▇\n\n\nhappy\n20\n1\n1.94\n1.05\n1\n1\n2\n3\n5\n▇▃▃▁▁\n\n\nworried\n12\n1\n3.69\n1.16\n1\n3\n4\n5\n5\n▁▃▆▇▇\n\n\nproud\n27\n1\n2.00\n1.15\n1\n1\n2\n3\n5\n▇▃▃▂▁\n\n\nirritated\n14\n1\n3.85\n1.11\n1\n3\n4\n5\n5\n▁▂▅▇▇\n\n\nnervous\n9\n1\n3.54\n1.20\n1\n3\n4\n5\n5\n▂▃▇▇▇\n\n\n\n\n\n\n3.1.0.1 Step 1: Evaluating Correlations Between Political Emotions\nFirst, we need to check the correlations between your variables - here, political emotions - to see how related each of the individual items are. Below, we create a matrix with all the correlations between the individual items and graph the correlations using a heat-map for easier viewing.\n\n#Step 1: Evaluate correlations \ncor_matrix&lt;-cor(df, use = \"pairwise.complete.obs\") #Saves correlation matrix\n\ncorrplot(cor_matrix, method = \"circle\") # Plot correlation matrix as a heatmap\n\n\n\ncor_matrix\n\n                hope     afraid    outrage      angry      happy    worried\nhope       1.0000000 -0.4214833 -0.3488169 -0.3654797  0.6252765 -0.4296574\nafraid    -0.4214833  1.0000000  0.6235087  0.6420478 -0.4527532  0.7566750\noutrage   -0.3488169  0.6235087  1.0000000  0.7798428 -0.4336406  0.6483833\nangry     -0.3654797  0.6420478  0.7798428  1.0000000 -0.4469920  0.6599894\nhappy      0.6252765 -0.4527532 -0.4336406 -0.4469920  1.0000000 -0.5004800\nworried   -0.4296574  0.7566750  0.6483833  0.6599894 -0.5004800  1.0000000\nproud      0.6262467 -0.4169572 -0.4029037 -0.4156885  0.7420263 -0.4557817\nirritated -0.3726374  0.6011975  0.7076147  0.7347953 -0.4720350  0.6496327\nnervous   -0.4306049  0.7711057  0.6188526  0.6410278 -0.4745889  0.7744649\n               proud  irritated    nervous\nhope       0.6262467 -0.3726374 -0.4306049\nafraid    -0.4169572  0.6011975  0.7711057\noutrage   -0.4029037  0.7076147  0.6188526\nangry     -0.4156885  0.7347953  0.6410278\nhappy      0.7420263 -0.4720350 -0.4745889\nworried   -0.4557817  0.6496327  0.7744649\nproud      1.0000000 -0.4472481 -0.4453810\nirritated -0.4472481  1.0000000  0.6175571\nnervous   -0.4453810  0.6175571  1.0000000\n\n###At least 2 distinct and possibly 3 distinct factors from examining \n\n\nResults indicate 2 and probably 3 unique factors + Positive Emotions: Happy, Pride, Hope\n\nResults indicate at least two and likely three distinct factors in the political emotions’ variables. Three factors would match the dominant belief in the literature on how political emotions are structured (Marcus et al. 2006) (Marcus et al. 2006). The three positive emotions are clearly positively related to each other with smaller and negative coefficients to the other six emotions. For the negative emotions, all six items are positively and significantly related to one another indicating they might all be measuring the same concept. However, closer examination of the results indicates that being afraid is more highly correlated with being worried or nervous than it is anger, outrage, or irritation. That matches the underlying theoretical belief that those three items represent an “anxiety about politics” factor, whereas anger, outrage, and irritation represent an “aversion” factor towards politics.\nConducting a factor analysis will help us better understand how these 9 individual political emotions are related to one another. We will conduct both exploratory and confirmatory factor analysis to illustrate these methods.\n\n\n\n\nMarcus, George E, Michael MacKuen, Jennifer Wolak, and Luke Keele. 2006. “The Measure and Mismeasure of Emotion.” In Feeling Politics: Emotion in Political Information Processing, 31–45. Springer."
  }
]