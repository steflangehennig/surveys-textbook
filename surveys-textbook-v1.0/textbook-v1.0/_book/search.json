[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Survey Methods for Social Scientists",
    "section": "",
    "text": "Welcome\nThis is the github repository for the work-in-progress first edition of Survey Methods for Social Scientists.\nThe goal of this book is to teach you how to create, conduct, and analyze surveys with R. While the book is primarily geared towards undergraduate students in the social sciences (political science, psychology, sociology, economics, etc.), the practical uses for this book can be extended to any discipline.\nThe book is divided into three sections:\n\nThe Science Behind Survey Methods\nBest Practices of Survey Design\nConducting Survey Analyses\n\nEach section comprises multiple chapters that succinctly explain the why behind the method, as well as how to apply the method. Built into each chapter are comprehensive R code tutorials alongside practical explanations."
  },
  {
    "objectID": "ch01intro.html",
    "href": "ch01intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "This book is geared toward undergraduates learning survey methodologies. This includes, but is not limited to, the psychology of the survey response process; creating and disseminating survey questions; survey modes; sampling and weighting; ethics; and survey analysis and reporting."
  },
  {
    "objectID": "ch01intro.html#learning-objectives",
    "href": "ch01intro.html#learning-objectives",
    "title": "1  Introduction",
    "section": "1.1 Learning Objectives",
    "text": "1.1 Learning Objectives"
  },
  {
    "objectID": "ch02psych_response.html",
    "href": "ch02psych_response.html",
    "title": "3  Psychology of Survey Response",
    "section": "",
    "text": "4 Psychology of Survey Response"
  },
  {
    "objectID": "ch02psych_response.html#learning-objectives",
    "href": "ch02psych_response.html#learning-objectives",
    "title": "2  Psychology of Survey Response Response",
    "section": "2.1 Learning Objectives",
    "text": "2.1 Learning Objectives"
  },
  {
    "objectID": "ch07weights.html",
    "href": "ch07weights.html",
    "title": "7  Survey Weighting",
    "section": "",
    "text": "Note: This tutorial uses the anesrake package to calculate the survey weights. They are many other packages to calculate weights so this is just one possible approach that could successfully be used to create survey weights.\nSurvey weights are widely used in survey research for a variety of purposes. In this tutorial, we will be focusing on one specific form of survey weights called a “rake weight”. Rake weights are typically used to make the survey sample match the target population on a set of demographic, and sometimes attitudinal, measures. They are used to ensure the sample’s demographics match the target population’s demographics. This numerical correction will change how much each individual case in your dataset is contributing to the overall, or sub-group, mean values across your sample data.\nFirst, we load necessary packages to compute and analyze the weights. If a package is not installed on your machine, you must first install it before this chunk of code will run."
  },
  {
    "objectID": "ch07weights.html#import-your-survey-data-into-r",
    "href": "ch07weights.html#import-your-survey-data-into-r",
    "title": "8  Survey Weighting",
    "section": "8.2 Import your survey data into R",
    "text": "8.2 Import your survey data into R\nWe need to import our survey data into R. The way we do this will vary by the format of your data, but in this case the data is saved as a “.dta” file so we will use the haven package to import it.\nYou should always examine your data and the base R function head shows the first 5 cases along with all of your column labels.\n\n\n [1] \"caseid\"         \"pid_4\"          \"ideo5\"          \"gov_choice\"    \n [5] \"prop_111\"       \"prop_112\"       \"trump_app\"      \"hick_app\"      \n [9] \"gardner_app\"    \"cong_app\"       \"scotus_app\"     \"pot_law\"       \n[13] \"gambling\"       \"fracking\"       \"gun_control\"    \"anger\"         \n[17] \"pride\"          \"hope\"           \"disgust\"        \"worry\"         \n[21] \"trump_app2\"     \"hick_app2\"      \"gardner_app2\"   \"cong_app2\"     \n[25] \"scotus_app2\"    \"pot_law2\"       \"gambling2\"      \"fracking2\"     \n[29] \"gun_control2\"   \"old_weight_old\" \"pid_x\"          \"sex\"           \n[33] \"race_4\"         \"speakspanish\"   \"marstat\"        \"child18\"       \n[37] \"employ\"         \"faminc_new\"     \"casscd\"         \"religiosity\"   \n[41] \"age_group\"      \"educ\""
  },
  {
    "objectID": "ch07weights.html#save-your-target-population-demographic-parameters",
    "href": "ch07weights.html#save-your-target-population-demographic-parameters",
    "title": "8  Survey Weighting",
    "section": "8.3 Save Your Target Population Demographic Parameters",
    "text": "8.3 Save Your Target Population Demographic Parameters\nYou will need to know the target population proportion for each of the variables you wish to weight your sample data on. How easy it will be to find your population values will be based on your specific target population.\nSome populations will be relatively easy to find (e.g. think adult demographic proportions in the United States from the Census, CPS, or ACS results and all the sub-geographic levels that accompany them), but others won’t be as easy. Sometimes, you cannot know your target population proportions so in those cases you will not be able to weight your survey sample data.\nIn this chunk of R code, we are creating the target population parameters for two specific demographic variables measured in our sample political poll data. This was a political poll conducted in October 2018 with the sample consisting of likely Colorado voters in the then upcoming 2018 election. This gubernatorial election year poll measured multiple things including: - 2018 Colorado Gubernatorial Preference - Jared Polis or Walker Stapleton - Policy Questions: Marijuana Legalization, Fracking, Gun Control laws - Approval ratings: President Donald Trump, Governor John Hickenlooper (at the time), US Congress - Demographic questions for survey weighting purposes\n\n8.3.1 Saving New Vectors With Target Population Demographic Values\nUsing this data, let’s create some survey weights. To illustrate the principle, we will start with basic weights using just two demographic variables commonly used in calculating survey weights: sex (unfortunately only biological sex was collected in this survey) and age (split into 5 categories). We must save a vector of data with the target population demographic proportions, so in this case we will save two vectors one called sex and one called age_group.\nThere are two critical things to get correct in this step.\n1. Matching Names The names we give these vectors matter and must match the names of the appropriate demographic variable in your sample data. Since the vector names we chose were sex and age_group, the variable names in the sample data must be exactly sex and age_group. Otherwise, the code will not be able to match the two and will fail.\n2. Matching Orders The second critical thing to get correct is the order the proportion values are entered into the vector must match the order the proportion values are stored in the sample data. In this example, the order of proportions stored in the sex variable in the sample data is (female, male) so the values we give the sex vector must be in that exact order as well. The same is true for the age_group variable, which has 5 groups in the sample data: 18-29, 30-39, 40-49, 50-64, 65+. The proportion order in our vector for the age_group must match that exactly as well otherwise you are creating incorrect weights or best-case scenario getting an error message.\n\nsex &lt;- c(.525, .475)  #Target values for females and males; label order (female, male)\nsum(sex) #proportions should = 1 so this checks that it does\n\n[1] 1\n\nage_group  &lt;- c(.182, .203, .17, .218, .227)  #Target values for 5 age groups; 18-29, 30-39, 40-49, 50-64, 65+\nsum(age_group) #proportions should = 1 so this checks that it does\n\n[1] 1\n\n\nFirst, let’s look at the unweighted values in both the age and sex variables.\n\n#Shows the unweighted size of the age_group demographics\nsample %&gt;%\n  group_by(age_group) %&gt;%\n  summarise(n = n()) %&gt;% \n  mutate(proportion = n /sum(n))\n\n# A tibble: 5 × 3\n  age_group     n proportion\n  &lt;dbl+lbl&gt; &lt;int&gt;      &lt;dbl&gt;\n1 1 [18-29]    33     0.0412\n2 2 [30-39]   101     0.126 \n3 3 [40-49]   118     0.148 \n4 4 [50-64]   306     0.382 \n5 5 [65+]     242     0.302 \n\n#Shows the unweighted size of the sex demographics\nsample %&gt;%\n  group_by(sex) %&gt;%\n  summarise(n = n()) %&gt;% \n  mutate(proportion = n /sum(n))\n\n# A tibble: 2 × 3\n  sex            n proportion\n  &lt;dbl+lbl&gt;  &lt;int&gt;      &lt;dbl&gt;\n1 1 [Female]   423      0.529\n2 2 [Male]     377      0.471"
  },
  {
    "objectID": "ch07weights.html#calculating-your-rake-weights",
    "href": "ch07weights.html#calculating-your-rake-weights",
    "title": "8  Survey Weighting",
    "section": "8.4 Calculating Your Rake Weights",
    "text": "8.4 Calculating Your Rake Weights\nNow that we have our target population parameters saved in vectors for each demographic variable we plan to weight our survey data on and our sample data with matching names and orders, we can begin to create our survey weights.\n1. Create List First, we create a list that merges the two demographic vectors for use in the raking process. Remember, this names must match the column names in the sample data. We give this list the name of targets to reflect this is the target population parameters we want to match the sample data to. We then give the column names to match with the sample data.\n2. Calculate the Weights Now, it is time to create some survey weights using the anesrake function. This function has many possible items that could be used, with all the possible items listed in the following R chunk. You should view the R documentation for all possible things it can do.\nFor our purposes, we will be focusing on a few things that will be noted. We will calculate a new dataframe called myweights where we input the targets list, the name of our sample data cpc, a caseid value that uniquely identifies each case, the cap item tells the function to cape the size of the survey weights at 8 and not allow any case to have a weight larger than that value. The type item tells the function how it should handle, if at all, a target population demographic that is very close to the sample value for that same demographic.\nYou’ll see in the output once you run the anesrake function how many iterations it took for the raking to converge on this specific set of weights. Here, it took 15 iterations across the two target demographic variables.\n3. Save Weights in Sample Data Next, we save that newly created weight as a new variable in our existing sample data, and now we have a weight variable that we can use in our analysis of the data.\n\n#Now we save these values as a list and call the list targets\n#Step 1: Save the target list \ntargets &lt;- list(sex, age_group)\n# remember, these names will have to match\nnames(targets) &lt;- c(\"sex\", \"age_group\")\n\n#anesrake(targets, dataframe, caseid, weightvec = NULL, cap = 5,\n#verbose = FALSE, maxit = 1000, type = \"pctlim\", pctlim = 5,\n#nlim = 5, filter = 1, choosemethod = \"total\", iterate = TRUE)\n\n#Step 2 - Calculate the Rake Weight\nset.seed(1599385) #Set the seed for replication  \nmyweights &lt;- anesrake(targets, sample, \n                      caseid = sample$caseid, cap = 8, type = \"nolim\", pctlim=.05)\n\n[1] \"Raking converged in 12 iterations\"\n\n#Step 3 - Save the Rake Weight at the end of your sample data\nsample$weight  &lt;- unlist(myweights[1])"
  },
  {
    "objectID": "ch07weights.html#reviewing-the-newly-created-survey-weights",
    "href": "ch07weights.html#reviewing-the-newly-created-survey-weights",
    "title": "8  Survey Weighting",
    "section": "8.5 Reviewing the Newly Created Survey Weights",
    "text": "8.5 Reviewing the Newly Created Survey Weights\nBefore we start the analysis of the weighted data, let’s examine the newly created survey weights saved in our sample data.\nWith only 2 target weighting variables with 10 total categories combined between them, we can examine the weights individually by group. To do this, we will use the srvyr package to examine the weight size by the target groups.\n\n#Displays summary of the weight size to see range\nsummary(sample$weight)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.5347  0.6086  0.6997  1.0000  1.0912  4.8008 \n\n#Shows the weight size by demographic groups used in the weighting scheme\nsample %&gt;% \n  as_survey(weights = c(weight)) %&gt;%\n  group_by(sex, age_group) %&gt;% \n  summarise(weight = survey_mean(weight, na.rm = T))\n\n# A tibble: 10 × 4\n# Groups:   sex [2]\n   sex        age_group weight weight_se\n   &lt;dbl+lbl&gt;  &lt;dbl+lbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;\n 1 1 [Female] 1 [18-29]  4.22   0       \n 2 1 [Female] 2 [30-39]  1.51   2.86e-17\n 3 1 [Female] 3 [40-49]  1.09   0       \n 4 1 [Female] 4 [50-64]  0.535  0       \n 5 1 [Female] 5 [65+]    0.700  0       \n 6 2 [Male]   1 [18-29]  4.80   0       \n 7 2 [Male]   2 [30-39]  1.72   3.22e-17\n 8 2 [Male]   3 [40-49]  1.24   0       \n 9 2 [Male]   4 [50-64]  0.609  0       \n10 2 [Male]   5 [65+]    0.796  0       \n\n\nNow we see the weight size for each of the 10 groups that we weighted our sample data on. Obviously, with more demographic variables including in the weighting scheme this list would get much more cumbersome but for pedagogical purposes it is important to look at these values to understand their meaning.\nFor females between the age of 18-29 the weight equals 4.21. This means that females between the ages of 18-29 are under-represented in the sample data since the value is over 1. Fundamentally what this means is that for each female between the ages of 18-29 in the sample data, they are “speaking” for 4.21 females between the age of 18-29 from the target population. Compare this value to females between the ages of 50 and 64 (group 4) who have a weight value of .53. This means that females in this age group were over-represented in the sample data since the weight value is under 1.\nWe should also look at the weighted demographic values to ensure the weights worked as we hope they do - i.e. the weighted sample demographic values match the target population values.\n\n#Shows the unweighted size of the age_group demographics\nsample %&gt;%\n  group_by(age_group) %&gt;%\n  summarise(n = n()) %&gt;% \n  mutate(proportion = n /sum(n))\n\n# A tibble: 5 × 3\n  age_group     n proportion\n  &lt;dbl+lbl&gt; &lt;int&gt;      &lt;dbl&gt;\n1 1 [18-29]    33     0.0412\n2 2 [30-39]   101     0.126 \n3 3 [40-49]   118     0.148 \n4 4 [50-64]   306     0.382 \n5 5 [65+]     242     0.302 \n\n#Shows the weighted size of the age_group demographics\nsample %&gt;%\n  as_survey(weights = c(weight)) %&gt;%\n  group_by(age_group) %&gt;%\n  summarise(n = survey_total()) %&gt;% \n  mutate(proportion = n /sum(n))\n\n# A tibble: 5 × 4\n  age_group     n  n_se proportion\n  &lt;dbl+lbl&gt; &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt;\n1 1 [18-29]  146. 24.9       0.182\n2 2 [30-39]  162. 15.2       0.203\n3 3 [40-49]  136  11.6       0.17 \n4 4 [50-64]  174.  7.87      0.218\n5 5 [65+]    182.  9.78      0.227\n\n#Saves the weighted & unweighted size of the age_group demographics\nag_w&lt;-sample %&gt;%\n  as_survey(weights = c(weight)) %&gt;%\n  group_by(age_group) %&gt;%\n  summarise(n = survey_total()) %&gt;% \n  mutate(weighted_sample = n /sum(n))\n\nag_uw&lt;- sample %&gt;%\n  group_by(age_group) %&gt;%\n  summarise(n = n()) %&gt;% \n  mutate(unweighted_sample = n /sum(n))\nag_combo&lt;-left_join(ag_w, ag_uw, by = \"age_group\", suffix = c(\"\", \"_pop\")) %&gt;%\n  group_by(age_group)\n\nag_combo$ag_diff_per&lt;- 100*(ag_combo$weighted_sample-ag_combo$unweighted_sample)\nag_combo\n\n# A tibble: 5 × 7\n# Groups:   age_group [5]\n  age_group     n  n_se weighted_sample n_pop unweighted_sample ag_diff_per\n  &lt;dbl+lbl&gt; &lt;dbl&gt; &lt;dbl&gt;           &lt;dbl&gt; &lt;int&gt;             &lt;dbl&gt;       &lt;dbl&gt;\n1 1 [18-29]  146. 24.9            0.182    33            0.0412       14.1 \n2 2 [30-39]  162. 15.2            0.203   101            0.126         7.68\n3 3 [40-49]  136  11.6            0.17    118            0.148         2.25\n4 4 [50-64]  174.  7.87           0.218   306            0.382       -16.4 \n5 5 [65+]    182.  9.78           0.227   242            0.302        -7.55\n\nprint(ag_combo$weighted_sample)\n\n[1] 0.182 0.203 0.170 0.218 0.227\n\nprint(targets)\n\n$sex\n[1] 0.525 0.475\n\n$age_group\n[1] 0.182 0.203 0.170 0.218 0.227\n\n\nWe see that the weighted age_group values match the target population values we inputted earlier so this weighting scheme seems to be working in the way that we hoped it would.\n\n#Shows the unweighted size of the sex demographics\nsample %&gt;%\n  group_by(sex) %&gt;%\n  summarise(n = n()) %&gt;% \n  mutate(proportion = n /sum(n))\n\n# A tibble: 2 × 3\n  sex            n proportion\n  &lt;dbl+lbl&gt;  &lt;int&gt;      &lt;dbl&gt;\n1 1 [Female]   423      0.529\n2 2 [Male]     377      0.471\n\n#Shows the weighted size of the sex demographics\nsample %&gt;%\n  as_survey(weights = c(weight)) %&gt;%\n  group_by(sex) %&gt;%\n  summarise(n = survey_total()) %&gt;% \n  mutate(proportion = n /sum(n))\n\n# A tibble: 2 × 4\n  sex            n  n_se proportion\n  &lt;dbl+lbl&gt;  &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt;\n1 1 [Female]   420  22.0      0.525\n2 2 [Male]     380  20.4      0.475\n\n\nFor the sex measure, the unweighted and weighted values matched nearly identically, but that is because the unweighted sample nearly matched the target population identically without any statistical correction. In these instances, it is common to drop the demographic variable that does not need much adjustment. The standard limit is 5% or less should not get an adjustment meaning that you should only apply weights with the target population values and the sample values are 5% points or more different."
  },
  {
    "objectID": "ch07weights.html#evaluating-influence-of-weights-on-reported-mean-values-in-the-sample-data---smallish-weights",
    "href": "ch07weights.html#evaluating-influence-of-weights-on-reported-mean-values-in-the-sample-data---smallish-weights",
    "title": "8  Survey Weighting",
    "section": "8.6 Evaluating Influence of Weights on Reported Mean Values in the Sample Data - Smallish Weights",
    "text": "8.6 Evaluating Influence of Weights on Reported Mean Values in the Sample Data - Smallish Weights\nNow let’s see what impact these weights have on our sample values. First, we use the surveys package must create a new dataframe that incorporates the survey weights. Let’s call it sample.weighted to signal that this is the weighted version of the sample data.\nWe need to calculate the weighted and unweighted means of the same variable. Using the fracking2 variable which measures support for fracking in the Colorado which various safety measures, we can compare the influence of the survey weights on the conclusions we would draw about support for fracking in Colorado.\nOnce we run the following R chunk, we see that there is virtually no difference between the weighted and unweighted estimates of how supportive Coloradoans are of fracking. Why is this? This occurs sometimes when the weights that applied simply do not change the sample composition enough to have an influence on the overall sample mean.\n\nfracking_uw&lt;-sample %&gt;% #Looks at the unweighted support for fracking in CO\n  summarise(unweight_support = mean(fracking2, na.rm = T))\n\nfracking_w&lt;-sample %&gt;% #Looks at the weighted support for fracking in CO\n  as_survey(weights = c(weight)) %&gt;%\n   summarise(weight_support = survey_mean(fracking2, na.rm = T))\n\n\nfracking_combo&lt;-cbind(fracking_uw, fracking_w ) \n\nfracking_combo &lt;- mutate(fracking_combo, difference = weight_support - unweight_support)\nfracking_combo\n\n  unweight_support weight_support weight_support_se  difference\n1        0.5277778      0.5025514         0.0229558 -0.02522642\n\n#Gubernatorial Vote Choice - Weighted & Unweighted \ngov_w&lt;-sample %&gt;%\n  as_survey(weights = c(weight)) %&gt;%\n  filter(!is.na(gov_choice)) %&gt;% \n  group_by(gov_choice) %&gt;%\n  summarise(n = survey_total()) %&gt;% \n  mutate(weight_support = n /sum(n)) \n\ngov_uw&lt;-sample %&gt;%\n  group_by(gov_choice) %&gt;%\n  filter(!is.na(gov_choice)) %&gt;% \n  summarise(n = n()) %&gt;% \n  mutate(unweight_support = n /sum(n))\n\ngov_combo&lt;-cbind(gov_uw, gov_w) \n\n\ngov_combo$diff &lt;- gov_combo$weight_support - gov_combo$unweight_support\ngov_combo\n\n  gov_choice   n unweight_support gov_choice         n      n_se weight_support\n1          1 413          0.51625          1 438.30863 22.858613     0.54788579\n2          2 346          0.43250          2 312.66152 17.843921     0.39082691\n3          3  30          0.03750          3  31.10659  6.029889     0.03888324\n4          4  11          0.01375          4  17.92325  7.817047     0.02240406\n          diff\n1  0.031635792\n2 -0.041673095\n3  0.001383243\n4  0.008654059"
  },
  {
    "objectID": "ch07weights.html#create-new-weighting-scheme-that-incorporates-more-demographic-variables",
    "href": "ch07weights.html#create-new-weighting-scheme-that-incorporates-more-demographic-variables",
    "title": "8  Survey Weighting",
    "section": "8.7 Create New Weighting Scheme That Incorporates More Demographic Variables",
    "text": "8.7 Create New Weighting Scheme That Incorporates More Demographic Variables\nTypically, when creating survey weights you will include more than just 2 demographic variables into your weighting scheme. Here, we use 5 variables to create a new weight: sex, age, race/ethnicity, education, and partisanship.\n\n#Save new vectors with target population values for weights \nsex &lt;- c(.525, .475)  ##Target values for females and males; label order (female, male)\nage_group  &lt;- c(.132, .183, .15, .248, .287)   #Target values for 5 age groups \nrace_4 &lt;-c(.7143, .0501, .1768, .0588) #Target values race/ethnic identities - white, black, Hispanic, all others\neduc &lt;-c(.2075, .2445, .0828, .2398, .2254) #Target values education - HS or less, Some college, AA, BA, Graduate degree\npid_4 &lt;-c(.3375, .2838, .335, .0437) #Target values Party Registration - (Democrats, Independents, Republicans, All 3rd Parties)  \n\n#Combine the demographic vectors into a list\ntargets &lt;- list(sex, age_group, race_4, educ, pid_4)\n# remember, these names will have to match the column names & order in the sample data \nnames(targets) &lt;- c(\"sex\", \"age_group\", \"race_4\", \"educ\", \"pid_4\")\n\nset.seed(1984)\nmyweights &lt;- anesrake(targets, sample, \n                      caseid = sample$caseid, cap = 8, type = \"pctlim\", pctlim=.05)    \n\n[1] \"Raking converged in 20 iterations\"\n[1] \"Raking converged in 21 iterations\"\n\nsample$full_weight  &lt;- unlist(myweights[1])\n\nsummary(sample$full_weight)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.3218  0.5685  0.7672  1.0000  1.0729  7.9999 \n\n\nLet’s look at how well the weights worked to match the sample data to the target population values for the race4 and educ5 measures.\n\n#Shows the weighted size of the educ5 demographics\n##We can also bind the two values together to calculate their differences \ned_w&lt;-sample %&gt;%\n  as_survey(weights = c(full_weight)) %&gt;%\n  group_by(educ) %&gt;%\n  summarise(n = survey_total()) %&gt;% \n  mutate(weighted_sample = n /sum(n))\n\ned_uw&lt;- sample %&gt;%\n  group_by(educ) %&gt;%\n  summarise(n = n()) %&gt;% \n  mutate(unweighted_sample = n /sum(n))\ned_combo&lt;-left_join(ed_w, ed_uw, by = \"educ\") %&gt;%\n  group_by(educ)\n\ned_combo$ed_diff_per&lt;- 100*(ed_combo$weighted_sample-ed_combo$unweighted_sample)\ned_combo\n\n# A tibble: 5 × 7\n# Groups:   educ [5]\n  educ             n.x  n_se weighted_sample   n.y unweighted_sample ed_diff_per\n  &lt;dbl+lbl&gt;      &lt;dbl&gt; &lt;dbl&gt;           &lt;dbl&gt; &lt;int&gt;             &lt;dbl&gt;       &lt;dbl&gt;\n1 1 [HS grad or… 166.  22.1           0.208     92             0.115       9.25 \n2 2 [Some colle… 196.  17.2           0.244    187             0.234       1.07 \n3 3 [AA degree]   66.2  7.29          0.0828    96             0.12       -3.72 \n4 4 [BA]         192.  14.3           0.240    249             0.311      -7.15 \n5 5 [Post-grad]  180.  14.3           0.225    176             0.22        0.540\n\n#Shows the weighted size of the race4 demographics\n##We can also bind the two values together to calculate their differences \nr_w&lt;-sample %&gt;%\n  as_survey(weights = c(full_weight)) %&gt;%\n  group_by(race_4) %&gt;%\n  summarise(n = survey_total()) %&gt;% \n  mutate(weighted_sample = n /sum(n))\n\nr_uw&lt;- sample %&gt;%\n  group_by(race_4) %&gt;%\n  summarise(n = n()) %&gt;% \n  mutate(unweighted_sample = n /sum(n))\nr_combo&lt;-left_join(r_w, r_uw, by = \"race_4\") %&gt;%\n  group_by(race_4)\n\nr_combo$r_diff_per&lt;- 100*(r_combo$weighted_sample-r_combo$unweighted_sample)\nr_combo\n\n# A tibble: 4 × 7\n# Groups:   race_4 [4]\n  race_4          n.x  n_se weighted_sample   n.y unweighted_sample r_diff_per\n  &lt;dbl+lbl&gt;     &lt;dbl&gt; &lt;dbl&gt;           &lt;dbl&gt; &lt;int&gt;             &lt;dbl&gt;      &lt;dbl&gt;\n1 1 [White]     571.  14.0           0.714    692            0.865     -15.1  \n2 2 [Black]      40.1  9.97          0.0501    22            0.0275      2.26 \n3 3 [Hispanic]  141.  25.1           0.177     41            0.0512     12.6  \n4 4 [Other POC]  47.0  7.56          0.0588    45            0.0562      0.255\n\n#Shows the weighted size of the pid_4 registered voter variable \n##We can also bind the two values together to calculate their differences \npid_w&lt;-sample %&gt;%\n  as_survey(weights = c(full_weight)) %&gt;%\n  group_by(pid_4) %&gt;%\n  summarise(n = survey_total()) %&gt;% \n  mutate(weighted_sample = n /sum(n))\n\npid_uw&lt;- sample %&gt;%\n  group_by(pid_4) %&gt;%\n  summarise(n = n()) %&gt;% \n  mutate(unweighted_sample = n /sum(n))\npid_combo&lt;-left_join(pid_w, pid_uw, by = \"pid_4\") %&gt;%\n  group_by(pid_4)\n\npid_combo$pid_diff_per&lt;- 100*(pid_combo$weighted_sample-pid_combo$unweighted_sample)\npid_combo\n\n# A tibble: 4 × 7\n# Groups:   pid_4 [4]\n  pid_4           n.x  n_se weighted_sample   n.y unweighted_sample pid_diff_per\n  &lt;dbl+lbl&gt;     &lt;dbl&gt; &lt;dbl&gt;           &lt;dbl&gt; &lt;int&gt;             &lt;dbl&gt;        &lt;dbl&gt;\n1 1 [Democrat]  270.  21.1           0.338    283            0.354        -1.62 \n2 2 [Independe… 227.  17.2           0.284    214            0.268         1.63 \n3 3 [Republica… 268.  19.8           0.335    266            0.332         0.250\n4 4 [Other]      35.0  6.55          0.0437    37            0.0462       -0.255\n\n\n\n#Gubernatorial Vote Choice - Weighted & Unweighted \ngov_w&lt;-sample %&gt;%\n  as_survey(weights = c(full_weight)) %&gt;%\n  filter(!is.na(gov_choice)) %&gt;% \n  group_by(gov_choice) %&gt;%\n  summarise(n = survey_total()) %&gt;% \n  mutate(weight_support = n /sum(n)) \n\ngov_uw&lt;-sample %&gt;%\n  group_by(gov_choice) %&gt;%\n  filter(!is.na(gov_choice)) %&gt;% \n  summarise(n = n()) %&gt;% \n  mutate(unweight_support = n /sum(n))\n\ngov_combo&lt;-cbind(gov_uw, gov_w) \n\n\ngov_combo$diff &lt;- gov_combo$weight_support - gov_combo$unweight_support\n\ngov_outcome&lt;-cbind(gov_combo$gov_choice, gov_combo$weight_support, gov_combo$unweight_support, gov_combo$diff) \n\ncolnames(gov_outcome) &lt;- c(\"candidate\", \"weighted support\", \"unewighted support\", \"diff\") \ngov_outcome\n\n     candidate weighted support unewighted support         diff\n[1,]         1       0.50298793            0.51625 -0.013262068\n[2,]         2       0.43459577            0.43250  0.002095774\n[3,]         3       0.04328807            0.03750  0.005788066\n[4,]         4       0.01912823            0.01375  0.005378227\n\n####Comparing weighted to unweighted fracking support in Colorado \n\nfracking_uw&lt;-sample %&gt;% #Looks at the unweighted support for fracking in CO\n  summarise(unweight_support = mean(fracking2, na.rm = T))\n\nfracking_w&lt;-sample %&gt;% #Looks at the weighted support for fracking in CO\n  as_survey(weights = c(full_weight)) %&gt;%\n   summarise(weight_support = survey_mean(fracking2, na.rm = T))\n\n\nfracking_combo&lt;-cbind(fracking_uw, fracking_w ) \n\nfracking_combo &lt;- mutate(fracking_combo, frack_diff = weight_support - unweight_support)\n\n#Do the same analysis for gun control then pot laws\ngc_uw&lt;-sample %&gt;% #Looks at the unweighted support for fracking in CO\n  summarise(unweight_support = mean(gun_control2, na.rm = T))\n\ngc_w&lt;-sample %&gt;% #Looks at the weighted support for fracking in CO\n  as_survey(weights = c(full_weight)) %&gt;%\n   summarise(weight_support = survey_mean(gun_control2, na.rm = T))\n\n\ngc_combo&lt;-cbind(gc_uw, gc_w ) \n\ngc_combo &lt;- mutate(gc_combo, gun_control_diff = weight_support - unweight_support)\n\npl_uw&lt;-sample %&gt;% #Looks at the unweighted support for fracking in CO\n  summarise(unweight_support = mean(pot_law2, na.rm = T))\n\npl_w&lt;-sample %&gt;% #Looks at the weighted support for fracking in CO\n  as_survey(weights = c(full_weight)) %&gt;%\n   summarise(weight_support = survey_mean(pot_law2, na.rm = T))\n\n\npl_combo&lt;-cbind(pl_uw, pl_w ) \n\npl_combo &lt;- mutate(pl_combo, pot_law_diff = weight_support - unweight_support)\n\n\ncombo&lt;-cbind(100*(fracking_combo$frack_diff),100*(gc_combo$gun_control_diff), 100*(pl_combo$pot_law_diff)) \n\ncolnames(combo) &lt;- c(\"fracking_diff\", \"gun_control_diff\", \"pot_law_diff\") \ncombo&lt;-round(combo,3)\ncombo\n\n     fracking_diff gun_control_diff pot_law_diff\n[1,]         2.651            0.531       -0.303\n\n\nAbove, shows the impact that the new weighting scheme had on the differences in support for fracking, new gun control policies, support for marijuana legalization and a tax revenue proposition on the upcoming ballot. For fracking, the weighted sample supported fracking 3.3 percentage points higher than the unweighted sample. While that might not seem like a large difference, in a polarized American electorate 3.3 percentage points easily be the differences between winning an election or going down in defeat.\nThe other three items all saw a decrease in support in the weighted data. Overall, the results showed increased support for a generally conservative supported issue, fracking, while revealing decreased support for 3 more liberal supported issues, gun control, marijuana legalization, and increased governmental spending. This is likely caused by weighting older voters to be\nWe can also use the newly created survey weights in regression analyses. To do so, you first must create a new weighted survey dataset that you then conduct the analysis on.\n\n#Using srvyr package to create a new weighted dataset for analysis purposes\n#Step 1: Create Weighted Survey Data for Analysis\nsample.weighted &lt;- sample %&gt;% \n  as_survey_design(ids = 1, # 1 for no cluster ids; use this for a simple random sample \n                   weights = full_weight, # No weight added\n                   strata = NULL # sampling was simple (no strata) \n                  )\n\nnonweighted &lt;-lm(gambling ~ pid_x + ideo5 + sex , data=sample)\nweighted &lt;-lm(gambling ~ pid_x + ideo5 + sex, data=sample, weights=sample$weight)\nweighted2 &lt;-lm(gambling ~ pid_x + ideo5 + sex, data=sample, weights=sample$full_weight)\n\nstargazer(nonweighted, weighted,  weighted2, type=\"text\")\n\n\n============================================================\n                                    Dependent variable:     \n                               -----------------------------\n                                         gambling           \n                                  (1)       (2)       (3)   \n------------------------------------------------------------\npid_x                           -0.017    -0.010    -0.013  \n                                (0.031)   (0.032)   (0.029) \n                                                            \nideo5                           -0.059    -0.065   -0.096** \n                                (0.051)   (0.051)   (0.047) \n                                                            \nsex                             0.184**  0.215***   0.188** \n                                (0.080)   (0.080)   (0.079) \n                                                            \nConstant                       2.391***  2.441***  2.591*** \n                                (0.153)   (0.154)   (0.157) \n                                                            \n------------------------------------------------------------\nObservations                      669       669       669   \nR2                               0.015     0.018     0.025  \nAdjusted R2                      0.011     0.013     0.021  \nResidual Std. Error (df = 665)   1.030     1.020     0.998  \nF Statistic (df = 3; 665)       3.481**  4.032***  5.678*** \n============================================================\nNote:                            *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01\n\n\nResults are largely similar across the regression models, but there are some slight differences between the unweighted model and the two weighted ones. This happens because certain respondent’s opinions are being given more or less weight to the overall average relationship which can cause the conclusions you draw from your analysis to differ. This is one reason why it is critically important to create your survey weights using defensible target population values."
  },
  {
    "objectID": "ch07weights.html#concluding-thoughts",
    "href": "ch07weights.html#concluding-thoughts",
    "title": "8  Survey Weighting",
    "section": "8.8 Concluding Thoughts",
    "text": "8.8 Concluding Thoughts\nThis is an important lesson for the application of the survey weights. The target population values that you weight your survey sample data to match can have profound implications on the conclusions you and others draw from your survey results. In the case, the decision to weight the survey to give more voice to the Republican members of the sample influenced the conclusions drawn about support for various policies being debated in the public realm. This makes it critically important to make sure that the target population values that are chosen are as accurate as possible and publicly defensible.\nOverall, this tutorial has taken you through how to calculate survey weights using the anesrake package. Using a sample political poll, you hopefully learned how to create target demographic population vectors, which then merge with our sample demographic values. Following this, you learned how to calculate directly survey weights, evaluate the success/failure of the survey weighting process, and compare the impact of using the survey weight on the conclusions drawn from the results.\n\nsessionInfo()\n\nR version 4.3.0 (2023-04-21 ucrt)\nPlatform: x86_64-w64-mingw32/x64 (64-bit)\nRunning under: Windows 10 x64 (build 19044)\n\nMatrix products: default\n\n\nlocale:\n[1] LC_COLLATE=English_United Kingdom.utf8 \n[2] LC_CTYPE=English_United Kingdom.utf8   \n[3] LC_MONETARY=English_United Kingdom.utf8\n[4] LC_NUMERIC=C                           \n[5] LC_TIME=English_United Kingdom.utf8    \n\ntime zone: America/Denver\ntzcode source: internal\n\nattached base packages:\n[1] grid      stats     graphics  grDevices utils     datasets  methods  \n[8] base     \n\nother attached packages:\n [1] stargazer_5.2.3   data.table_1.14.8 anesrake_0.80     weights_1.0.4    \n [5] Hmisc_5.1-0       survey_4.2-1      survival_3.5-5    Matrix_1.5-4     \n [9] srvyr_1.2.0       poliscidata_2.3.0 skimr_2.1.5       lubridate_1.9.2  \n[13] forcats_1.0.0     stringr_1.5.0     purrr_1.0.1       readr_2.1.4      \n[17] tibble_3.2.1      tidyverse_2.0.0   knitr_1.43        tidyr_1.3.0      \n[21] dplyr_1.1.2       ggplot2_3.4.2     haven_2.5.2      \n\nloaded via a namespace (and not attached):\n [1] tidyselect_1.2.0   bitops_1.0-7       fastmap_1.1.1      digest_0.6.32     \n [5] rpart_4.1.19       timechange_0.2.0   lifecycle_1.0.3    cluster_2.1.4     \n [9] gdata_2.19.0       magrittr_2.0.3     compiler_4.3.0     rlang_1.1.1       \n[13] tools_4.3.0        plotrix_3.8-2      utf8_1.2.3         htmlwidgets_1.6.2 \n[17] plyr_1.8.8         repr_1.1.6         abind_1.4-5        KernSmooth_2.23-20\n[21] withr_2.5.0        foreign_0.8-84     nnet_7.3-18        fansi_1.0.4       \n[25] jomo_2.7-6         caTools_1.18.2     xtable_1.8-4       colorspace_2.1-0  \n[29] mice_3.16.0        MASS_7.3-58.4      scales_1.2.1       gtools_3.9.4      \n[33] iterators_1.0.14   cli_3.6.1          crayon_1.5.2       rmarkdown_2.23    \n[37] generics_0.1.3     rstudioapi_0.14    tzdb_0.4.0         minqa_1.2.5       \n[41] DBI_1.1.3          splines_4.3.0      base64enc_0.1-3    mitools_2.4       \n[45] vctrs_0.6.3        boot_1.3-28.1      glmnet_4.1-7       jsonlite_1.8.5    \n[49] carData_3.0-5      car_3.1-2          hms_1.1.3          mitml_0.4-5       \n[53] Formula_1.2-5      htmlTable_2.4.1    foreach_1.5.2      glue_1.6.2        \n[57] pan_1.8            nloptr_2.0.3       codetools_0.2-19   stringi_1.7.12    \n[61] gtable_0.3.3       shape_1.4.6        lme4_1.1-33        munsell_0.5.0     \n[65] pillar_1.9.0       htmltools_0.5.5    gplots_3.1.3       R6_2.5.1          \n[69] evaluate_0.21      lattice_0.21-8     backports_1.4.1    broom_1.0.5       \n[73] descr_1.1.7        Rcpp_1.0.10        nlme_3.1-162       gridExtra_2.3     \n[77] checkmate_2.2.0    xfun_0.39          pkgconfig_2.0.3"
  },
  {
    "objectID": "ch09hyp_analysis_rep.html#hypothesis-testing",
    "href": "ch09hyp_analysis_rep.html#hypothesis-testing",
    "title": "9  Hypothesis Testing, Analysis, & Reporting",
    "section": "9.2 Hypothesis Testing",
    "text": "9.2 Hypothesis Testing"
  },
  {
    "objectID": "ch09hyp_analysis_rep.html#analysis",
    "href": "ch09hyp_analysis_rep.html#analysis",
    "title": "9  Hypothesis Testing, Analysis, & Reporting",
    "section": "9.3 Analysis",
    "text": "9.3 Analysis"
  },
  {
    "objectID": "ch09hyp_analysis_rep.html#reporting",
    "href": "ch09hyp_analysis_rep.html#reporting",
    "title": "9  Hypothesis Testing, Analysis, & Reporting",
    "section": "9.4 Reporting",
    "text": "9.4 Reporting"
  },
  {
    "objectID": "ch07weights.html#step-by-step-guide-to-creating-basic-rake-weights-in-r",
    "href": "ch07weights.html#step-by-step-guide-to-creating-basic-rake-weights-in-r",
    "title": "8  Survey Weighting",
    "section": "8.1 Step-By-Step Guide to Creating Basic Rake Weights in R",
    "text": "8.1 Step-By-Step Guide to Creating Basic Rake Weights in R\nNote: This tutorial uses the anesrake package to calculate the survey weights. They are many other packages to calculate weights so this is just one possible approach that could successfully be used to create survey weights.\nSurvey weights are widely used in survey research for a variety of purposes. In this tutorial, we will be focusing on one specific form of survey weights called a “rake weight”. Rake weights are typically used to make the survey sample match the target population on a set of demographic, and sometimes attitudinal, measures. They are used to ensure the sample’s demographics match the target population’s demographics. This numerical correction will change how much each individual case in your dataset is contributing to the overall, or sub-group, mean values across your sample data.\nFirst, we load necessary packages to compute and analyze the weights. If a package is not installed on your machine, you must first install it before this chunk of code will run."
  },
  {
    "objectID": "part_intro.html#section-outline",
    "href": "part_intro.html#section-outline",
    "title": "The Science of Survey Methods",
    "section": "Section Outline",
    "text": "Section Outline\nIn addition to getting our working environment set up to conduct survey analyses, this section introduces students to the science behind survey methods, including how individuals approach and answer surveys, as well as the ethics behind designing, administering, and synthesizing surveys.\n\nChapter 1: Introduction to Survey Methods\nChapter 2: Setting Up our Environment for Survey Methods\nChapter 3: Psychology of Survey Response\nChapter 4: Survey Ethics"
  },
  {
    "objectID": "ch00setup.html#cleaning-and-recoding-data-prior-to-analysis",
    "href": "ch00setup.html#cleaning-and-recoding-data-prior-to-analysis",
    "title": "2  Data Cleaning and Recoding",
    "section": "2.1 Cleaning and Recoding Data Prior to Analysis",
    "text": "2.1 Cleaning and Recoding Data Prior to Analysis\nAnytime you import a survey, or really any kind of data into R, you should always ask yourself what you need to do to get it prepared for analysis. With survey data specifically, there are usually a few things that should always be checked and changed prior to conducting any type of analysis.\nWe will cover several of them here, including the following:\n\nReordering a survey scale so higher values = more of something\nReorder incorrectly coded ordinal scales\nCombine multiple questions into one response\nChange the name of a variable to indicate what it measures\nCombine multiple options into fewer groups\nSetting specific values to NAs\nApplying survey weights"
  },
  {
    "objectID": "ch00setup.html#importing-data",
    "href": "ch00setup.html#importing-data",
    "title": "2  Data Cleaning and Recoding",
    "section": "2.2 Importing Data",
    "text": "2.2 Importing Data\nWe will be importing data from a Stata file downloaded from the American National Election Survey website for the 2020 presidential election in the United States. This survey interviewed over 8,000 US residents asking over 1,000 questions combined between a pre and post survey.\n\n\n\n\n\n\nStarting a New Analysis\n\n\n\nYou should always start a new analysis with a clean data set, meaning one that has not been previously altered. This allows for easier replication by other researchers and ensures that all of the changes made to variables and cases in your data set can be followed.\n\n\nSurvey data is unique to other types of data because most variables are a combination of numbers and variable labels. Because survey questions require respondents to answer with pre-written scale options, for which there is a corresponding number and value label, it adds an additional complication to simply importing data. The haven package handles this unique aspect well by reading both the number and the variable label into R. This gives us more information about our data that we can use to make informed decisions on how to clean prior to analysis.\n\n\n\n\n\n\nDownloading Codebooks\n\n\n\nYou should always download the codebook for any available survey data set so that you have access to how each variable was asked along with how the variables are coded.\n\n\n\nanes &lt;- read_dta('C:/Users/Stefani.Langehennig/OneDrive - University of Denver/Documents/research/surveys-textbook-data/anes_timeseries_2020_stata_20220210.dta')"
  },
  {
    "objectID": "ch00setup.html#recoding-variables",
    "href": "ch00setup.html#recoding-variables",
    "title": "2  Data Cleaning and Recoding",
    "section": "2.3 Recoding Variables",
    "text": "2.3 Recoding Variables\n\n2.3.0.1 Naming Conventions\nIt is important when you start creating new variables to keep a few best practices in mind. First, names should always be lowercase. It is okay to have multiple words in a variable name but you should use ‘snake_case’, which means connecting words with an underscore ‘_’. Next, keep names short but as informative as possible. The name should reflect what the new variable is measuring. So if the new variables measures presidential approval, the name should convey that with something like pres_app where pres = presidential and app = approval. This follows all of the best practices: lowercase, snake_case, and short but informative.\nKeep this in mind as we work through this code.\n\n\n2.3.0.2 Flipping Order of Scale\nTo simplify, we will focus only on a few variables in the 2020 ANES data. Let’s start with examining how the ANES survey coded approval ratings of the president of the United States. When we examine the codebook, we find that the approval rating question is V201129x, and it is coded where 1 = Strongly Approve and 4 = Strongly Disapprove. This is a classic example of when we would want to flip the scale so that higher values equal approval rather than disapproval. This makes it easier to discuss the variable as we are used to talking about the approval of something rather than its negation, disapproval.\nFirst, let’s examine the attributes of the variable so that we can learn more about it before the transformations.\n\nattributes(anes$V201129x) #This gives you the variable label, class, and value labels for a survey object\n\n$label\n[1] \"PRE: SUMMARY: Approve or disapprove President handling job\"\n\n$format.stata\n[1] \"%12.0g\"\n\n$class\n[1] \"haven_labelled\" \"vctrs_vctr\"     \"double\"        \n\n$labels\n-2. DK/RF in V201127 or V201128             1. Approve strongly \n                             -2                               1 \n        2. Approve not strongly      3. Disapprove not strongly \n                              2                               3 \n         4. Disapprove strongly \n                              4 \n\n\nHere, we confirm that the variable we are analyzing is the one we want to analyze per the codebook. The label tells us the question is the presidential job approval one, the class tells us it is in haven_labelled format, and the labels tell us the value label for each specific value. We also see that -2 values are non-substantive responses that should be removed from the analysis.\nNext, we will get a frequency distribution for the presidential approval rating question to understand its distribution using the tidyverse.\n\nanes %&gt;%     #Data we are using             \n  count(V201129x) %&gt;%                            # Variable we want the distribution for \n  mutate(percent = scales::percent(n / sum(n))) # calculate percent  \n\n# A tibble: 5 × 3\n  V201129x                                 n percent\n  &lt;dbl+lbl&gt;                            &lt;int&gt; &lt;chr&gt;  \n1 -2 [-2. DK/RF in V201127 or V201128]    56 0.7%   \n2  1 [1. Approve strongly]              2603 31.4%  \n3  2 [2. Approve not strongly]           759 9.2%   \n4  3 [3. Disapprove not strongly]        502 6.1%   \n5  4 [4. Disapprove strongly]           4360 52.7%  \n\n\nFrom this output, we see an important fact: missing data is currently coded as -2 and if we tried to analyze these data without cleaning, we would be including that -2 in all calculations. That would lead to bias in our results, causing us to draw incorrect conclusions.\n\n\n\n\n\n\nImportant\n\n\n\nMissing data that is coded in a particular way can bias your results. Always check the frequency distribution of your data prior to analysis.\n\n\nWe also know from above that 1 = Strongly Approve while 4 = Strongly Disapprove. Next, we will flip the scale order while removing the missing data and saving a new variable called pres_app for presidential approval.\nNote, that we will use case_when here for recoding rather than the explicit recode function. This is because case_when is more flexible at handling labelled variables like our survey data but does require slightly more code. We also need case_when for more complex transformations such as combining two variables into one.\n\nanes &lt;- anes %&gt;% #This creates new variable called 'pres_app' where 4 = strongly approve and 1 = strongly disapprove while setting -2 to NA \n  mutate(pres_app = case_when(\n    V201129x ==1 ~ 4,\n    V201129x ==2 ~ 3,\n    V201129x ==3 ~ 2,\n    V201129x ==4 ~ 1, \n  V201129x ==-2 ~ NA_real_)) #This code makes all values of -2 = NA for analysis purposes. \n\n# Define value labels\n\nvalue_labels &lt;- c(\"Strongly Disapprove\", \"Disapprove\", \"Approve\", \"Strongly Approve\") #Add value labels to your new measure. Only use this if you need to as it changes the variable type to 'factor' which influences the types of analysis you can do with it. Previously, it was 'numeric'. \n\n# Assign value labels to the Response variable\nanes$pres_app &lt;- factor(anes$pres_app, levels = 1:4, labels = value_labels)\n\nanes %&gt;%     #Data we are using             \n  count(pres_app) %&gt;%                            # Variable we want the distribution for \n  mutate(percent = scales::percent(n / sum(n))) # calculate percent \n\n# A tibble: 5 × 3\n  pres_app                n percent\n  &lt;fct&gt;               &lt;int&gt; &lt;chr&gt;  \n1 Strongly Disapprove  4360 52.7%  \n2 Disapprove            502 6.1%   \n3 Approve               759 9.2%   \n4 Strongly Approve     2603 31.4%  \n5 &lt;NA&gt;                   56 0.7%   \n\nCrossTable(anes$V201129x, anes$pres_app, expected = FALSE, chisq=FALSE,  prop.c=TRUE, prop.r=FALSE, prop.t=FALSE, prop.chisq = FALSE)\n\n   Cell Contents \n|-------------------------|\n|                       N | \n|           N / Col Total | \n|-------------------------|\n\n===============================================================================\n                 anes$pres_app\nanes$V201129x    Strngly Dspprv   Disapprove   Approve   Strongly Apprv   Total\n-------------------------------------------------------------------------------\n-2                            0            0         0                0       0\n                              0            0         0                0        \n-------------------------------------------------------------------------------\n1                             0            0         0             2603    2603\n                              0            0         0                1        \n-------------------------------------------------------------------------------\n2                             0            0       759                0     759\n                              0            0         1                0        \n-------------------------------------------------------------------------------\n3                             0          502         0                0     502\n                              0            1         0                0        \n-------------------------------------------------------------------------------\n4                          4360            0         0                0    4360\n                              1            0         0                0        \n-------------------------------------------------------------------------------\nTotal                      4360          502       759             2603    8224\n                          0.530        0.061     0.092            0.317        \n===============================================================================\n\n\nThis example illustrates how to flip the order of a question scale so that higher values indicate more of whatever the question measures, in this case presidential approval. We named the new variable pres_app to keep the label short but also informative of what the question measures. We then added value labels to ensure we know what each value represents.\nIt is important to examine the distribution of your new variable to ensure nothing went wrong in the recoding process. We did two checks:\n\nWe checked the frequency distribution of the new variable to ensure we see what we expect to see.\nWe run a crosstab between our new variable and the original. We should expect to see a mirror image in the crosstab, which we do. This means our recode was successful and 4 = Strongly Approve while 1 = Strongly Disapprove.\n\nWe then used the haven package to label the new pres_app with more informative information to ensure we remember what the variable is measuring.\nNext, we will look at one additional way to flip your scale. This is least amount of code but also offers the largest possibility of error. Here we will the if_else command along with mutate to create a new presidential approval variable pres_app3, which is the inverse of the original presidential approval variable V201129x. We multiply the original value by -1 and then add one more than the total number of scale points. Here, we had 4 total scale points - strongly approve, approve, disapprove, strongly disapprove - so we add 4+1 (5) to the scale. This mathematically flips the scale so that the original values of 1=4, 2=3, 3=2, and 4=1. It will always work provided you add the appropriate number of points.\n\n#Additional way to flip your scale \nanes &lt;- anes %&gt;% \n  mutate(pres_app3 = if_else(V201129x&gt;=1, (V201129x*-1)+5, NA)) #When original variable &gt;= 1, we will multiply the original variable by -1 and then add 1 more than the total scale points. Since there were 4 scale points in the original scale, we add 5. We also recode anything that was &gt;=1 originally as NA since that reflects a non-substantive response. \n\n#1 becomes 4 b/c (1*-1)=-1+5=4\n#2 becomes 3 b/c (2*-1)=-2+5=3\n#3 becomes 2 b/c (3*-1)=-3+5=2\n#4 becomes 1 b/c (4*-1)=-4+5=1\n\nanes %&gt;%     #Data we are using             \n  group_by(V201129x)  %&gt;% #Original variable\n  count(pres_app3)         # New variable \n\n# A tibble: 5 × 3\n# Groups:   V201129x [5]\n  V201129x                             pres_app3     n\n  &lt;dbl+lbl&gt;                                &lt;dbl&gt; &lt;int&gt;\n1 -2 [-2. DK/RF in V201127 or V201128]        NA    56\n2  1 [1. Approve strongly]                     4  2603\n3  2 [2. Approve not strongly]                 3   759\n4  3 [3. Disapprove not strongly]              2   502\n5  4 [4. Disapprove strongly]                  1  4360\n\n\n\n\n2.3.0.3 Combining Two Variables into One\nNow, let’s combine two variables into one. Oftentimes, surveys will ask branching questions that need to be combined for analysis purposes. In fact, the presidential approval measure we just analyzed is the combination of two questions. Most survey firms will not pre-combine these two questions into one so it is important to learn how to do so.\nThere are two variables to combine:\n\nV201127 (1= approve & 2=disapprove) #Approve/Disapprove of performance\nV201128 (1=strongly & 2 = not strongly) #How strongly approve/disapprove of performance\n\nRemember, higher values should equal more approval so we want 4 = Strongly Approve and 1 = Strongly Disapprove.\n\n#First run a crosstab between your two existing variables to get the distribution across cells. \n\nanes %&gt;%     #Data we are using             \n  group_by(V201128 )  %&gt;% #X Variable in Crosstab \n  count(V201127)         # Y Variable in Crosstab \n\n# A tibble: 8 × 3\n# Groups:   V201128 [4]\n  V201128               V201127                 n\n  &lt;dbl+lbl&gt;             &lt;dbl+lbl&gt;           &lt;int&gt;\n1 -9 [-9. Refused]       1 [1. Approve]         3\n2 -9 [-9. Refused]       2 [2. Disapprove]      1\n3 -1 [-1. Inapplicable] -9 [-9. Refused]       44\n4 -1 [-1. Inapplicable] -8 [-8. Don't know]     8\n5  1 [1. Strongly]       1 [1. Approve]      2603\n6  1 [1. Strongly]       2 [2. Disapprove]   4360\n7  2 [2. Not strongly]   1 [1. Approve]       759\n8  2 [2. Not strongly]   2 [2. Disapprove]    502\n\n#Next, create your new variable based on the 4 possible combinations using a series of & statements. Be careful.\n\nanes &lt;- anes %&gt;% #This creates new variable called 'pid_x' where higher values=more republican \n  mutate(pres_app2 = case_when(\n    V201127==1 & V201128 ==1 ~ 4,\n    V201127==1 & V201128 ==2 ~ 3,\n    V201127==2 & V201128 ==2 ~ 2,\n    V201127==2 & V201128 ==1 ~ 1))\n\n#Now, if we want we can make a new factor variable that saves the labels. \n\nanes &lt;- anes %&gt;% #This creates new variable called 'pres_app2_f' which is a factorized version of the above pres_app2 measure.  \n  mutate(pres_app2_f = case_when(\n    V201127==1 & V201128 ==1 ~ 'Strong Approve',\n    V201127==1 & V201128 ==2 ~ 'Approve',\n    V201127==2 & V201128 ==2 ~ 'Disapprove',\n    V201127==2 & V201128 ==1 ~ 'Strong Disapprove'))\n\n#Check our work against the original variables.\n\nanes %&gt;%     #Data we are using             \n  group_by(V201128, V201127)  %&gt;% #X Variable in Crosstab \n  count(pres_app2)         # Y Variable in Crosstab  \n\n# A tibble: 8 × 4\n# Groups:   V201128, V201127 [8]\n  V201128               V201127             pres_app2     n\n  &lt;dbl+lbl&gt;             &lt;dbl+lbl&gt;               &lt;dbl&gt; &lt;int&gt;\n1 -9 [-9. Refused]       1 [1. Approve]            NA     3\n2 -9 [-9. Refused]       2 [2. Disapprove]         NA     1\n3 -1 [-1. Inapplicable] -9 [-9. Refused]           NA    44\n4 -1 [-1. Inapplicable] -8 [-8. Don't know]        NA     8\n5  1 [1. Strongly]       1 [1. Approve]             4  2603\n6  1 [1. Strongly]       2 [2. Disapprove]          1  4360\n7  2 [2. Not strongly]   1 [1. Approve]             3   759\n8  2 [2. Not strongly]   2 [2. Disapprove]          2   502\n\n#Check our work against our previously created variable.\n\nanes %&gt;%     #Data we are using             \n  group_by(pres_app)  %&gt;% #X Variable in Crosstab \n  count(pres_app2)         # Y Variable in Crosstab  \n\n# A tibble: 5 × 3\n# Groups:   pres_app [5]\n  pres_app            pres_app2     n\n  &lt;fct&gt;                   &lt;dbl&gt; &lt;int&gt;\n1 Strongly Disapprove         1  4360\n2 Disapprove                  2   502\n3 Approve                     3   759\n4 Strongly Approve            4  2603\n5 &lt;NA&gt;                       NA    56\n\n\nOnce we run the above code, we see that we successfully created our new variable combining two separate variables. This is a flexible approach that can be applied to any two or more variables provided you are careful in coding the correct values. For more complex combinations, it can be useful to map it on a whiteboard or piece of paper first.\n\n\n2.3.0.4 Collapsing Variables into Smaller Groups\nOftentimes, we want to collapse the number of groups in a variable into fewer groups or even into a dummy, otherwise known as dichotomous, variable. Using education (V201507x) as the example, let’s look at several ways to collapse groups into fewer options. Remember, since we are doing data transformations, we want to keep the original variable untransformed and save a new one.\nStart by examining the frequency distribution for the variable you want to transform. For education, we see that less than HS degree is only selected 376 times out of the 8,000+ cases. Because it is selected so infrequently, it should definitely be combined with the next option, HS degree. We also see that some college but no degree option is the modal response for the scale. This suggests that, depending on our theory and planned analysis, we should create a few different educational attainment variables.\nFirst, we will keep the original categories the same but combine the less than HS degree with HS degree options.\nThen, we will create a dummy variable for college degree or not (0 or 1):\n\nCollege Degree = People with Bachelor’s degree or graduate degree\nNo College Degree = People with some college but no degree, HS degree, or no HS degree\n\nLastly, we will create a 3-point scale splitting the no college degree into Some college or No College at all:\n\nCollege Degree = People with Bachelor’s degree or graduate degree\nSome College = People with some college but no degree\nHigh school degree or less = HS degree or no HS degree\n\n\n#First look at the age distribution\nanes %&gt;%     #Data we are using             \n  count(V201511x)         # Variable to analyze \n\n# A tibble: 8 × 2\n  V201511x                                                              n\n  &lt;dbl+lbl&gt;                                                         &lt;int&gt;\n1 -9 [-9. Refused]                                                     33\n2 -8 [-8. Don't know]                                                   1\n3 -2 [-2. Missing, other specify not coded for preliminary release]    97\n4  1 [1. Less than high school credential]                            376\n5  2 [2. High school credential]                                     1336\n6  3 [3. Some post-high school, no bachelor's degree]                2790\n7  4 [4. Bachelor's degree]                                          2055\n8  5 [5. Graduate degree]                                            1592\n\nanes &lt;- anes %&gt;%\n  mutate(college = ifelse(V201511x %in% c(1, 2), 1, V201511x-1)) #Recodes all values from the original education variables that are 1 or 2 to be = 1 and then sets all other values to their original value - 1 t0 keep the order in tact of 1, 2, 3, 4. \n\nvalue_labels &lt;- c(\"HS or Less\", \"Some College\", \"College Degree\", \"Graduate Degree\") #Add value labels to your new measure. Only use this if you need to as it changes the variable type to 'factor' which influences the types of analysis you can do with it. Previously, it was 'numeric'. \n\n# Assign value labels to the Response variable\nanes$college &lt;- factor(anes$college, levels = 1:4, labels = value_labels)\n\n#You should notice that -9 = missing data. -9 will automatically become NA in the new variable if you simply do nothing with it in this code. \n\n##Second way to recode our college variable; this time to create a dummy variable. \nanes &lt;- anes %&gt;% # \n  mutate(college2 = case_when(\n    V201511x==1 ~ 'No College Degree',\n    V201511x==2  ~ 'No College Degree',\n    V201511x==3  ~ 'No College Degree',\n    V201511x==4  ~ 'College Degree',\n    V201511x==5  ~ 'College Degree'))\n\nanes &lt;- anes %&gt;% # \n  mutate(college2 = case_when(\n    V201511x==1 ~ 0,\n    V201511x==2  ~ 0,\n    V201511x==3  ~ 0,\n    V201511x==4  ~ 1,\n    V201511x==5  ~ 1))\n\nclass(anes$college)\n\n[1] \"factor\"\n\n#Checks distribution of new variable \nanes %&gt;%     #Data we are using             \n  count(college)         #  New Variable \n\n# A tibble: 5 × 2\n  college             n\n  &lt;fct&gt;           &lt;int&gt;\n1 HS or Less       1712\n2 Some College     2790\n3 College Degree   2055\n4 Graduate Degree  1592\n5 &lt;NA&gt;              131\n\n#Crosstab between original and new variable to ensure recode success\nanes %&gt;%     #Data we are using    \n  group_by(V201511x) %&gt;% #Original Variable \n  count(college)         # New Variable \n\n# A tibble: 8 × 3\n# Groups:   V201511x [8]\n  V201511x                                                         college     n\n  &lt;dbl+lbl&gt;                                                        &lt;fct&gt;   &lt;int&gt;\n1 -9 [-9. Refused]                                                 &lt;NA&gt;       33\n2 -8 [-8. Don't know]                                              &lt;NA&gt;        1\n3 -2 [-2. Missing, other specify not coded for preliminary releas… &lt;NA&gt;       97\n4  1 [1. Less than high school credential]                         HS or …   376\n5  2 [2. High school credential]                                   HS or …  1336\n6  3 [3. Some post-high school, no bachelor's degree]              Some C…  2790\n7  4 [4. Bachelor's degree]                                        Colleg…  2055\n8  5 [5. Graduate degree]                                          Gradua…  1592\n\n###Different approach to creating a dichotomous variable. Here, you have to explicitly make missing data NA \nanes &lt;- anes %&gt;% \n  mutate(college2 = if_else(V201511x&gt;3, 1, 0))\n\nanes &lt;- anes %&gt;% #Explicitly making the values -1/-9 NA \n  mutate(college2 = replace(college2, V201511x &lt;= -1, NA)) #Removes NA from new college2 variable  \n\n#Crosstab between original and new variable to ensure recode success\nanes %&gt;%     #Data we are using    \n  group_by(V201511x) %&gt;% #Original Variable \n  count(college2)         # New Variable \n\n# A tibble: 8 × 3\n# Groups:   V201511x [8]\n  V201511x                                                        college2     n\n  &lt;dbl+lbl&gt;                                                          &lt;dbl&gt; &lt;int&gt;\n1 -9 [-9. Refused]                                                      NA    33\n2 -8 [-8. Don't know]                                                   NA     1\n3 -2 [-2. Missing, other specify not coded for preliminary relea…       NA    97\n4  1 [1. Less than high school credential]                               0   376\n5  2 [2. High school credential]                                         0  1336\n6  3 [3. Some post-high school, no bachelor's degree]                    0  2790\n7  4 [4. Bachelor's degree]                                              1  2055\n8  5 [5. Graduate degree]                                                1  1592\n\n#Create our third new education variable; this one with 3 options\nanes &lt;- anes %&gt;% # \n  mutate(college3 = case_when(\n    V201511x==1 ~ 'HS Degree or Less',\n    V201511x==2  ~ 'HS Degree or Less',\n    V201511x==3  ~ 'Some College',\n    V201511x==4  ~ 'College Degree',\n    V201511x==5  ~ 'College Degree'))"
  },
  {
    "objectID": "ch00setup.html#working-with-missing-datanon-substantive-responses",
    "href": "ch00setup.html#working-with-missing-datanon-substantive-responses",
    "title": "2  Data Cleaning and Recoding",
    "section": "2.4 Working with Missing Data/non-Substantive Responses",
    "text": "2.4 Working with Missing Data/non-Substantive Responses\nNearly all survey data will include missing data that should be investigated. Oftentimes, missing data is coded as a real value in your survey such as ‘-2’ or ‘99’. If this is the case, you must ensure that R knows what values should not be included in your analysis otherwise you will introduce bias into your calculations.\nIf we only want to change a specific value or values to NA without recoding the entire variable, we can use the replace function from tidyverse. We still want to save a new variable since we are transforming the orginal variable in some way but we will not need to do anything else to the variable. Here we recode ‘-9’ to system missing for a feeling thermometer measure rating how much Americans liked Donald Trump in 2020.\nThis code easily handles more complexity as the second recode changes any values &lt;= -1 or &gt;= 101 to system missing since the codebook identifies all these values as non-valid answers. This is another reminder to always work from an up-to-date codebook so that you can catch any potential issues.\n\nanes &lt;- anes %&gt;%\n  mutate(trump_feel = replace(V201152, V201152 == -9, NA)) #Create new variable called 'trump_feel' which equals the original variable 'V201151' but replace all -9 values as NA\n\nanes &lt;- anes %&gt;%\n  mutate(biden_feel = replace(V201151, (V201151 &lt;= -1 | V201151 &gt;= 101), NA)) #Create new variable called 'biden_feel' which equals the original variable 'V201151' but replace all -9 values as NA\n\n\n\n\n\n\n\nNote\n\n\n\nRemember, we already looked at how to make a value system missing using case_when when recoding the entire variable.\n\n\n\nanes &lt;- anes %&gt;% #This creates new variable called 'pres_app' where 4 = strongly approve and 1 = strongly disapprove while setting -2 to NA \n  mutate(pres_app = case_when(\n    V201129x ==1 ~ 4,\n    V201129x ==2 ~ 3,\n    V201129x ==3 ~ 2,\n    V201129x ==4 ~ 1, \n  V201129x ==-2 ~ NA_real_)) #This code makes all values of -2 = NA for analysis purposes."
  },
  {
    "objectID": "ch00setup.html#merging-external-data-into-survey-data",
    "href": "ch00setup.html#merging-external-data-into-survey-data",
    "title": "2  Data Cleaning and Recoding",
    "section": "2.5 Merging External Data into Survey Data",
    "text": "2.5 Merging External Data into Survey Data\nDepending on the sample, sometimes it is possible to link external data - i.e. data not collected in the survey but from some source external to the interview - to individual survey records. This can include things like a student’s grades if you work for a school system or voting records for political poll respondents. In these cases, researchers can utilize these external files to do a variety of interesting analyses. However, the first step is to merge the two files, which is not always easy to do.\nHere, we are going to merge the original 2020 American Election Study Survey with validated voting information collected by a third party vendor. Validated vote data essential is just a public record of if an individual tied their vote to their survey responses, all while keeping their identify confidential to researchers. By merging these files, researchers can then answer important questions around who actually turns out to vote versus who says they turned out to vote.\nLet’s go through that process here.\nTo begin, we need to identify the case_id, which is the column which should provide a unique identification number for all individual survey responses. This will be the critical variable that needs to be matched between your main file - for us the 2020 ANES - and the file to be merged - the validated vote file. We need to examine both files to ensure they have the same structure for their case_id variable and ideally that the variable names match. If these two columns do not match exactly, you will only get partial to zero matches.\n\n\n\n\n\n\nImportant\n\n\n\nAlways closely examine what variables are in your columns before trying to merge, as this will influence the quality of your matches.\n\n\n\nvvote &lt;- read_dta('C:/Users/Stefani.Langehennig/OneDrive - University of Denver/Documents/research/surveys-textbook-data/anes_timeseries_2020_stata_VoterValidation.dta')\n\nhead(anes$V200001)\n\n[1] 200015 200022 200039 200046 200053 200060\n\nhead(vvote$V200001)\n\n[1] 200015 200022 200039 200046 200053 200060\n\nanes_vv &lt;- merge(anes, vvote, by = \"V200001\", all = FALSE)\n\nWe’ll start by looking at the codebook for the validated vote data which tells us that V200001 is the name for the case_id variable and that it should match the original 2020 ANES name exactly. Using the head function, we can quickly verify that indeed these two variables have the same structure and same values for the first few cases. This indicates that it is safe to move forward with merging the two files.\nNote, there are 18,430 cases in the vvote data frame while there are only 8,280 in the anes data. That means the vvote data has cases that do not exist in the anes data, which is fine. We do not want to merge data that is not included in the ANES file. By including all = FALSE, we tell R to only include variables that are in the main file in the merge. This is one reason why it is important to pay attention to which file you are treating as the main file, which will be listed first in the merge code.\n\nanes_vv &lt;- merge(anes, vvote, by = \"V200001\")\nview(anes_vv)\nnames(vvote)\n\n [1] \"version\"           \"V200001\"           \"val1_addmatch\"    \n [4] \"val1_number\"       \"val1_matchprob\"    \"val1_turnout16\"   \n [7] \"val1_turnout20\"    \"val1_hh_turnout16\" \"val1_hh_turnout20\"\n[10] \"val2_addmatch\"     \"val2_number\"       \"val2_matchprob\"   \n[13] \"val2_match\"        \"val2_turnout16\"    \"val2_turnout20\"   \n[16] \"val2_hh_turnout16\" \"val2_hh_turnout20\" \"val2_c1_minor\"    \n[19] \"val2_c1_major\"     \"val2_c1_match\"     \"val2_c2_minor\"    \n[22] \"val2_c2_major\"     \"val2_c2_match\"    \n\n#Distribution of responses\nanes_vv %&gt;%     #Data we are using             \n  count(val1_turnout20)         # Y Variable in Crosstab \n\n  val1_turnout20    n\n1              2  261\n2              3 1605\n3              4 2965\n4              5  933\n5              6 2516\n\nanes_vv %&gt;%     #Data we are using             \n  count(val2_turnout20)         # Y Variable in Crosstab \n\n  val2_turnout20    n\n1              2  424\n2              3 1034\n3              7 6822\n\nCrossTable(anes_vv$val1_turnout20, anes_vv$val2_turnout20, expected = FALSE, chisq=TRUE,  prop.c=TRUE, prop.r=FALSE, prop.t=FALSE, prop.chisq = FALSE)\n\n   Cell Contents \n|-------------------------|\n|                       N | \n|           N / Col Total | \n|-------------------------|\n\n=======================================================\n                          anes_vv$val2_turnout20\nanes_vv$val1_turnout20        2       3       7   Total\n-------------------------------------------------------\n2                            67      55     139     261\n                          0.158   0.053   0.020        \n-------------------------------------------------------\n3                           187     710     708    1605\n                          0.441   0.687   0.104        \n-------------------------------------------------------\n4                            82     115    2768    2965\n                          0.193   0.111   0.406        \n-------------------------------------------------------\n5                            22      41     870     933\n                          0.052   0.040   0.128        \n-------------------------------------------------------\n6                            66     113    2337    2516\n                          0.156   0.109   0.343        \n-------------------------------------------------------\nTotal                       424    1034    6822    8280\n                          0.051   0.125   0.824        \n=======================================================\n\nStatistics for All Table Factors\n\nPearson's Chi-squared test \n------------------------------------------------------------\nChi^2 = 2492.817      d.f. = 8      p &lt;2e-16 \n\nattributes(anes_vv$V201529)\n\n$label\n[1] \"PRE: Describe R's employment\"\n\n$format.stata\n[1] \"%12.0g\"\n\n$labels\n                                                                     -9. Refused \n                                                                              -9 \n                                                                -1. Inapplicable \n                                                                              -1 \n                                           1. For-profit company or organization \n                                                                               1 \n  2. Non-profit organization (including tax-exempt and charitable organizations) \n                                                                               2 \n               3. Local government (for example: city or county school district) \n                                                                               3 \n                     4. State government (including state colleges/universities) \n                                                                               4 \n                          5. Active duty U.S. Armed Forces or Commissioned Corps \n                                                                               5 \n                                         6. Federal government civilian employee \n                                                                               6 \n           7. Owner of non-incorporated business, professional practice, or farm \n                                                                               7 \n               8. Owner of incorporated business, professional practice, or farm \n                                                                               8 \n9. Worked without pay in a for-profit family business or farm for 15 hours or mo \n                                                                               9 \n\n$class\n[1] \"haven_labelled\" \"vctrs_vctr\"     \"double\"        \n\nanes_vv %&gt;% \n  count(V201529)\n\n   V201529    n\n1       -9  181\n2       -1  234\n3        1 4141\n4        2  920\n5        3  772\n6        4  401\n7        5   49\n8        6  330\n9        7  683\n10       8  423\n11       9  146\n\n\nNow that the files are merged, we can quickly see that in the original anes file we had 1,775 variables while in the new anes_vv merged file we have 1,797 variables, or 22 additional. A quick visual inspection of the end of the anes_vv file reveals that the variables that were in the vvote file are now appended to the end of our original anes file. This means our merge was successful, and we can now start to analyze the newly created file."
  },
  {
    "objectID": "ch00setup.html#applying-survey-weights-in-the-analysis",
    "href": "ch00setup.html#applying-survey-weights-in-the-analysis",
    "title": "2  Data Cleaning and Recoding",
    "section": "2.6 Applying Survey Weights in the Analysis",
    "text": "2.6 Applying Survey Weights in the Analysis\nSurvey weights make the survey sample’s demographic profile look more like its population while also sometimes accounting for things such as unequal likelihood of being selected to participate. Many publicly accessible large-n surveys will come with associated survey weights that should be applied when conducting analysis. For the most part, provided weights should always be used when analyzing survey data.\nThe first step to using survey weights is to review the codebook and find what it has to say about its survey weights. While some survey data sets will come with only 1 survey weight, many, like the ANES which has 14 unique weights, will have multiple and utilizing the codebook is imperative in this instance.\nBy reviewing the 2020 ANES codebook, we see that we want to use V200010b as our primary weight since we are analyzing data collected in the post-election survey wave. In this case, we also want to include a strata weight per the instructions. Not all surveys will include both types of weights but when they do each should be utilized.\nThe fact that we are required to use the post-election survey weight will pose an additional problem that not every respondent who took the pre-election survey returned for the post-election wave. This code requires no missing data in the weighting variable otherwise it will not run. So we first will remove the cases that do not have values in the post-election weight variable.\nWe will use the svydesign function from the survey package to create new weighted data sets for analysis purposes. The new weighted file can then be used in analysis.\n\nanes_post &lt;- anes_vv[complete.cases(anes_vv[, c(\"V200010b\")]), ]\nanes_weighted &lt;- svydesign(ids = ~1, weights =~V200010b, data = anes_post) #Creates new weighted data for analysis using the population weights only \nanes_weighted2 &lt;- svydesign(ids = ~V200010c, weights =~V200010b, strata=~V200010d, nest=TRUE, data = anes_post) #For more complex survey designs that includes additional weights including strata and PSUs. PSU weight = 'ids' while 'strata' = strata weight as specified in code book   \n\n#Creates new variable for use in analysis \nanes_post &lt;- anes_post %&gt;%\n  mutate(trump_feel = replace(V201152, V201152 == -9, NA)) #Create new variable called 'trump_feel' which equals the original variable 'V201152' but replace all -9 values as NA\n\n###Analyze the mean of the Trump feeling thermometer measure for the different files \nsvymean(~trump_feel, anes_weighted, na.rm = TRUE) #Get weighted mean for simple weighting scheme\n\n             mean     SE\ntrump_feel 41.289 0.6584\n\nsvymean(~trump_feel, anes_weighted2, na.rm = TRUE) #Get weighted mean for complex weighting scheme\n\n             mean     SE\ntrump_feel 41.289 0.6973\n\nmean(anes_post$trump_feel, na.rm=TRUE) #Get unweighted mean for sample \n\n[1] 39.97184\n\n\nBy examining the means across the weighted and unweighted files, we see that the two weighted files return identical means, with slightly different standard errors, but the unweighted mean being 1.3 points lower. The difference between the weighted and unweighted means is why we want to use the survey weights as the weighted data should provide more accurate point estimates."
  },
  {
    "objectID": "part_design.html#section-outline",
    "href": "part_design.html#section-outline",
    "title": "Introduction to Survey Design",
    "section": "Section Outline",
    "text": "Section Outline"
  },
  {
    "objectID": "part_analysis.html#section-outline",
    "href": "part_analysis.html#section-outline",
    "title": "Introduction to Survey Analysis",
    "section": "Section Outline",
    "text": "Section Outline\nOnce we have completed the design of our survey, we are ready to launch the instrument! This means we will have data for which we can analyze. When we conduct a survey, we usually have a research or business objective in mind. Often, we will start with a hypothesis, collect our data, analyze it, and report back on what we found. This section will cover each of those things in depth:\n\nChapter 10: Hypothesis Testing\nChapter 11: Analyzing Survey Data\nChapter 12: Reporting Survey Data"
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "Survey Methods for Social Scientists",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nMore to come!"
  },
  {
    "objectID": "ch10conclusion.html#benefits-of-survey-design-analysis-in-r",
    "href": "ch10conclusion.html#benefits-of-survey-design-analysis-in-r",
    "title": "11  Conclusion",
    "section": "11.1 Benefits of Survey Design & Analysis in R",
    "text": "11.1 Benefits of Survey Design & Analysis in R"
  },
  {
    "objectID": "ch09hypothesis.html#learning-objectives",
    "href": "ch09hypothesis.html#learning-objectives",
    "title": "10  Hypothesis Testing in Survey Analysis",
    "section": "10.1 Learning Objectives",
    "text": "10.1 Learning Objectives"
  },
  {
    "objectID": "ch10analysis.html#learning-objectives",
    "href": "ch10analysis.html#learning-objectives",
    "title": "11  Analyzing Survey Data",
    "section": "11.1 Learning Objectives",
    "text": "11.1 Learning Objectives"
  },
  {
    "objectID": "ch11report.html#learning-objectives",
    "href": "ch11report.html#learning-objectives",
    "title": "12  Reporting Survey Data",
    "section": "12.1 Learning Objectives",
    "text": "12.1 Learning Objectives"
  },
  {
    "objectID": "ch12conclusion.html#benefits-of-survey-design-analysis-in-r",
    "href": "ch12conclusion.html#benefits-of-survey-design-analysis-in-r",
    "title": "Conclusion",
    "section": "Benefits of Survey Design & Analysis in R",
    "text": "Benefits of Survey Design & Analysis in R"
  },
  {
    "objectID": "ch02psych_response.html#factor-analysis-to-understan-political-emotions",
    "href": "ch02psych_response.html#factor-analysis-to-understan-political-emotions",
    "title": "3  Psychology of Survey Response",
    "section": "3.1 Factor Analysis to Understan Political Emotions",
    "text": "3.1 Factor Analysis to Understan Political Emotions\nWe will conduct both exploratory and confirmatory factor analysis using the nationally representative 2020 American National Election Survey for our data. In this survey, 9 unique political emotions variables were asked of respondents, and we will use these 9 variables to understand how political emotions are structured. Currently, it is widely believed that three latent emotional factors best explain the structure of political emotions (Marcus et al. 2006) with three primary latent factors:\n\nAversion to Politics: Measures include anger, outrage, and irritation\nWorry about Politics: Measures include fear, worry, and nervousness\nEnthusiasm about Politics: Measures include happiness, hope, and pride\n\nWe will use factor analysis to test whether a three factor solution really is the best way to explain these political emotions.\nTo start, we read in our data, in this case the 2020 American National Election study. Then we save a new data frame that includes only the nine political emotions variables we want to include in our analysis. We will use this data frame throughout the code. Once we have saved the political emotions variable as a new data frame, we recode all the negative values to NA as these are non-substantive responses which should be removed (as indicated in the associated codebook).\nNext, we change the variables names to something that is more informative. This will aid in interpretation of the factor analysis results and should not be skipped. Lastly, we use the skimr package to quickly skim the variables in our dataset. We want to see the minimum value = 1 with no negative values, since negative values should be treated as missing data.\n\nlibrary(haven) #Imports stata file\nlibrary(skimr) #For data evaluation \nlibrary(psych) #For exploratory factor analysis\nlibrary(corrplot) #To graph correlations \nlibrary(lavaan) #For confirmatory factor analysis\nlibrary(semPlot) #For graphing CFA results \nlibrary(GPArotation) #For factor rotation\n\n\nanes &lt;- read_dta('C:/Users/Stefani.Langehennig/OneDrive - University of Denver/Documents/research/surveys-textbook-data/anes_timeseries_2020_stata_20220210.dta')\n\ndf &lt;- data.frame(anes$V201115, anes$V201116, anes$V201117, anes$V201118, anes$V201119, anes$V201120, anes$V201121, anes$V201122, anes$V201123  ) #Save the variables you want to include in your factor analysis into a new data frame\n\ndf[df &lt;= -1] &lt;- NA #Set missing values to NA to not include in analysis\n\nnew_names &lt;- c(\"hope\", \"afraid\", \"outrage\", \"angry\", \"happy\", \"worried\", \"proud\", \"irritated\", \"nervous\") #Give your variables new informative names \n\n# Update column names\ncolnames(df) &lt;- new_names #Apply new names to your data frame\n\nskim(df) #Checks the variables in your data frame; evaluate for missing data\n\n\nData summary\n\n\nName\ndf\n\n\nNumber of rows\n8280\n\n\nNumber of columns\n9\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n9\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nhope\n23\n1\n2.53\n1.17\n1\n2\n3\n3\n5\n▆▆▇▃▂\n\n\nafraid\n15\n1\n3.39\n1.23\n1\n3\n3\n4\n5\n▂▅▇▇▆\n\n\noutrage\n18\n1\n3.60\n1.25\n1\n3\n4\n5\n5\n▂▃▆▇▇\n\n\nangry\n17\n1\n3.58\n1.20\n1\n3\n4\n5\n5\n▂▃▇▇▇\n\n\nhappy\n20\n1\n1.94\n1.05\n1\n1\n2\n3\n5\n▇▃▃▁▁\n\n\nworried\n12\n1\n3.69\n1.16\n1\n3\n4\n5\n5\n▁▃▆▇▇\n\n\nproud\n27\n1\n2.00\n1.15\n1\n1\n2\n3\n5\n▇▃▃▂▁\n\n\nirritated\n14\n1\n3.85\n1.11\n1\n3\n4\n5\n5\n▁▂▅▇▇\n\n\nnervous\n9\n1\n3.54\n1.20\n1\n3\n4\n5\n5\n▂▃▇▇▇\n\n\n\n\n\n\n3.1.0.1 Step 1: Evaluating Correlations Between Political Emotions\nFirst, we need to check the correlations between your variables - here, political emotions - to see how related each of the individual items are. Below, we create a matrix with all the correlations between the individual items and graph the correlations using a heat-map for easier viewing.\n\n#Step 1: Evaluate correlations \ncor_matrix&lt;-cor(df, use = \"pairwise.complete.obs\") #Saves correlation matrix\n\ncorrplot(cor_matrix, method = \"circle\") # Plot correlation matrix as a heatmap\n\n\n\ncor_matrix\n\n                hope     afraid    outrage      angry      happy    worried\nhope       1.0000000 -0.4214833 -0.3488169 -0.3654797  0.6252765 -0.4296574\nafraid    -0.4214833  1.0000000  0.6235087  0.6420478 -0.4527532  0.7566750\noutrage   -0.3488169  0.6235087  1.0000000  0.7798428 -0.4336406  0.6483833\nangry     -0.3654797  0.6420478  0.7798428  1.0000000 -0.4469920  0.6599894\nhappy      0.6252765 -0.4527532 -0.4336406 -0.4469920  1.0000000 -0.5004800\nworried   -0.4296574  0.7566750  0.6483833  0.6599894 -0.5004800  1.0000000\nproud      0.6262467 -0.4169572 -0.4029037 -0.4156885  0.7420263 -0.4557817\nirritated -0.3726374  0.6011975  0.7076147  0.7347953 -0.4720350  0.6496327\nnervous   -0.4306049  0.7711057  0.6188526  0.6410278 -0.4745889  0.7744649\n               proud  irritated    nervous\nhope       0.6262467 -0.3726374 -0.4306049\nafraid    -0.4169572  0.6011975  0.7711057\noutrage   -0.4029037  0.7076147  0.6188526\nangry     -0.4156885  0.7347953  0.6410278\nhappy      0.7420263 -0.4720350 -0.4745889\nworried   -0.4557817  0.6496327  0.7744649\nproud      1.0000000 -0.4472481 -0.4453810\nirritated -0.4472481  1.0000000  0.6175571\nnervous   -0.4453810  0.6175571  1.0000000\n\n###At least 2 distinct and possibly 3 distinct factors from examining \n\n\nResults indicate 2 and probably 3 unique factors + Positive Emotions: Happy, Pride, Hope\n\nResults indicate at least two and likely three distinct factors in the political emotions’ variables. Three factors would match the dominant belief in the literature on how political emotions are structured (Marcus et al. 2006). The three positive emotions are clearly positively related to each other with smaller and negative coefficients to the other six emotions. For the negative emotions, all six items are positively and significantly related to one another indicating they might all be measuring the same concept. However, closer examination of the results indicates that being afraid is more highly correlated with being worried or nervous than it is anger, outrage, or irritation. That matches the underlying theoretical belief that those three items represent an “anxiety about politics” factor, whereas anger, outrage, and irritation represent an “aversion” factor towards politics.\nConducting a factor analysis will help us better understand how these 9 individual political emotions are related to one another. We will conduct both exploratory and confirmatory factor analysis to illustrate these methods.\n\n\n3.1.1 Exploratory Factor Analysis\nWe’ll start the factor analysis with an exploratory factor analysis approach using both principal component factor analysis (pcf) and principal axis factor analysis (paf). We’ll compare the results between these two approaches to evaluate differences. PCF handles non-linearity better than PAF approaches, so with many Likert style survey questions that are not normally distributed we utilize the PCF approach.\n\n3.1.1.1 Step 2: Screeplot\n\n#Step 2: Evaluate Screeplot - looking for number of factors &gt;= ~1 \nscree(df) #from 'psych' package and graphs scree plot for PCF and PAF approaches \n\n\n\n\n\n\n\n\n\n\nScree Plots\n\n\n\nWe use scree plots to visually decide how many factors or components to use in our factor analyses. We usually determine this number by looking at everything that precedes the elbow.\n\n\nThe scree plot shows eigenvalues from a PCF and a PAF, non-rotated, factor analysis. The PCF shows two clear factors with a third worth looking into whereas the PAF shows one clear factor with a second factor that is close. Knowing what we know from the correlations we examined, we will start with a three factor solution with our exploratory analysis. If two factors does indeed fit the data better than three, the factor analysis will show that.\n\n\n3.1.1.2 Step 3: Estimate the Exploratory Factor Analysis\nNow, we will estimate a series of factor analyses to illustrate PCF vs. PAF Rotation Types: None, Orthogonal, & Oblique\nNote the slight difference in code between the pcf and the paf approaches. The only difference is using principal for the pcf approach and fa for the paf. We also can indicate which form of rotation we want to perform for each of the factor analyses. Here, we do both no rotation and use varimax for the orthogonal rotations, and oblimin for the oblique rotations. We start with a basic three factor pcf approach without rotation.\n\npcf_result_no &lt;- principal(df,nfactors = 3,  rotate = \"none\") #PCF approach with no rotation\n\npcf_result_no\n\nPrincipal Components Analysis\nCall: principal(r = df, nfactors = 3, rotate = \"none\")\nStandardized loadings (pattern matrix) based upon correlation matrix\n            PC1  PC2   PC3   h2   u2 com\nhope      -0.63 0.57 -0.11 0.74 0.26 2.0\nafraid     0.82 0.21  0.35 0.85 0.15 1.5\noutrage    0.81 0.30 -0.31 0.83 0.17 1.6\nangry      0.82 0.29 -0.29 0.84 0.16 1.5\nhappy     -0.72 0.54  0.09 0.81 0.19 1.9\nworried    0.85 0.19  0.26 0.83 0.17 1.3\nproud     -0.68 0.58  0.11 0.82 0.18 2.0\nirritated  0.81 0.22 -0.31 0.80 0.20 1.4\nnervous    0.84 0.19  0.35 0.85 0.15 1.4\n\n                       PC1  PC2  PC3\nSS loadings           5.47 1.28 0.62\nProportion Var        0.61 0.14 0.07\nCumulative Var        0.61 0.75 0.82\nProportion Explained  0.74 0.17 0.08\nCumulative Proportion 0.74 0.92 1.00\n\nMean item complexity =  1.6\nTest of the hypothesis that 3 components are sufficient.\n\nThe root mean square of the residuals (RMSR) is  0.05 \n with the empirical chi square  1353.7  with prob &lt;  1.3e-282 \n\nFit based upon off diagonal values = 0.99\n\n\nLet’s evaluate the results. The first thing to review is the SS loadings row of results. The three values shown in that row are the eigenvalues for the 3 unique factors we specified. The first two factors both have eigenvalues &gt;1 while the third factor’s eigenvalue is not quite at 1. We also want to evaluate the proportion of the variance that each factor explains. Factor 1 clearly explains the most (~61%) while the third factor only adds 7% of additional explained variance.\nNext, we’ll review the actual factors and see which measures load on which factor. We see that the six negative emotions all seem to load on Factor 1 while the three positive emotions seem to load on Factor 2. The third factor seems to loosely be related to political anxiety and includes being afraid, worried, and nervous. While Factor 3 is not clearly unique in the unrotated factor analysis, the fact that there are reasonably strong factor loadings indicates that rotation may help to reveal a clearer pattern in the results.\n\n\n\n\n\n\nImportant\n\n\n\nOnce you assess which measures load onto which factors, always give the factors a substantive name that is intuitive.\n\n\n\nfa.diagram(pcf_result_no) #Graphs the relationship\n\n\n\n\nFinally, we can also graph the factor results and see that in the unrotated results all nine emotions load most strongly on Factor 1, even though the positive emotions and negatively related to the negative emotions. This graph takes the absolute value of the factor loadings and matches the highest factor loading for that item to the appropriate latent factor.\nWith our knowledge of the correlation matrix and the strong factors loadings from the unrotated pcf model, we will go ahead and rotate our factor analysis results. We will use an orthogonal rotation, varimax, which removes all shared variance between the latent factors.\n\npcf_result_var &lt;- principal(df,nfactors = 3,  rotate = \"varimax\") #PCF approach with varimax rotation\n\npcf_result_var #Rotation reveals cleaner factors that are obscured \n\nPrincipal Components Analysis\nCall: principal(r = df, nfactors = 3, rotate = \"varimax\")\nStandardized loadings (pattern matrix) based upon correlation matrix\n            RC1   RC2   RC3   h2   u2 com\nhope      -0.07  0.81 -0.28 0.74 0.26 1.3\nafraid     0.36 -0.23  0.82 0.85 0.15 1.5\noutrage    0.82 -0.19  0.35 0.83 0.17 1.5\nangry      0.82 -0.20  0.37 0.84 0.16 1.5\nhappy     -0.27  0.83 -0.19 0.81 0.19 1.3\nworried    0.42 -0.27  0.76 0.83 0.17 1.9\nproud     -0.25  0.86 -0.14 0.82 0.18 1.2\nirritated  0.79 -0.26  0.32 0.80 0.20 1.6\nnervous    0.36 -0.26  0.81 0.85 0.15 1.6\n\n                       RC1  RC2  RC3\nSS loadings           2.54 2.41 2.41\nProportion Var        0.28 0.27 0.27\nCumulative Var        0.28 0.55 0.82\nProportion Explained  0.34 0.33 0.33\nCumulative Proportion 0.34 0.67 1.00\n\nMean item complexity =  1.5\nTest of the hypothesis that 3 components are sufficient.\n\nThe root mean square of the residuals (RMSR) is  0.05 \n with the empirical chi square  1353.7  with prob &lt;  1.3e-282 \n\nFit based upon off diagonal values = 0.99\n\nfa.diagram(pcf_result_var) #Graphs the relationship \n\n\n\n\nLet’s review the same three items in this analysis. Starting with the SS loadings we see that the values are much more evenly aligned across the factors with eigenvalues &gt;=1 for all three factors. Because of this, the proportion of total variance explained is also very close across the three factors. This indicates that the rotation was needed and successful in revealing a clearer pattern in the data. There are likely three factors based on these results.\nNext, by examining the factor loadings for each latent factor, we see that the results follow our theoretical beliefs. Anger, outrage, and irritation loaded on Factor 1; hope, happiness, and pride loaded together on Factor 2; and anxiety, worry, and nervousness loaded on Factor 3. When we graph the results we see that clear pattern as well. Generally, the conclusion seems to be that political emotions are structured into three unique latent factors.\nThe following code compares the pcf approach used above alongside the paf approach, as well as changes the rotation time to oblimin for the oblique rotational approach.\n\n#General pcf code \"principal(data frame, nfactors=x, rotate)\"\n#General paf code \"fa(data frame, nfactors=x, rotate)\"\n\n#####Principal Components Factor Analysis, 3 factor solution with no rotation, orthogonal (varimax) & oblique (oblimin)\npcf_result_no &lt;- principal(df,nfactors = 3,  rotate = \"none\") #PCF approach with no rotation\npcf_result_no #Reports same Eigenvalues as reported in Scree Plot\n\nPrincipal Components Analysis\nCall: principal(r = df, nfactors = 3, rotate = \"none\")\nStandardized loadings (pattern matrix) based upon correlation matrix\n            PC1  PC2   PC3   h2   u2 com\nhope      -0.63 0.57 -0.11 0.74 0.26 2.0\nafraid     0.82 0.21  0.35 0.85 0.15 1.5\noutrage    0.81 0.30 -0.31 0.83 0.17 1.6\nangry      0.82 0.29 -0.29 0.84 0.16 1.5\nhappy     -0.72 0.54  0.09 0.81 0.19 1.9\nworried    0.85 0.19  0.26 0.83 0.17 1.3\nproud     -0.68 0.58  0.11 0.82 0.18 2.0\nirritated  0.81 0.22 -0.31 0.80 0.20 1.4\nnervous    0.84 0.19  0.35 0.85 0.15 1.4\n\n                       PC1  PC2  PC3\nSS loadings           5.47 1.28 0.62\nProportion Var        0.61 0.14 0.07\nCumulative Var        0.61 0.75 0.82\nProportion Explained  0.74 0.17 0.08\nCumulative Proportion 0.74 0.92 1.00\n\nMean item complexity =  1.6\nTest of the hypothesis that 3 components are sufficient.\n\nThe root mean square of the residuals (RMSR) is  0.05 \n with the empirical chi square  1353.7  with prob &lt;  1.3e-282 \n\nFit based upon off diagonal values = 0.99\n\npcf_result_var &lt;- principal(df,nfactors = 3,  rotate = \"varimax\") #PCF approach with varimax rotation\n\npcf_result_var #Rotation reveals cleaner factors that are obscured \n\nPrincipal Components Analysis\nCall: principal(r = df, nfactors = 3, rotate = \"varimax\")\nStandardized loadings (pattern matrix) based upon correlation matrix\n            RC1   RC2   RC3   h2   u2 com\nhope      -0.07  0.81 -0.28 0.74 0.26 1.3\nafraid     0.36 -0.23  0.82 0.85 0.15 1.5\noutrage    0.82 -0.19  0.35 0.83 0.17 1.5\nangry      0.82 -0.20  0.37 0.84 0.16 1.5\nhappy     -0.27  0.83 -0.19 0.81 0.19 1.3\nworried    0.42 -0.27  0.76 0.83 0.17 1.9\nproud     -0.25  0.86 -0.14 0.82 0.18 1.2\nirritated  0.79 -0.26  0.32 0.80 0.20 1.6\nnervous    0.36 -0.26  0.81 0.85 0.15 1.6\n\n                       RC1  RC2  RC3\nSS loadings           2.54 2.41 2.41\nProportion Var        0.28 0.27 0.27\nCumulative Var        0.28 0.55 0.82\nProportion Explained  0.34 0.33 0.33\nCumulative Proportion 0.34 0.67 1.00\n\nMean item complexity =  1.5\nTest of the hypothesis that 3 components are sufficient.\n\nThe root mean square of the residuals (RMSR) is  0.05 \n with the empirical chi square  1353.7  with prob &lt;  1.3e-282 \n\nFit based upon off diagonal values = 0.99\n\npcf_result_obl &lt;- principal(df,nfactors = 3,  rotate = \"oblimin\") #PCF approach with varimax rotation\n\npcf_result_obl #Rotation reveals cleaner factors that are obscured \n\nPrincipal Components Analysis\nCall: principal(r = df, nfactors = 3, rotate = \"oblimin\")\nStandardized loadings (pattern matrix) based upon correlation matrix\n            TC3   TC1   TC2   h2   u2 com\nhope      -0.20  0.19  0.83 0.74 0.26 1.2\nafraid     0.93  0.01  0.02 0.85 0.15 1.0\noutrage    0.04  0.89  0.01 0.83 0.17 1.0\nangry      0.08  0.86  0.00 0.84 0.16 1.0\nhappy      0.02 -0.12  0.85 0.81 0.19 1.0\nworried    0.80  0.12 -0.04 0.83 0.17 1.1\nproud      0.09 -0.11  0.89 0.82 0.18 1.0\nirritated  0.01  0.85 -0.08 0.80 0.20 1.0\nnervous    0.91  0.01 -0.02 0.85 0.15 1.0\n\n                       TC3  TC1  TC2\nSS loadings           2.55 2.50 2.32\nProportion Var        0.28 0.28 0.26\nCumulative Var        0.28 0.56 0.82\nProportion Explained  0.35 0.34 0.31\nCumulative Proportion 0.35 0.69 1.00\n\n With component correlations of \n      TC3   TC1   TC2\nTC3  1.00  0.72 -0.51\nTC1  0.72  1.00 -0.46\nTC2 -0.51 -0.46  1.00\n\nMean item complexity =  1\nTest of the hypothesis that 3 components are sufficient.\n\nThe root mean square of the residuals (RMSR) is  0.05 \n with the empirical chi square  1353.7  with prob &lt;  1.3e-282 \n\nFit based upon off diagonal values = 0.99\n\n#####Principal Axis Factor Analysis, 3 factor solution with no rotation, orthogonal (varimax) & oblique (oblimin)\npaf_result_no &lt;- fa(df, nfactors = 3, rotate = \"none\") #paf model\n\npaf_result_no #Reports same Eigenvalues as reported in Scree Plot\n\nFactor Analysis using method =  minres\nCall: fa(r = df, nfactors = 3, rotate = \"none\")\nStandardized loadings (pattern matrix) based upon correlation matrix\n            MR1  MR2   MR3   h2   u2 com\nhope      -0.59 0.43 -0.05 0.53 0.47 1.8\nafraid     0.81 0.19  0.26 0.76 0.24 1.3\noutrage    0.79 0.26 -0.25 0.75 0.25 1.4\nangry      0.82 0.26 -0.27 0.81 0.19 1.4\nhappy     -0.70 0.50  0.05 0.74 0.26 1.8\nworried    0.84 0.16  0.20 0.76 0.24 1.2\nproud     -0.67 0.55  0.06 0.75 0.25 1.9\nirritated  0.78 0.17 -0.20 0.68 0.32 1.2\nnervous    0.82 0.17  0.29 0.79 0.21 1.3\n\n                       MR1  MR2  MR3\nSS loadings           5.21 0.98 0.37\nProportion Var        0.58 0.11 0.04\nCumulative Var        0.58 0.69 0.73\nProportion Explained  0.79 0.15 0.06\nCumulative Proportion 0.79 0.94 1.00\n\nMean item complexity =  1.5\nTest of the hypothesis that 3 factors are sufficient.\n\ndf null model =  36  with the objective function =  6.5 with Chi Square =  53763.45\ndf of  the model are 12  and the objective function was  0.01 \n\nThe root mean square of the residuals (RMSR) is  0 \nThe df corrected root mean square of the residuals is  0.01 \n\nThe harmonic n.obs is  8255 with the empirical chi square  12.82  with prob &lt;  0.38 \nThe total n.obs was  8280  with Likelihood Chi Square =  80.36  with prob &lt;  3.5e-12 \n\nTucker Lewis Index of factoring reliability =  0.996\nRMSEA index =  0.026  and the 90 % confidence intervals are  0.021 0.032\nBIC =  -27.9\nFit based upon off diagonal values = 1\nMeasures of factor score adequacy             \n                                                   MR1  MR2  MR3\nCorrelation of (regression) scores with factors   0.98 0.88 0.79\nMultiple R square of scores with factors          0.95 0.78 0.62\nMinimum correlation of possible factor scores     0.91 0.57 0.24\n\npaf_result_var &lt;- fa(df, nfactors = 3, rotate = \"varimax\") #paf model\n\npaf_result_var\n\nFactor Analysis using method =  minres\nCall: fa(r = df, nfactors = 3, rotate = \"varimax\")\nStandardized loadings (pattern matrix) based upon correlation matrix\n            MR3   MR2   MR1   h2   u2 com\nhope      -0.15  0.67 -0.25 0.53 0.47 1.4\nafraid     0.40 -0.26  0.73 0.76 0.24 1.9\noutrage    0.76 -0.22  0.35 0.75 0.25 1.6\nangry      0.79 -0.23  0.36 0.81 0.19 1.6\nhappy     -0.24  0.80 -0.21 0.74 0.26 1.3\nworried    0.45 -0.30  0.69 0.76 0.24 2.1\nproud     -0.21  0.82 -0.17 0.75 0.25 1.2\nirritated  0.68 -0.29  0.36 0.68 0.32 1.9\nnervous    0.39 -0.29  0.75 0.79 0.21 1.8\n\n                       MR3  MR2  MR1\nSS loadings           2.31 2.19 2.07\nProportion Var        0.26 0.24 0.23\nCumulative Var        0.26 0.50 0.73\nProportion Explained  0.35 0.33 0.31\nCumulative Proportion 0.35 0.69 1.00\n\nMean item complexity =  1.6\nTest of the hypothesis that 3 factors are sufficient.\n\ndf null model =  36  with the objective function =  6.5 with Chi Square =  53763.45\ndf of  the model are 12  and the objective function was  0.01 \n\nThe root mean square of the residuals (RMSR) is  0 \nThe df corrected root mean square of the residuals is  0.01 \n\nThe harmonic n.obs is  8255 with the empirical chi square  12.82  with prob &lt;  0.38 \nThe total n.obs was  8280  with Likelihood Chi Square =  80.36  with prob &lt;  3.5e-12 \n\nTucker Lewis Index of factoring reliability =  0.996\nRMSEA index =  0.026  and the 90 % confidence intervals are  0.021 0.032\nBIC =  -27.9\nFit based upon off diagonal values = 1\nMeasures of factor score adequacy             \n                                                   MR3  MR2  MR1\nCorrelation of (regression) scores with factors   0.88 0.91 0.87\nMultiple R square of scores with factors          0.78 0.82 0.75\nMinimum correlation of possible factor scores     0.55 0.65 0.51\n\npaf_result_obl &lt;- fa(df,nfactors = 3,  rotate = \"oblimin\") #PCF approach with varimax rotation\n\npaf_result_obl #Rotation reveals cleaner factors that are obscured \n\nFactor Analysis using method =  minres\nCall: fa(r = df, nfactors = 3, rotate = \"oblimin\")\nStandardized loadings (pattern matrix) based upon correlation matrix\n            MR1   MR3   MR2   h2   u2 com\nhope      -0.16  0.10  0.69 0.53 0.47 1.1\nafraid     0.87  0.01  0.02 0.76 0.24 1.0\noutrage    0.01  0.87  0.02 0.75 0.25 1.0\nangry     -0.01  0.91  0.01 0.81 0.19 1.0\nhappy      0.00 -0.04  0.83 0.74 0.26 1.0\nworried    0.77  0.10 -0.03 0.76 0.24 1.0\nproud      0.06 -0.03  0.88 0.75 0.25 1.0\nirritated  0.06  0.73 -0.08 0.68 0.32 1.0\nnervous    0.91 -0.03 -0.01 0.79 0.21 1.0\n\n                       MR1  MR3  MR2\nSS loadings           2.33 2.22 2.02\nProportion Var        0.26 0.25 0.22\nCumulative Var        0.26 0.51 0.73\nProportion Explained  0.35 0.34 0.31\nCumulative Proportion 0.35 0.69 1.00\n\n With factor correlations of \n      MR1   MR3   MR2\nMR1  1.00  0.82 -0.60\nMR3  0.82  1.00 -0.56\nMR2 -0.60 -0.56  1.00\n\nMean item complexity =  1\nTest of the hypothesis that 3 factors are sufficient.\n\ndf null model =  36  with the objective function =  6.5 with Chi Square =  53763.45\ndf of  the model are 12  and the objective function was  0.01 \n\nThe root mean square of the residuals (RMSR) is  0 \nThe df corrected root mean square of the residuals is  0.01 \n\nThe harmonic n.obs is  8255 with the empirical chi square  12.82  with prob &lt;  0.38 \nThe total n.obs was  8280  with Likelihood Chi Square =  80.36  with prob &lt;  3.5e-12 \n\nTucker Lewis Index of factoring reliability =  0.996\nRMSEA index =  0.026  and the 90 % confidence intervals are  0.021 0.032\nBIC =  -27.9\nFit based upon off diagonal values = 1\nMeasures of factor score adequacy             \n                                                   MR1  MR3  MR2\nCorrelation of (regression) scores with factors   0.96 0.96 0.94\nMultiple R square of scores with factors          0.92 0.92 0.88\nMinimum correlation of possible factor scores     0.84 0.83 0.76\n\n\nThe results show interesting patterns. Both unrotated models reveal nearly identical results, where as all four rotated results revealed a likely three factor solution. By drawing the same conclusions from the different approaches, that makes it more likely that the three factor solution is best fit for the results.\n\n\n\n3.1.2 Confirmatory Factor Analysis (CFA)\nBecause we have a priori theory on the appropriate factor structure for these 9 political emotions, we can also use a confirmatory factor analysis to test that three factors does best fit the data.\nThe lavaan package is needed for this type of analysis. Because we are testing a priori theory with this approach, we have to specify which items create which latent factor. First, we will use the three factor solution theory, which suggests we will find:\n\nFactor 1 = Outrage, anger, and irritation\nFactor 2 = Pride, happiness, and hope\nFactor 3 = Being afraid, worried, and nervousness\n\n\n####Confirmatory Factor Analysis\n# Load required packages\nlibrary(lavaan)\n\n# Specify the CFA model; it must include the #\nmodel &lt;- '\n   # Factor 1\n   Factor1 =~ outrage + angry  + irritated\n   \n   # Factor 2\n   Factor2 =~ proud + happy  + hope\n   \n   # Factor 3\n   Factor3 =~ afraid  + nervous + worried\n'\n\n# Step 3: Fit the CFA model with varimax rotation\nfit &lt;- cfa(model, data = df)\n\n# Step 4: Summarize the results\nsummary(fit, standardized = TRUE, fit.measures = TRUE)  #Gives you summary statistics of the CFA\n\nlavaan 0.6.16 ended normally after 34 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        21\n\n                                                  Used       Total\n  Number of observations                          8217        8280\n\nModel Test User Model:\n                                                      \n  Test statistic                               403.975\n  Degrees of freedom                                24\n  P-value (Chi-square)                           0.000\n\nModel Test Baseline Model:\n\n  Test statistic                             53418.915\n  Degrees of freedom                                36\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.993\n  Tucker-Lewis Index (TLI)                       0.989\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)             -89742.500\n  Loglikelihood unrestricted model (H1)             NA\n                                                      \n  Akaike (AIC)                              179526.999\n  Bayesian (BIC)                            179674.292\n  Sample-size adjusted Bayesian (SABIC)     179607.558\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.044\n  90 Percent confidence interval - lower         0.040\n  90 Percent confidence interval - upper         0.048\n  P-value H_0: RMSEA &lt;= 0.050                    0.996\n  P-value H_0: RMSEA &gt;= 0.080                    0.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.018\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  Factor1 =~                                                            \n    outrage           1.000                               1.078    0.864\n    angry             0.995    0.009  105.099    0.000    1.072    0.891\n    irritated         0.854    0.009   93.941    0.000    0.920    0.829\n  Factor2 =~                                                            \n    proud             1.000                               0.975    0.850\n    happy             0.934    0.011   86.119    0.000    0.911    0.870\n    hope              0.872    0.012   72.027    0.000    0.850    0.731\n  Factor3 =~                                                            \n    afraid            1.000                               1.065    0.864\n    nervous           0.989    0.009  105.111    0.000    1.053    0.880\n    worried           0.959    0.009  105.883    0.000    1.021    0.884\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  Factor1 ~~                                                            \n    Factor2          -0.613    0.016  -39.099    0.000   -0.584   -0.584\n    Factor3           0.962    0.019   49.919    0.000    0.838    0.838\n  Factor2 ~~                                                            \n    Factor3          -0.646    0.016  -40.966    0.000   -0.622   -0.622\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .outrage           0.394    0.009   45.594    0.000    0.394    0.253\n   .angry             0.298    0.007   40.043    0.000    0.298    0.206\n   .irritated         0.385    0.008   50.521    0.000    0.385    0.313\n   .proud             0.366    0.009   38.550    0.000    0.366    0.278\n   .happy             0.266    0.008   34.284    0.000    0.266    0.243\n   .hope              0.631    0.012   53.734    0.000    0.631    0.466\n   .afraid            0.385    0.008   47.547    0.000    0.385    0.254\n   .nervous           0.324    0.007   44.753    0.000    0.324    0.226\n   .worried           0.292    0.007   43.961    0.000    0.292    0.219\n    Factor1           1.161    0.024   47.905    0.000    1.000    1.000\n    Factor2           0.950    0.021   44.977    0.000    1.000    1.000\n    Factor3           1.133    0.024   48.130    0.000    1.000    1.000\n\nsemPaths(fit, \"std\", whatLabels = \"est\", edge.label.cex = 0.8) #Graphs the CFA factor loadings\n\n\n\n\nThere are several things to evaluate in the results for a confirmatory factor analysis. We are more concerned with model fit here than we are in with exploratory factor analysis since we are testing specific hypotheses. We will evaluate the following CFA model fit using the following parameters:\n\nRoot Mean Square Error (RMSEA) where lower values = better fitting model.\nRMSEA &lt;=.06 considered good fit\nModel RMSEA = .044 which is below the (arbitrary) cut point of .06 to indicate good fitting model\nStandardized Root Mean Square Residual (SRMR) which is the standardized version of RMSEA.\n\nSRMR&lt;=.1 considered good fit\n\nModel SRMR = .018 which is below the (arbitrary) cut point of .1 to indicate a good fitting model\nComparative Fit Index reflects improvement in model fit compared to a null model.\n\nCloser to 1 indicates better fitting model with &gt;=.9 considered good fit\n\nModel CFI = .991 nearly 1 so indicates very good fitting model\nTucker Lewis Index is similar to CFI and reflects improvement in model fit compared to a null model.\n\nCloser to 1 indicates better fitting model with &gt;=.9 considered good fit\n\nModel TLI = .989 also higher than .9\nAIC and BIC are useful for comparing across nested models so we can compare these values to a two factor solution to see which best fits the data\n\nAIC = 179527;\nBIC = 179674\n\nWith large sample sizes like we have here, the chi-square calculation will almost always be significant so does not provide valuable information here.\n\nAll of the model fit parameters for the three factor solution indicate a good fitting model. This provides initial support that our hypothesized latent emotions do fact exist. However, we should change our model to evaluate if other factor solutions might fit the data better. We will use a two factor solution, combining the six negative emotions and three positive emotions into their own unique factors, and compare those results to the initial results.\nFirst, we create a new model that combines the emotions in the manner previously stated. Then we review the results.\n\nmodel2 &lt;- '\n   # Factor 1\n   Factor1 =~ outrage + angry  + irritated + afraid  + nervous + worried \n   \n   # Factor 2\n   Factor2 =~ proud + happy  + hope\n   \n\n'\n\n# Step 3: Fit the CFA model with varimax rotation\nfit2 &lt;- cfa(model2, data = df)\n\n# Step 4: Summarize the results\nsummary(fit2, standardized = TRUE, fit.measures = TRUE)\n\nlavaan 0.6.16 ended normally after 27 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        19\n\n                                                  Used       Total\n  Number of observations                          8217        8280\n\nModel Test User Model:\n                                                      \n  Test statistic                              3614.717\n  Degrees of freedom                                26\n  P-value (Chi-square)                           0.000\n\nModel Test Baseline Model:\n\n  Test statistic                             53418.915\n  Degrees of freedom                                36\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.933\n  Tucker-Lewis Index (TLI)                       0.907\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)             -91347.871\n  Loglikelihood unrestricted model (H1)             NA\n                                                      \n  Akaike (AIC)                              182733.741\n  Bayesian (BIC)                            182867.007\n  Sample-size adjusted Bayesian (SABIC)     182806.628\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.130\n  90 Percent confidence interval - lower         0.126\n  90 Percent confidence interval - upper         0.133\n  P-value H_0: RMSEA &lt;= 0.050                    0.000\n  P-value H_0: RMSEA &gt;= 0.080                    1.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.038\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  Factor1 =~                                                            \n    outrage           1.000                               1.003    0.804\n    angry             0.989    0.012   85.582    0.000    0.992    0.824\n    irritated         0.876    0.011   80.981    0.000    0.879    0.792\n    afraid            1.022    0.012   86.686    0.000    1.025    0.832\n    nervous           1.005    0.011   88.140    0.000    1.008    0.842\n    worried           0.989    0.011   90.551    0.000    0.992    0.858\n  Factor2 =~                                                            \n    proud             1.000                               0.975    0.850\n    happy             0.934    0.011   86.185    0.000    0.911    0.870\n    hope              0.871    0.012   71.972    0.000    0.849    0.730\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  Factor1 ~~                                                            \n    Factor2          -0.624    0.015  -40.925    0.000   -0.638   -0.638\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .outrage           0.549    0.010   55.378    0.000    0.549    0.353\n   .angry             0.464    0.009   53.993    0.000    0.464    0.321\n   .irritated         0.459    0.008   56.112    0.000    0.459    0.373\n   .afraid            0.467    0.009   53.375    0.000    0.467    0.308\n   .nervous           0.417    0.008   52.478    0.000    0.417    0.291\n   .worried           0.352    0.007   50.739    0.000    0.352    0.264\n   .proud             0.365    0.009   38.520    0.000    0.365    0.278\n   .happy             0.266    0.008   34.247    0.000    0.266    0.242\n   .hope              0.632    0.012   53.789    0.000    0.632    0.467\n    Factor1           1.006    0.023   43.291    0.000    1.000    1.000\n    Factor2           0.951    0.021   44.997    0.000    1.000    1.000\n\nsemPaths(fit2, \"std\", whatLabels = \"est\", edge.label.cex = 0.8) #Graphs the CFA factor loadings\n\n\n\n\nEvaluating the same model fit parameters as before, we can immediately see a worse fitting model compared to the prior three factor solution.\n\nRMSEA = .044 with three factor solution vs. .13 in two factor; closer to 0 the better -\nSRMR = .018 in three factor vs. .038 in 2 factor; closer to 0 the better -\nCFI = .991 in three factor vs. .933 in two factor, closer to 1 indicates better fitting model\nTLI = .989 in three factor vs. .907 in two factor, closer to 1 indicates better fitting model\nAIC =179527 in three factor vs. 182734 in two factor\nBIC =179674 in three factor vs. 182867 in two factor\n\nAcross all of the mode fit parameters, the three factor solution fits the data better than the two factor solution that combined all the negatively valenced emotions. These results match the results from the exploratory factor analysis as well.\n\n\n3.1.3 Conclusions\nIn this tutorial, we imported survey data and conducted various types of factor analysis techniques on political emotions in the United States. The results largely follow the prevailing theoretical belief that there are three distinct emotional latent factors:\n\nAversion to Politics: Measures anger, outrage, and irritation\nWorry about Politics: Measures fear, worry, and nervousness\nEnthusiasm about Politics: Measures happiness, hope, and pride\n\n\n\n\n\nMarcus, George E, Michael MacKuen, Jennifer Wolak, and Luke Keele. 2006. “The Measure and Mismeasure of Emotion.” In Feeling Politics: Emotion in Political Information Processing, 31–45. Springer."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Marcus, George E, Michael MacKuen, Jennifer Wolak, and Luke Keele. 2006.\n“The Measure and Mismeasure of Emotion.” In Feeling\nPolitics: Emotion in Political Information Processing, 31–45.\nSpringer."
  },
  {
    "objectID": "ch01intro.html#working-with-simulations-and-probability",
    "href": "ch01intro.html#working-with-simulations-and-probability",
    "title": "1  Introduction and Basics of Survey Methods",
    "section": "1.1 Working with Simulations and Probability",
    "text": "1.1 Working with Simulations and Probability\nWe will begin with using simulations in R. Simulations are important to understanding the statistical properties of your data, so it is important that you familiarize yourself with why we do this and how to do this in R.\n\n\n\n\n\n\nSimulation and Probability\n\n\n\nProbability helps us understand how frequently an outcome would occur if we observed the random process an infinite number of times. Simulation helps us model this process so we can understand what the “true probability” - which we never know - looks like.\n\n\n\n1.1.0.1 Birthday Problem\nIn a room of 15 people, what is the probability that 2 people will have the same birthday? This is a classic probability problem with numbers that typically surprise people.\nWe can first calculate the probability using conventional hand calculations and permutations. This gives us the true probability for any 15 random people having the same birthday at ‘0.2529013’.\nWe can also use simulation to calculate the same probability. Using simulation will then allow us to do other interesting calculations on the birthday problem. With the simulation setup, we use random sampling - 15 people and their random birthdays - from a population 100 times then count the number of times that two or more people have the same birthday.\n\n## Calculating by Hand ##\n1 - ((365*364*363*362*361*360*359*358*357*356*355*354*353*352*351)/(365^15))\n\n[1] 0.2529013\n\n## Calculating with Simulation ##\nset.seed(1234) #For replication\nreps &lt;- 100  #Number of times we draw a random sample of 15 people \npeople &lt;- 15 #Number of people to randomly draw for birthday calculation\nalldays &lt;- 1:365 #Numeric lists of birthdays \nsameday &lt;- 0   #Needed to calculate matching birthday or not \nfor (i in 1:reps) { #For loop that samples birthdays with replacement \n     room &lt;- sample(alldays, people, replace = TRUE) \n     if (length(unique(room)) &lt; people){\n          sameday &lt;- sameday+1\n     }\n} \nprint(sameday/reps)\n\n[1] 0.26\n\n\nFrom both methods, we get a probability of roughly 25% that two people in any room of 15 random people will have the same birthday. We can use simulation to easily advance our understanding of the birthday problem. Suppose we want to know at what number of people the probability that two people will have the same birthday equals 50%? We can manually plug and play different values but that isn’t necessary. We can add additional conditions to our simulation to identify at what number the probability of 2 people having the same birthday is 50%.\nTo do this, we will change our code slightly. To simplify the process, we first write a function that calculates the probability based on the previous code. This code is identical to the previous example except we are now saving it as a function so that we can easily change the number of people in our birthday analysis.\n\nnum_simulations &lt;- 100  # Number of simulations i.e. times we will draw random samples of people to test for matching birthdays \n\nsameday &lt;- 0          # We use this to count if 2 people have same birthday in the simulation\n\ncalculate_birthday_prob &lt;- function(num_people) {\n  for (i in 1:num_simulations) {\n    birthdays &lt;- sample(1:365, num_people, replace = TRUE)  # Generate random birthdays\n    if (length(birthdays) != length(unique(birthdays))) {   # Check for shared birthdays\n    sameday &lt;- sameday + 1\n    }\n  }\n    probability &lt;- sameday / num_simulations  # Calculate the probability\n    return(probability)\n}\n\n# Now, we create a vector of people in the group using the 'num_people' line of code. \nnum_people &lt;- 2:100 #Here we are saying to use 2, 3, 4, 5, ...., 100 people in the calculation of matching birthdays \n\nprobabilities &lt;- sapply(num_people, calculate_birthday_prob) #This code applies the above numbers to the birthday problem function and saves it as a vector\n\ndata &lt;- data.frame(num_people = num_people, probabilities = probabilities) # Changes the vector to a data frame for easier graphing\n\nLet’s plot these data so we can see our birthday simulation visually:\n\n# Plot the results using ggplot\nggplot(data, aes(x = num_people, y = probabilities)) +\n  geom_line() +\n  geom_hline(yintercept = 0.5, linetype = \"dashed\", color = \"red\") +\n  labs(x = \"Number of People\", y = \"Probability\") +\n  ggtitle(\"Probability of Shared Birthdays\") +\n  theme_minimal()\n\n\n\n\nWith this simulation done, we can see that with roughly 25 people in a group there is a 50% chance that 2 people will have the exact same birthday. At around 60 people in a group the probability that 2 people will have the same birthday is virtually 100%.\nYou might be wondering why the line is so jagged. If so, that is the correct intuition. It occurs because we only randomly sampled 100 cases each time. As the number of cases sampled increases, the estimates will become more precise making the line smoother.\nLet’s illustrate this by raising the number of cases sampled from 100 to 1,000 and then rerun the analysis.\n\nnum_simulations &lt;- 1000  # Number of simulations i.e. times we will draw random samples of people to test for matching birthdays \n\nnum_success &lt;- 0          # We use this to count if 2 people have same birthday in the simulation\n\ncalculate_birthday_prob &lt;- function(num_people) {\n  for (i in 1:num_simulations) {\n    birthdays &lt;- sample(1:365, num_people, replace = TRUE)  # Generate random birthdays\n    if (length(birthdays) != length(unique(birthdays))) {   # Check for shared birthdays\n    num_success &lt;- num_success + 1\n    }\n  }\n    probability &lt;- num_success / num_simulations  # Calculate the probability\n    return(probability)\n}\n\n# Now, we create a vector of people in the group using the 'num_people' line of code. \nnum_people &lt;- 2:100 #Here we are saying to use 2, 3, 4, 5, ...., 100 people in the calculation of matching birthdays \n\nprobabilities &lt;- sapply(num_people, calculate_birthday_prob) #This code applies the above numbers to the birthday problem function and saves it as a vector\n\ndata &lt;- data.frame(num_people = num_people, probabilities = probabilities) # Changes the vector to a data frame for easier graphing\n\nLet’s plot the result again, however this time to see our line with more samples included:\n\n# Plot the results using ggplot\nggplot(data, aes(x = num_people, y = probabilities)) +\n  geom_line() +\n  geom_hline(yintercept = 0.5, linetype = \"dashed\", color = \"red\") +\n  labs(x = \"Number of People\", y = \"Probability\") +\n  ggtitle(\"Probability of Shared Birthdays\") +\n  theme_minimal()\n\n\n\n\nNow, the line is much smoother which is entirely because the number of random cases sampled increased from 100 to 1,000.\nIf we wanted to, we could do this analysis at the same time with a slightly updated function. We need to add code to randomly draw samples with a different number of cases. Using the code below, we now see both lines plotted next to each other. The line for the sample size of 100 is much more jagged because the confidence interval is larger due to the smaller sample size compared to the line for the sample size of 1,000. By increasing the number of trials in each sample from 100 to 1,000, our estimates become more precise, thus leading to a smoother graph.\n\ncalculate_birthday_prob &lt;- function(num_people, num_simulations) {\n  for (i in 1:num_simulations) {\n    birthdays &lt;- sample(1:365, num_people, replace = TRUE)  # Generate random birthdays\n        if (length(birthdays) != length(unique(birthdays))) {   # Check for shared birthdays\n      num_success &lt;- num_success + 1\n    }\n  }\n    probability &lt;- num_success / num_simulations  # Calculate the probability\n    return(probability)\n}\n\n# We set the number of people in the group and  the number of times to draw random groups. We'll do 100, 1000, and 5000 but the more times you simulate the longer it takes to complete. \nnum_people &lt;- 2:75\n\nnum_simulations &lt;- c(100, 1000)\n\n# Create empty data frame to store results\nresults &lt;- data.frame()\n\n# Iterate through number of simulations\nfor (sim in num_simulations) {\n  probabilities &lt;- sapply(num_people, calculate_birthday_prob, num_simulations = sim)\n  # Append results to data frame\n  results &lt;- rbind(results, data.frame(num_people = num_people, probabilities = probabilities, num_simulations = sim))\n}\n\n# Plot the results using ggplot\nggplot(results, aes(x = num_people, y = probabilities, color = as.factor(num_simulations))) +\n  theme_bw(base_size = 20) + geom_line(linewidth=.75) +\n  geom_hline(yintercept = 0.5, linetype = \"dashed\", color = \"red\") +\n  labs(x = \"Number of People\", y = \"Probability\", color = \"Simulations\") +\n  ggtitle(\"Probability of Shared Birthdays\") +\n  theme_minimal()\n\n\n\n\nWhat did we just learn? A few interesting and important things. First, it does not take all that many people in a room before you can be confident that two people have the same birthday. Next, we learned how to use simulation to illustrate important principles of probabilities. Specifically, the more random trials we take and the larger our sample size gets, the more precise our estimates get to the true population value we are estimating. With this in mind, we can better understand our sampling distributions and how that impacts our analyses and results when using survey data."
  },
  {
    "objectID": "ch08measurement.html#computing-cronbachs-alpha",
    "href": "ch08measurement.html#computing-cronbachs-alpha",
    "title": "9  Measurement & Item Scaling",
    "section": "9.1 Computing Cronbach’s Alpha",
    "text": "9.1 Computing Cronbach’s Alpha\nThis tutorial will cover calculating Cronbach’s Alpha for a unidimensional scale. We will use the 2020 American National Election Survey and its scale for “Racial Resentment”. This scale is designed to measure feelings of racial animosity towards black Americans in a more indirect way than simply asking respondents if they are racist. It consists of 4 variables:\n\n“Irish, Italian, Jewish and many other minorities overcame prejudice and worked their way up. Blacks should do the same without any special favors.”\n“Generations of slavery and discrimination have created conditions that make it difficult for blacks to work their way out of the lower class.”\n“Over the past few years, blacks have gotten less than they deserve.”\n“It’s really a matter of some people not trying hard enough; if blacks would only try harder they could be just as well off as whites.”\n\nWe test the alpha level for reliability for these four items to better understand if they reliably measure the same underlying latent concept.\nLet’s start by importing the packages we need, as well as our ANES data.\n\nlibrary(psych)\nlibrary(corrplot)\nlibrary(tidyverse)\n\nlibrary(haven)\nlibrary(MASS)\nlibrary(survey)\nlibrary(Hmisc)\nlibrary(stats)\nlibrary(skimr) #To quickly review your data\n\nanes &lt;- read_dta('C:/Users/Stefani.Langehennig/OneDrive - University of Denver/Documents/research/surveys-textbook-data/anes_timeseries_2020_stata_20220210.dta')\n\nNext, we can subset our data and start exploring some of the trends around our variables of interest.\n\ndf &lt;- data.frame(anes$V202300, anes$V202301, anes$V202302, anes$V202303)\n\nnew_names &lt;- c(\"resent_gen\", \"resent_fav\", \"resent_try\", \"resent_deserve\")#Give variables more informative names \n\n# Update column names\ncolnames(df) &lt;- new_names #Apply names to data frame for analysis\n\nskim(df) #Check for missing data that should be recoded\n\n\nData summary\n\n\nName\ndf\n\n\nNumber of rows\n8280\n\n\nNumber of columns\n4\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n4\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nresent_gen\n0\n1\n1.94\n3.17\n-9\n1\n2\n4\n5\n▁▂▁▆▇\n\n\nresent_fav\n0\n1\n1.77\n3.08\n-9\n1\n2\n4\n5\n▁▂▁▇▇\n\n\nresent_try\n0\n1\n1.90\n3.14\n-9\n1\n3\n4\n5\n▁▂▁▆▇\n\n\nresent_deserve\n0\n1\n2.36\n3.27\n-9\n2\n3\n5\n5\n▁▁▁▃▇\n\n\n\n\ndf[df &lt;= -1] &lt;- NA #Recode negative values to NA for analysis\n\nskim(df) #Validate that missing data is now treated as NA\n\n\nData summary\n\n\nName\ndf\n\n\nNumber of rows\n8280\n\n\nNumber of columns\n4\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n4\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nresent_gen\n911\n0.89\n2.94\n1.47\n1\n2\n3\n4\n5\n▇▇▆▆▇\n\n\nresent_fav\n899\n0.89\n2.73\n1.45\n1\n1\n2\n4\n5\n▇▇▃▅▅\n\n\nresent_try\n914\n0.89\n2.90\n1.40\n1\n2\n3\n4\n5\n▇▇▇▆▇\n\n\nresent_deserve\n913\n0.89\n3.42\n1.37\n1\n2\n4\n5\n5\n▃▅▅▅▇\n\n\n\n\n\nLet’s start by examining the correlation matrix for the individual survey items of interest. All items are correlated at .6 or higher as we should expect if they are measuring the same latent concept. Note that two of the items have negative correlations. This indicates that the variable is reverse coded so that higher levels of the measure is equal to lower levels of racial resentment. This will need to be remembered if combining to create a new latent measure of racial resentment.\n\n# Calculate correlation matrix\ncor_matrix &lt;- cor(df, use = \"pairwise.complete.obs\")\n\n# Display correlation matrix as a table\ncor_table &lt;- round(cor_matrix, 2)\n\nprint(cor_table)\n\n               resent_gen resent_fav resent_try resent_deserve\nresent_gen           1.00      -0.62      -0.65           0.70\nresent_fav          -0.62       1.00       0.74          -0.60\nresent_try          -0.65       0.74       1.00          -0.62\nresent_deserve       0.70      -0.60      -0.62           1.00\n\n# Plot correlation matrix as a heatmap\ncorrplot(cor_matrix, method = \"color\")\n\n\n\n\nNow that we have a sense of the correlations between our variables of interest, we can compute our Cronbach’s Alpha using the psych package in R.\n\n#Calculate Cronbach's Alpha using 'psych' package\n##Generic format 'alpha(data, na.rm=TRUE, check.keys=TRUE) \n#check.keys=TRUE is important as it checks the scale direction and, if necessary, flips the order of the scale prior to running the analysis. This deals with the negative correlation we saw in the correlation matrix. Default = FALSE and code will not run if you have an oppositely signed variable. \nlibrary(psych)\n\nalpha(df, na.rm = TRUE, check.keys=TRUE) #Run the alpha calculation\n\nInterpreting alpha is very straightforward. First, we will evaluate the actual alpha level, which here is a robust .88. Remember, alpha ranges from 0 to 1, with higher values indicating a more reliable scale. Based on the Kaiser criterion, the general cut point for a reliable scale is .7 or larger. Alpha of .88 here represents a very strong and internally reliable scale.\nThe second thing to evaluate are the individual items in the analysis, specifically how alpha would change if it were to be dropped. This metric gives insight into how well each individual item fits the overall latent factor. If alpha goes up with its removal, that indicates the individual item might not truly be part of that concept and should potentially be removed from the scale. If the alpha goes down with its removal, that indicates the individual item is important to the overall latent factor and should be kept in the scale.\nFor illustration purposes, let’s add three additional variables that are not related to the racial resentment scale. If the new items are not related to racial resentment, we will see that removing the new items would result in a higher alpha level. We’ll add a series of three questions designed to measure rural resentment, or the perception that Americans who live in rural parts of the country are being overlooked and have too little influence in politics. These three questions measure:\n\nHow much assistance rural areas get from government\nHow much influence rural areas have in government\nHow much respect rural people get from others\n\nNote, this is entirely for pedagogical purposes. I do not believe these two concepts to be related. Like before, let’s go through our steps of subsetting the data and checking out our correlations.\n\ndf &lt;- data.frame(anes$V202300, anes$V202301, anes$V202302, anes$V202303, anes$V202276x , anes$V202279x , anes$V202282x)\n\nskim(df)\n\n\nData summary\n\n\nName\ndf\n\n\nNumber of rows\n8280\n\n\nNumber of columns\n7\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n7\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nanes.V202300\n0\n1\n1.94\n3.17\n-9\n1\n2\n4\n5\n▁▂▁▆▇\n\n\nanes.V202301\n0\n1\n1.77\n3.08\n-9\n1\n2\n4\n5\n▁▂▁▇▇\n\n\nanes.V202302\n0\n1\n1.90\n3.14\n-9\n1\n3\n4\n5\n▁▂▁▆▇\n\n\nanes.V202303\n0\n1\n2.36\n3.27\n-9\n2\n3\n5\n5\n▁▁▁▃▇\n\n\nanes.V202276x\n0\n1\n3.51\n3.60\n-7\n4\n4\n6\n7\n▂▁▁▇▆\n\n\nanes.V202279x\n0\n1\n3.44\n3.66\n-7\n4\n4\n6\n7\n▂▁▁▇▇\n\n\nanes.V202282x\n0\n1\n3.82\n3.67\n-7\n4\n4\n6\n7\n▂▁▁▇▇\n\n\n\n\nnew_names &lt;- c(\"resent_gen\", \"resent_fav\", \"resent_try\", \"resent_deserve\", \"rural_assist\", \"rural_influence\", \"rural_respect\")#Give variables more informative names \n\n# Update column names\ncolnames(df) &lt;- new_names #Apply names to data frame for analysis\n\nskim(df) #Check for missing data that should be recoded\n\n\nData summary\n\n\nName\ndf\n\n\nNumber of rows\n8280\n\n\nNumber of columns\n7\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n7\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nresent_gen\n0\n1\n1.94\n3.17\n-9\n1\n2\n4\n5\n▁▂▁▆▇\n\n\nresent_fav\n0\n1\n1.77\n3.08\n-9\n1\n2\n4\n5\n▁▂▁▇▇\n\n\nresent_try\n0\n1\n1.90\n3.14\n-9\n1\n3\n4\n5\n▁▂▁▆▇\n\n\nresent_deserve\n0\n1\n2.36\n3.27\n-9\n2\n3\n5\n5\n▁▁▁▃▇\n\n\nrural_assist\n0\n1\n3.51\n3.60\n-7\n4\n4\n6\n7\n▂▁▁▇▆\n\n\nrural_influence\n0\n1\n3.44\n3.66\n-7\n4\n4\n6\n7\n▂▁▁▇▇\n\n\nrural_respect\n0\n1\n3.82\n3.67\n-7\n4\n4\n6\n7\n▂▁▁▇▇\n\n\n\n\ndf[df &lt;= -1] &lt;- NA #Recode negative values to NA for analysis\n\nskim(df) #Validate that missing data is now treated as NA\n\n\nData summary\n\n\nName\ndf\n\n\nNumber of rows\n8280\n\n\nNumber of columns\n7\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n7\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nresent_gen\n911\n0.89\n2.94\n1.47\n1\n2\n3\n4\n5\n▇▇▆▆▇\n\n\nresent_fav\n899\n0.89\n2.73\n1.45\n1\n1\n2\n4\n5\n▇▇▃▅▅\n\n\nresent_try\n914\n0.89\n2.90\n1.40\n1\n2\n3\n4\n5\n▇▇▇▆▇\n\n\nresent_deserve\n913\n0.89\n3.42\n1.37\n1\n2\n4\n5\n5\n▃▅▅▅▇\n\n\nrural_assist\n994\n0.88\n4.74\n1.33\n1\n4\n4\n6\n7\n▁▁▇▁▆\n\n\nrural_influence\n969\n0.88\n4.64\n1.62\n1\n4\n4\n6\n7\n▂▁▇▁▇\n\n\nrural_respect\n966\n0.88\n5.06\n1.28\n1\n4\n5\n6\n7\n▁▁▇▂▇\n\n\n\n\n\n\n#Run correlations between the items in the proposed scale\n# Calculate correlation matrix\ncor_matrix &lt;- cor(df, use = \"pairwise.complete.obs\")\n\n# Display correlation matrix as a table\ncor_table &lt;- round(cor_matrix, 2)\n\nprint(cor_table)\n\n                resent_gen resent_fav resent_try resent_deserve rural_assist\nresent_gen            1.00      -0.62      -0.65           0.70        -0.09\nresent_fav           -0.62       1.00       0.74          -0.60         0.09\nresent_try           -0.65       0.74       1.00          -0.62         0.08\nresent_deserve        0.70      -0.60      -0.62           1.00        -0.07\nrural_assist         -0.09       0.09       0.08          -0.07         1.00\nrural_influence      -0.32       0.32       0.31          -0.28         0.44\nrural_respect        -0.16       0.16       0.17          -0.15         0.37\n                rural_influence rural_respect\nresent_gen                -0.32         -0.16\nresent_fav                 0.32          0.16\nresent_try                 0.31          0.17\nresent_deserve            -0.28         -0.15\nrural_assist               0.44          0.37\nrural_influence            1.00          0.46\nrural_respect              0.46          1.00\n\n# Plot correlation matrix as a heatmap\ncorrplot(cor_matrix, method = \"color\")\n\n\n\n\nExamining the correlations, we see that the three new items are not strongly related to the existing racial resentment items and even have relatively weak correlations between each other. This will help illustrate how to identify items that do not belong in a scale.\nNext, we re-estimate the alpha level with the three additional variables included. Remember, the initial alpha level was .88 so anything below that would indicate a less reliable scale with items that might not belong.\n\nalpha(df, na.rm = TRUE, check.keys=TRUE) #Run the alpha calculation\n\nThe first thing to note is that the overall alpha of this new seven item scale is lower, ~.79, than the original .88 results indicating a less reliable scale. The new items included have harmed the reliability of the scale overall.\nNext, we look at the alpha level if each item were removed. The four initial items included in the racial resentment scale all have alpha levels lower than the overall alpha, which indicates the scale would be worse if any of them were removed. That is what we expected to happen. For the other three items, the alpha levels would either stay the same or get larger if each individual one was removed, indicating that these new items probably do not fit the overall latent concept of racial resentment.\nCoupling these findings with the small correlations and the lack of theory, we would conclude that the rural resentment questions do not measure the same concept as racial resentment.\nThe final step to know is how to combine existing survey questions into a scale. The easiest way, provided they are on the exact same scale (which generally should be the case), is to combine them and divide by the total number of items in the scale. For the racial resentment scale, we would sum across the four items and then divide by 4, since there are 4 items in the scale. However, this is when we must flip the scale so that higher values equal the same thing.\n\n\n\n\n\n\nImportant\n\n\n\nAlways check the coding scheme for your variables. Some variables may be coded in different directions (“reverse coded”). Not having variables coded in the same direction can introduce bias into your analyses and result in wrong conclusions.\n\n\nWe start by creating new variables for each of the four racial resentment questions. First, we examine the codebook and determine that the resentment favoritism and try harder questions are reverse coded so that must be accounted for when creating the new measures. We simply flip the scale direction while saving a new measure in the existing anes data frame.\n\n#Working out of the original data frame, anes, so we can save the new variable there for analysis purposes. \n\nanes &lt;- anes %&gt;% #This creates new variable \n  mutate(resent_gen = case_when(\n    V202300 ==1 ~ 1,\n    V202300 ==2 ~ 2,\n    V202300 ==3 ~ 3,\n    V202300 ==4 ~ 4,\n    V202300 ==5 ~ 5\n  ))\n\nanes &lt;- anes %&gt;% #Note the reverse coding\n  mutate(resent_fav = case_when(\n    V202301 ==1 ~ 5,\n    V202301 ==2 ~ 4,\n    V202301 ==3 ~ 3,\n    V202301 ==4 ~ 2,\n    V202301 ==5 ~ 1\n  ))\n\nanes &lt;- anes %&gt;%  #Note the reverse coding \n  mutate(resent_try = case_when(\n    V202302 ==1 ~ 5,\n    V202302 ==2 ~ 4,\n    V202302 ==3 ~ 3,\n    V202302 ==4 ~ 2,\n    V202302 ==5 ~ 1\n  ))\n\nanes &lt;- anes %&gt;%\n  mutate(resent_deserve = case_when(\n    V202303 ==1 ~ 1,\n    V202303 ==2 ~ 2,\n    V202303 ==3 ~ 3,\n    V202303 ==4 ~ 4,\n    V202303 ==5 ~ 5\n  ))\n\n#With the new variables coded in same direction, we create the new scale 'racial_resent'\nanes &lt;- anes %&gt;%\n  mutate(racial_resent = (resent_gen + resent_fav + resent_try + resent_deserve) / 4) #Add across individual items and divide by the total number of items. Note this uses casewise deletion so any case that did not answer each question is removed from the calculation \n\nsummary(anes$racial_resent) #Examine the \n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n   1.00    2.25    3.00    3.18    4.25    5.00     941 \n\nanes %&gt;% \n  count(racial_resent)\n\n# A tibble: 18 × 2\n   racial_resent     n\n           &lt;dbl&gt; &lt;int&gt;\n 1          1      410\n 2          1.25   218\n 3          1.5    332\n 4          1.75   326\n 5          2      406\n 6          2.25   385\n 7          2.5    451\n 8          2.75   473\n 9          3      753\n10          3.25   376\n11          3.5    395\n12          3.75   364\n13          4      428\n14          4.25   372\n15          4.5    377\n16          4.75   400\n17          5      873\n18         NA      941\n\n# Create a new df object with updated variables\n# df &lt;- anes %&gt;%\n  #select(resent_gen, resent_fav, resent_try, resent_deserve, racial_resent) \n\nThe final check is to correlate the new scale with the individual items. Let’s take a look at the correlations and correlation matrix for these updated variables:\n\n# Calculate the correlation matrix\ncor_matrix &lt;- cor(df, use = \"complete.obs\") #Note \"complete.obs\" removes any case with a NA value  \n\n# View the correlation matrix\nprint(cor_matrix)\n\n                 resent_gen  resent_fav  resent_try resent_deserve rural_assist\nresent_gen       1.00000000 -0.62106012 -0.64831418     0.70164739  -0.09200728\nresent_fav      -0.62106012  1.00000000  0.74010498    -0.59809976   0.09488715\nresent_try      -0.64831418  0.74010498  1.00000000    -0.61623995   0.08439315\nresent_deserve   0.70164739 -0.59809976 -0.61623995     1.00000000  -0.06925494\nrural_assist    -0.09200728  0.09488715  0.08439315    -0.06925494   1.00000000\nrural_influence -0.31386554  0.32085224  0.31090924    -0.27931345   0.44199218\nrural_respect   -0.16305488  0.16372950  0.16845675    -0.14736070   0.37249420\n                rural_influence rural_respect\nresent_gen           -0.3138655    -0.1630549\nresent_fav            0.3208522     0.1637295\nresent_try            0.3109092     0.1684567\nresent_deserve       -0.2793135    -0.1473607\nrural_assist          0.4419922     0.3724942\nrural_influence       1.0000000     0.4594196\nrural_respect         0.4594196     1.0000000\n\n# Graph the results\ncorrplot(cor_matrix, method = \"color\")\n\n\n\n\nLastly, we want to examine the newly created measure to ensure that it was created appropriately. Since the recoding approach we took kept the original scale in tact of 1-5, we should see 1 as the minimum value and 5 as the maximum. That is what we see in the results. We also see values between the whole numbers such as 1.25 and 1.5 since the denominator in our recode was 4. All of these indicators look good.\nBased on the correlations above, the new scale should be highly, but not perfectly, correlated with each of the individual items. That is exactly what we see here. The correlation is at least .84 between the new scale and the individual items but none are perfectly correlated. This indicates that our new scale was created successfully and is now ready to be analyzed."
  },
  {
    "objectID": "ch10analysis.html#analyzing-dichotomous-dependent-variables",
    "href": "ch10analysis.html#analyzing-dichotomous-dependent-variables",
    "title": "11  Survey Analysis: Logitistic & Probit Regression",
    "section": "11.1 Analyzing Dichotomous Dependent Variables",
    "text": "11.1 Analyzing Dichotomous Dependent Variables\nWe are going use the binomial distribution to simulate a dichotomous dependent variable, y, with a known relationship to an independent variable, x. We will then estimate the relationship using OLS, logistic, and probit regressions and compare the results. Along the way, we will talk about the differences in the modeling approaches.\nLet’s start with simulating some data for our analysis.\n\n# Create X as a sequence from 1 to 11\nX &lt;- 1:11\n\n# Define the probabilities for Y = 1 at each X value with steeper relationship in middle and flatter at edges\nprobabilities &lt;- c(0.95, .925, .875, 0.8, .7, 0.5, 0.3, 0.2, 0.125, .075, 0.05)\n\n# Simulate data with binomial distribution for Y\nsample &lt;- 1000  # Number of trials for each x \n\n# Create an empty data frame to store the results\nsimulated_data &lt;- data.frame(X = numeric(),\n                             Y = numeric(),\n                             stringsAsFactors = FALSE)\n\n# Simulate Y=1 probabilistically for each X value iteration\nfor (i in 1:sample) {\n  # Sample Y=1 probabilistically at each X value\n  simulated_Y &lt;- sapply(probabilities, function(p) sample(c(0, 1), size = 1, prob = c(1 - p, p))) #Probabilistically draw 1 | 0 at the specified probability level \n  \n  # Create a data frame for the current iteration\n  iteration_data &lt;- data.frame(x = X, y = simulated_Y)\n  \n  # Append the current iteration data to the main data frame\n  simulated_data &lt;- rbind(simulated_data, iteration_data)\n}\n\nhead(simulated_data) #Look at first few cases \n\n  x y\n1 1 1\n2 2 1\n3 3 1\n4 4 1\n5 5 1\n6 6 0\n\n\n\n11.1.1 Reviewing Simulated Data\nUsing the group_by function from the tidyverse package, we check the probability that Y = 1 at each X level to see if the simulation worked as hoped. The results show that it did with P(Y=1∣X )roughly following the population parameters stipulated earlier.\n\n\n# A tibble: 11 × 2\n       x mean_y\n   &lt;int&gt;  &lt;dbl&gt;\n 1     1  0.943\n 2     2  0.927\n 3     3  0.864\n 4     4  0.789\n 5     5  0.708\n 6     6  0.506\n 7     7  0.292\n 8     8  0.202\n 9     9  0.133\n10    10  0.085\n11    11  0.061\n\n\n\n\n11.1.2 Estimating Regression Models\nWith our simulated data, now we’ll estimate three separate regressions changing the Generalized Linear Model (GLM) type using: Logistic, Probit, and OLS.\nThe two models, logistic and probit, are designed to work with dichotomous DVs and should work better with our simulated data compared to the OLS estimator (which assumes a normal distribution). We use GLMs to estimate all three models, including the OLS, to more easily compare model fit across estimators. Both the logitistic and probit models use the family=binomial argument in the code but have the different link functions to account for the different calculations. The OLS GLM model uses the Gaussian distribution family with the standard identity link function.\nWe estimate each of the three models than compare the results using the stargazer package.\n\n# Logistic regression\nlogit &lt;- glm(y ~ x , data = simulated_data, family = binomial(link = \"logit\"))\nsummary(logit)\n\n\nCall:\nglm(formula = y ~ x, family = binomial(link = \"logit\"), data = simulated_data)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  3.73811    0.07101   52.65   &lt;2e-16 ***\nx           -0.62191    0.01101  -56.47   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 15249.2  on 10999  degrees of freedom\nResidual deviance:  9465.7  on 10998  degrees of freedom\nAIC: 9469.7\n\nNumber of Fisher Scoring iterations: 5\n\n# Probit regression\nprobit &lt;- glm(y ~ x , data = simulated_data, family = binomial(link = \"probit\"))\nsummary(probit)\n\n\nCall:\nglm(formula = y ~ x, family = binomial(link = \"probit\"), data = simulated_data)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  2.143882   0.036857   58.17   &lt;2e-16 ***\nx           -0.356525   0.005632  -63.30   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 15249.2  on 10999  degrees of freedom\nResidual deviance:  9497.6  on 10998  degrees of freedom\nAIC: 9501.6\n\nNumber of Fisher Scoring iterations: 4\n\n# OLS regression\nols &lt;- glm(y ~ x, data = simulated_data, family = gaussian(link = \"identity\"))\nsummary(ols)\n\n\nCall:\nglm(formula = y ~ x, family = gaussian(link = \"identity\"), data = simulated_data)\n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  1.131509   0.007640  148.11   &lt;2e-16 ***\nx           -0.105100   0.001126  -93.31   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 0.1395644)\n\n    Null deviance: 2750.0  on 10999  degrees of freedom\nResidual deviance: 1534.9  on 10998  degrees of freedom\nAIC: 9559.1\n\nNumber of Fisher Scoring iterations: 2\n\n# Compare results\nstargazer(ols, logit, probit, type=\"text\")\n\n\n==================================================\n                        Dependent variable:       \n                  --------------------------------\n                                 y                \n                    normal    logistic    probit  \n                     (1)        (2)        (3)    \n--------------------------------------------------\nx                 -0.105***  -0.622***  -0.357*** \n                   (0.001)    (0.011)    (0.006)  \n                                                  \nConstant           1.132***   3.738***   2.144*** \n                   (0.008)    (0.071)    (0.037)  \n                                                  \n--------------------------------------------------\nObservations        11,000     11,000     11,000  \nLog Likelihood    -4,777.565 -4,732.837 -4,748.806\nAkaike Inf. Crit. 9,559.131  9,469.674  9,501.613 \n==================================================\nNote:                  *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01\n\n\n\n11.1.2.1 Synthesizing Model Results\nThe results show that x is a significant predictor of y across all three model estimation strategies, which it should be given the data generating process that created these data. However, the coefficients are interpreted differently in each of the three models. For the OLS model, the coefficient of -.105 reflects a linear estimation of how x is related to y so that for every 1 unit increase in x, regardless of where on the x scale you are, results in a .105, or 10.5%, decrease in the probability of Y. The OLS model is assuming a linear relationship and is creating the coefficient that best describes the linear relationship between x and y.\nThe coefficients on the logistic and probit models do not assume linearity, meaning that the impact of x on values of y will vary depending on where on the x scale you are. With x values closer to the middle of the scale, the relationship will be stronger so that a 1 unit increase in x results in the largest possible change in y With x values closer to either its maximum or minimum, the relationship between x and y will be weaker so that a 1 unit increase in x results in a smaller change in y.\nFor the logistic model, the coefficient values represent the logged odds. This is important to know so that you do not immediately start to interpret the coefficient values like it is an OLS coefficient. We can easily interpret direction of relationship and significance with the logged odds, but to understand the substantive impact, we should convert to predicted probabilities, which we will do below.\nFor the probit model, the coefficient values represents the change in the z-score for each 1-unit increase in x. In same manner as logitistic models, we can easily interpret direction of relationship and significance between x and y in the probit model but not the substantive impact. We will convert to predicted probabilities for that as well.\nSince each of the three models are estimated using GLMs on the exact same data, we can evaluate which of the three estimators are the best with these data. We will can evaluate both the log-likelihood result and the AIC. Remember, for both results, a lower value indicates a better fitting model.\nWith these data, the logistic model seems to fit the best as it has the lowest value on both the AIC and the log-likelihood result. Probit performs worse than logistic model, but better than the OLS results. This is to be expected since the logistic and probit models are designed to worked on a binary DV whereas OLS is designed for a normally distributed DV.\nNext, let’s calculate the predicted probability/value of y at each value of x across the three estimators and compare how well the predictions fit the underlying data generating process.\n\n\n11.1.2.2 Calculating Predicted Probabilities by Model Type\nUsing the predict function, we first create a new data frame with the appropriate x values before saving the predict probabilities (values) from each of the three models in the new file. Then we will append the actual probabilities from the population for each value of x to check the model fit.\n\n#Creates new data frames and saves individual predicted values/probabilities into them\n# For OLS\nnew_data_ols &lt;- expand.grid(x = seq(1, 11, 1))\nols_n&lt;-predict(ols, newdata = new_data_ols,  type = \"response\", se.fit = TRUE, interval = \"confidence\", level = 0.95)\n\n# For logistic\nnew_data_logit &lt;- expand.grid(x = seq(1, 11, 1))\nlogit_n&lt;-predict(logit, newdata = new_data_logit,  type = \"response\", se.fit = TRUE, interval = \"confidence\", level = 0.95)\n\n# For probit\nnew_data_probit &lt;- expand.grid(x = seq(1, 11, 1))\nprobit_n&lt;-predict(probit, newdata = new_data_probit,  type = \"response\", se.fit = TRUE, interval = \"confidence\", level = 0.95)\n\n#Using 'predict' function, we save the predicted values/probabilities at X=seq(1:11, by 1) \nnew_data_combo &lt;- expand.grid(x = seq(1, 11, 1)) #Create new data frame to save results \n\nnew_data_combo$predicted_ols &lt;- ols_n$fit  #Save OLS predicted values \n\nnew_data_combo$predicted_logit &lt;- logit_n$fit #Save logit predicted probabilities\n\nnew_data_combo$predicted_probit &lt;- probit_n$fit  #Save probit predicted probabilities \n\n#Calculates mean predicted values by x for each model then combines \nc&lt;-simulated_data %&gt;% #\n  group_by(x) %&gt;%\n  summarize(mean_y = mean(y))\nnew_data_combo &lt;- merge(new_data_combo, c, by = \"x\")\n\n#Calculates the change in predicted y at each 1 unit increase in x\nlog_delta &lt;- diff(new_data_combo$predicted_logit)\nprob_delta &lt;- diff(new_data_combo$predicted_probit)\nols_delta &lt;- diff(new_data_combo$predicted_ols)\nact_delta &lt;- diff(new_data_combo$mean_y)\n\nprint (new_data_combo)\n\n    x predicted_ols predicted_logit predicted_probit mean_y\n1   1    1.02640909      0.95755602       0.96306010  0.943\n2   2    0.92130909      0.92374027       0.92376081  0.927\n3   3    0.81620909      0.86673258       0.85865742  0.864\n4   4    0.71110909      0.77737938       0.76355408  0.789\n5   5    0.60600909      0.65215976       0.64104623  0.708\n6   6    0.50090909      0.50165826       0.50188764  0.506\n7   7    0.39580909      0.35085567       0.36249659  0.292\n8   8    0.29070909      0.22492490       0.23937374  0.202\n9   9    0.18560909      0.13480722       0.14347336  0.133\n10 10    0.08050909      0.07719952       0.07760481  0.085\n11 11   -0.02459091      0.04298643       0.03771065  0.061\n\n# Create a new data frame with the differences\ndifferences &lt;- data.frame(y = act_delta, log = log_delta, prob = prob_delta, ols=ols_delta)\nprint(differences)\n\n        y         log        prob     ols\n1  -0.016 -0.03381575 -0.03929929 -0.1051\n2  -0.063 -0.05700769 -0.06510339 -0.1051\n3  -0.075 -0.08935319 -0.09510334 -0.1051\n4  -0.081 -0.12521963 -0.12250785 -0.1051\n5  -0.202 -0.15050150 -0.13915858 -0.1051\n6  -0.214 -0.15080259 -0.13939105 -0.1051\n7  -0.090 -0.12593077 -0.12312285 -0.1051\n8  -0.069 -0.09011768 -0.09590038 -0.1051\n9  -0.048 -0.05760770 -0.06586855 -0.1051\n10 -0.024 -0.03421309 -0.03989415 -0.1051\n\n\nThe first table shows the predicted probability - i.e. P(Y = 1 | X) for each of the three estimators and the true DGP that we created. Notice that when X=1 the predicted values for y for the logistic and probit models are close to 1 but not quite exactly 1, whereas the predicted value for the OLS estimator is 1.03. This prediction is out-of-bounds of possibilities, as it is impossible to have a probability &gt;1 like this for our outcome of interest. This illustrates one reason why running linear probability models is not advised here.\nBefore we graph the results, let’s examine the table with the differences in predicted y for the three different models at each 1 unit increase in x. The OLS model, as mentioned, assumes the same change in y of -.106 at each x. The logistic, probit, and, most importantly, the true DGP results reveal a non-linear relationship between x and y whereby the change in y at the extremes is much less steep than compared to the middle of the x scale.\nFor instance, moving from x=1 to x=2 resulted in a predicted probability decrease of .033 in the y estimate. While still larger than the true DGP result of -.023, it is much closer than the OLS estimate of -.106. The probit results showed a similar decrease as the logistic result of -.038. At the middle of the x scale, moving from x=5 to x=6 resulted in a decrease of .153 and .142 for logistic/probit estimators respectively, which is a much steeper estimated relationship than at the extremes. This pattern of results matches the DGP much more closely because both logistic and probit estimators are designed to work with the type of distribution that created the dependent variable (the binomial distribution).\n\n\n11.1.2.3 Graphing the Results\nLet’s finish by visually examining the results saved in the new_data_combo data frame using ggplot2. We’ll compare the predicted values for each estimator by graphing.\n\nggplot(new_data_combo, aes(x = x)) +\n  geom_line(aes(y = predicted_ols), color = \"red\", size = .75, linetype=\"dashed\") +\n  geom_line(aes(y = predicted_logit), color = \"black\", size = .75) +\n  geom_line(aes(y = predicted_probit), color = \"grey\", size = .75) +\n  labs(title = \"Predicted Values by Estimator\") +\n  theme_minimal()\n\n\n\n\nThis plot visually reflects the discussion from above. The OLS estimator is a linear line, whereas the logistic and probit lines are steeper in the middle of the data and flatter at the extremes, just like it should be based on the underlying DGP.\nOne thing to note is that we do not have confidence intervals on the graph at this point. Let’s focus on the logistic result and graph the predicted probabilities with 95% confidence intervals on the plot to help with our inference.\n\n#Using 'predict' function, we save the predicted values/probabilities at X=seq(1:11, by 1) \nnew_data_logit &lt;- expand.grid(x = seq(1, 11, 1))\nlogit_n&lt;-predict(logit, newdata = new_data_logit,  type = \"response\", se.fit = TRUE, interval = \"confidence\", level = 0.95) #Saves predicted probabilities\n\nlogit_n$ci_lower &lt;- logit_n$fit - 1.96 * logit_n$se #Calculates lower bound \nlogit_n$ci_upper &lt;- logit_n$fit + 1.96 * logit_n$se #Calculates upper bound\nlogit_n$x &lt;- 1:11 #Sets the x variable \nlogit_n&lt;-as.data.frame(logit_n) #Creates a data frame for graphing \n\n# Create the plot\nggplot(logit_n, aes(x = x, y = fit)) +\n  geom_line(color = \"black\", linewidth=.5) +                          # Line plot for predicted probabilities\n  geom_ribbon(aes(ymin = ci_lower, ymax = ci_upper), alpha = 0.5, fill = \"grey\") +  # CI ribbon\n  labs(x = \"x\", y = \"Predicted Probability\", title = \"Predicted Probability with 95% CI\") +\n  theme_minimal() + scale_x_continuous(breaks = 1:11) +\n  geom_hline(yintercept = 0.5, linetype = \"dashed\", color = \"red\") \n\n\n\n\nAbove, we graphed the predicted probability of y = 1 at each value of x from our logistic model. The black line represents the average predicted probability of y at that level of x while the light grey bar reflects the 95% confidence interval. Because we have 11,000 cases in our data, the confidence intervals are really tight (close) around the point estimate. This will not be the case with real-world data that typically has much smaller sample sizes.\n\n\n11.1.2.4 Evaluating Model Fit *Note that we will evaluate model fit here on the logistic results but the approach stays identical if you have estimated a probit model.\nUnlike with an OLS estimator, the residuals (predicted y - actual y), do not give us good information with a binary DV. Instead, we create binned residuals plots to understand if we are systematically over or under-estimating y at different levels of our predictor.\nFirst, let’s calculate and graph conventional residuals to understand why they are not informative.\n\n# Fitted values\nfv.logit &lt;- logit$fitted.values  # Gets fitted values at each X\n\ny.logit &lt;- logit$y # The dependent variable (i.e., actual values)\n\n# Residual plot\nplot(fv.logit, y.logit - fv.logit, pch = 19)\nabline(h = 0, lwd = 2, lty = 3)\n\n\n\n\nSince Y is binary, the traditional residuals are not informative, as they are simply the differences between the average predicted value at each x versus the actual value of y at each x. What we really want to know is if we consistently predict y at each level of x.\nFor this, we will use binned residuals plots. Binned residuals divide the data into categories (bins) based on their fitted values then plot the average residual against the average fitted value by bin.\n\n# Create bins based on the range of x\nsimulated_data$predicted_probs&lt;-predict(logit, type=\"response\")\nsimulated_data$residuals &lt;- simulated_data$y - simulated_data$predicted_probs\nsimulated_data$bins &lt;- cut(simulated_data$x, breaks = c(0:12))\n\n# Summarize the residuals for each bin\nbinned_data &lt;- simulated_data %&gt;%\n  group_by(bins) %&gt;%\n  summarize(\n    mean_residual = mean(residuals),\n    ci_lower = mean_residual - 1.96 * sd(residuals) / sqrt(n()),\n    ci_upper = mean_residual + 1.96 * sd(residuals) / sqrt(n())\n  )\n\n#Create a plot to visualize the mean residuals and their CIs\nggplot(binned_data, aes(x = bins, y = mean_residual)) +\n  geom_point(color = \"black\", size = 3) +      # Dot graph with points\n  geom_errorbar(aes(ymin = ci_lower, ymax = ci_upper), width = 0.2, position = position_dodge(width = 0.5)) +  # CIs\n  labs(x = \"Bins of x\", y = \"Avg Binned Residual\", title = \"Binned Residuals with 95% CI\") +\n  theme_minimal() + geom_hline(yintercept=0, linetype=\"dashed\",color=\"red\")\n\n\n\n\nThe residuals plot above shows the bins along with the 95% confidence interval for the average prediction. Generally, we are looking for any systematic pattern in our data to suggest that we have a misspecified model - omitted variable bias, etc. - or a IV that is need of transformation.\nHere, we see some significant misses - at x=5 and x=7 - but in general there is no systematic pattern on our misses. This indicates that our model is generally well performing and we do not need to add additional predictors or transform our IV. We should expect this since we created the data with known parameters.\nNext, we will examine the classification accuracy for the logistic results, but once again the code will work the same on the estimated probit model. Classification accuracy will tell us how much better our predictions are versus simply guessing 0 | 1 depending on whichever one is more prevalent in the data. In our simulated data, y = .495 or simply a coin flip. Guessing 0 would mean we were correct 1-.495 or 50.5% of the time. Hopefully, our model is able to increase our prediction accuracy over simply guessing the average.\nCalculating classification accuracy is fairly straightforward. First, we calculate the null percent correct - i.e. the mean of y - which was .505 for our simulated data. Then we calculate the accuracy of the predicted y to see if we improve on simply guessing the mean. Note, we should since we created the DGP for this set of data.\n\n#Create new vector with 0|1 predictions from the model\npredicted_class &lt;- ifelse(simulated_data$predicted_probs &gt;= 0.5, 1, 0)\n\n# Compare the predicted binary outcomes to the actual y \nactual_class &lt;- simulated_data$y\n\n# Calculate the classification accuracy\naccuracy &lt;- mean(predicted_class == actual_class)\nprint(accuracy)\n\n[1] 0.8149091\n\n# Calculate the classification accuracy improvement\naccuracy_improve &lt;- accuracy-mean(simulated_data$y)\nprint(accuracy_improve)\n\n[1] 0.314\n\n\nLooking at the classification accuracy results, we see a value of .814 which means we correctly predicted the value of y 81.4% of the time. Since the naive guessing approach would have results in us being accurate .505, we also calculate the improvement accuracy, which gives us a value of .323. This value means that adding x to our logistic model predicting y improved our model by 32.2% over guessing the mean. That is an impressive improvement and one that you are not likely to see in real-world data.\n\n\n\n11.1.3 Conclusions\nIn this tutorial, we reviewed analyzing a binary DV. We showed that using a linear probability model results in a worse fitting model and with predictions outside the bounds of reality. This was to be expected since OLS is an estimate for a normally distributed DV and not a binary one.\nWe also reviewed how to estimate logistic and probit models, evaluate the results, translate the coefficients into predicted probabilities, graph predicted probabilities with confidence intervals, and finally how to evaluate the model fit."
  }
]