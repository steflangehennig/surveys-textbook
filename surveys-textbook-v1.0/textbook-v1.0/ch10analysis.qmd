---
title: "Survey Analysis: Logitistic & Probit Regression"
bibliography: references.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE)

library(ggplot2)
library(stargazer)
library(MASS)
library(tidyverse)
set.seed(1964)

```

## Analyzing Dichotomous Dependent Variables

We are going use the binomial distribution to simulate a dichotomous dependent variable, _y_, with a known relationship to an independent variable, _x_. We will then estimate the relationship using OLS, logistic, and probit regressions and compare the results. Along the way, we will talk about the differences in the modeling approaches. 

Let's start with simulating some data for our analysis.

```{r simulate data}
# Create X as a sequence from 1 to 11
X <- 1:11

# Define the probabilities for Y = 1 at each X value with steeper relationship in middle and flatter at edges
probabilities <- c(0.95, .925, .875, 0.8, .7, 0.5, 0.3, 0.2, 0.125, .075, 0.05)

# Simulate data with binomial distribution for Y
sample <- 1000  # Number of trials for each x 

# Create an empty data frame to store the results
simulated_data <- data.frame(X = numeric(),
                             Y = numeric(),
                             stringsAsFactors = FALSE)

# Simulate Y=1 probabilistically for each X value iteration
for (i in 1:sample) {
  # Sample Y=1 probabilistically at each X value
  simulated_Y <- sapply(probabilities, function(p) sample(c(0, 1), size = 1, prob = c(1 - p, p))) #Probabilistically draw 1 | 0 at the specified probability level 
  
  # Create a data frame for the current iteration
  iteration_data <- data.frame(x = X, y = simulated_Y)
  
  # Append the current iteration data to the main data frame
  simulated_data <- rbind(simulated_data, iteration_data)
}

head(simulated_data) #Look at first few cases 

```
### Reviewing Simulated Data

Using the `group_by` function from the `tidyverse` package, we check the probability that Y = 1 at each X level to see if the simulation worked as hoped. The results show that it did with P(Y=1âˆ£X )roughly following the population parameters stipulated earlier.

```{r pressure, echo=FALSE}
simulated_data %>%
  group_by(x) %>%
  summarize(mean_y = mean(y))

```
### Estimating Regression Models

With our simulated data, now we'll estimate three separate regressions changing the Generalized Linear Model (GLM) type using: _Logistic, Probit,_ and _OLS_.

The two models, logistic and probit, are designed to work with dichotomous DVs and should work better with our simulated data compared to the OLS estimator (which assumes a normal distribution). We use GLMs to estimate all three models, including the OLS, to more easily compare model fit across estimators. Both the logitistic and probit models use the `family=binomial` argument in the code but have the different link functions to account for the different calculations. The OLS GLM model uses the _Gaussian_ distribution family with the standard _identity_ link function.

We estimate each of the three models than compare the results using the `stargazer` package. 

```{r modeling}
# Logistic regression
logit <- glm(y ~ x , data = simulated_data, family = binomial(link = "logit"))
summary(logit)

# Probit regression
probit <- glm(y ~ x , data = simulated_data, family = binomial(link = "probit"))
summary(probit)

# OLS regression
ols <- glm(y ~ x, data = simulated_data, family = gaussian(link = "identity"))
summary(ols)

# Compare results
stargazer(ols, logit, probit, type="text")
```
#### Synthesizing Model Results

The results show that _x_ is a significant predictor of _y_ across all three model estimation strategies, which it should be given the data generating process that created these data. However, the coefficients are interpreted differently in each of the three models. For the OLS model, the coefficient of -.105 reflects a linear estimation of how _x_ is related to _y_ so that for every 1 unit increase in _x_, regardless of where on the _x_ scale you are, results in a .105, or 10.5%, decrease in the probability of _Y_. The OLS model is assuming a linear relationship and is creating the coefficient that best describes the linear relationship between _x_ and _y_.

The coefficients on the logistic and probit models do not assume linearity, meaning that the impact of _x_ on values of _y_ will vary depending on where on the _x_ scale you are. With _x_ values closer to the middle of the scale, the relationship will be stronger so that a 1 unit increase in _x_ results in the largest possible change in _y_ With _x_ values closer to either its maximum or minimum, the relationship between _x_ and _y_ will be weaker so that a 1 unit increase in _x_ results in a smaller change in _y_.

For the logistic model, the coefficient values represent the logged odds. This is important to know so that you do not immediately start to interpret the coefficient values like it is an OLS coefficient. We can easily interpret direction of relationship and significance with the logged odds, but to understand the substantive impact, we should convert to predicted probabilities, which we will do below.

For the probit model, the coefficient values represents the change in the z-score for each 1-unit increase in _x_. In same manner as logitistic models, we can easily interpret direction of relationship and significance between _x_ and _y_ in the probit model but not the substantive impact. We will convert to predicted probabilities for that as well.

Since each of the three models are estimated using GLMs on the exact same data, we can evaluate which of the three estimators are the best with these data. We will can evaluate both the log-likelihood result and the AIC. Remember, for both results, a lower value indicates a better fitting model.

With these data, the logistic model seems to fit the best as it has the lowest value on both the AIC and the log-likelihood result. Probit performs worse than logistic model, but better than the OLS results. This is to be expected since the logistic and probit models are designed to worked on a binary DV whereas OLS is designed for a normally distributed DV.

Next, let's calculate the predicted probability/value of _y_ at each value of _x_ across the three estimators and compare how well the predictions fit the underlying data generating process.

#### Calculating Predicted Probabilities by Model Type

Using the `predict` function, we first create a new data frame with the appropriate _x_ values before saving the predict probabilities (values) from each of the three models in the new file. Then we will append the actual probabilities from the population for each value of _x_ to check the model fit.

```{r}
#Creates new data frames and saves individual predicted values/probabilities into them
# For OLS
new_data_ols <- expand.grid(x = seq(1, 11, 1))
ols_n<-predict(ols, newdata = new_data_ols,  type = "response", se.fit = TRUE, interval = "confidence", level = 0.95)

# For logistic
new_data_logit <- expand.grid(x = seq(1, 11, 1))
logit_n<-predict(logit, newdata = new_data_logit,  type = "response", se.fit = TRUE, interval = "confidence", level = 0.95)

# For probit
new_data_probit <- expand.grid(x = seq(1, 11, 1))
probit_n<-predict(probit, newdata = new_data_probit,  type = "response", se.fit = TRUE, interval = "confidence", level = 0.95)

#Using 'predict' function, we save the predicted values/probabilities at X=seq(1:11, by 1) 
new_data_combo <- expand.grid(x = seq(1, 11, 1)) #Create new data frame to save results 

new_data_combo$predicted_ols <- ols_n$fit  #Save OLS predicted values 

new_data_combo$predicted_logit <- logit_n$fit #Save logit predicted probabilities

new_data_combo$predicted_probit <- probit_n$fit  #Save probit predicted probabilities 

#Calculates mean predicted values by x for each model then combines 
c<-simulated_data %>% #
  group_by(x) %>%
  summarize(mean_y = mean(y))
new_data_combo <- merge(new_data_combo, c, by = "x")

#Calculates the change in predicted y at each 1 unit increase in x
log_delta <- diff(new_data_combo$predicted_logit)
prob_delta <- diff(new_data_combo$predicted_probit)
ols_delta <- diff(new_data_combo$predicted_ols)
act_delta <- diff(new_data_combo$mean_y)

print (new_data_combo)

# Create a new data frame with the differences
differences <- data.frame(y = act_delta, log = log_delta, prob = prob_delta, ols=ols_delta)
print(differences)
```


The first table shows the predicted probability - i.e. P(Y = 1 \| X) for each of the three estimators and the true DGP that we created. Notice that when X=1 the predicted values for _y_ for the logistic and probit models are close to 1 but not quite exactly 1, whereas the predicted value for the OLS estimator is 1.03. This prediction is out-of-bounds of possibilities, as it is impossible to have a probability \>1 like this for our outcome of interest. This illustrates one reason why running linear probability models is not advised here. 

Before we graph the results, let's examine the table with the differences in predicted _y_ for the three different models at each 1 unit increase in _x_. The OLS model, as mentioned, assumes the same change in _y_ of -.106 at each _x_. The logistic, probit, and, most importantly, the true DGP results reveal a non-linear relationship between _x_ and _y_ whereby the change in _y_ at the extremes is much less steep than compared to the middle of the _x_ scale.

For instance, moving from x=1 to x=2 resulted in a predicted probability decrease of .033 in the _y_ estimate. While still larger than the true DGP result of -.023, it is much closer than the OLS estimate of -.106. The probit results showed a similar decrease as the logistic result of -.038. At the middle of the _x_ scale, moving from x=5 to x=6 resulted in a decrease of .153 and .142 for logistic/probit estimators respectively, which is a much steeper estimated relationship than at the extremes. This pattern of results matches the DGP much more closely because both logistic and probit estimators are designed to work with the type of distribution that created the dependent variable (the binomial distribution).

#### Graphing the Results

Let's finish by visually examining the results saved in the `new_data_combo` data frame using `ggplot2`. We'll compare the predicted values for each estimator by graphing.  

```{r}

ggplot(new_data_combo, aes(x = x)) +
  geom_line(aes(y = predicted_ols), color = "red", size = .75, linetype="dashed") +
  geom_line(aes(y = predicted_logit), color = "black", size = .75) +
  geom_line(aes(y = predicted_probit), color = "grey", size = .75) +
  labs(title = "Predicted Values by Estimator") +
  theme_minimal()

```

This plot visually reflects the discussion from above. The OLS estimator is a linear line, whereas the logistic and probit lines are steeper in the middle of the data and flatter at the extremes, just like it should be based on the underlying DGP.

One thing to note is that we do not have confidence intervals on the graph at this point. Let's focus on the logistic result and graph the predicted probabilities with 95% confidence intervals on the plot to help with our inference.

```{r}
#Using 'predict' function, we save the predicted values/probabilities at X=seq(1:11, by 1) 
new_data_logit <- expand.grid(x = seq(1, 11, 1))
logit_n<-predict(logit, newdata = new_data_logit,  type = "response", se.fit = TRUE, interval = "confidence", level = 0.95) #Saves predicted probabilities

logit_n$ci_lower <- logit_n$fit - 1.96 * logit_n$se #Calculates lower bound 
logit_n$ci_upper <- logit_n$fit + 1.96 * logit_n$se #Calculates upper bound
logit_n$x <- 1:11 #Sets the x variable 
logit_n<-as.data.frame(logit_n) #Creates a data frame for graphing 

# Create the plot
ggplot(logit_n, aes(x = x, y = fit)) +
  geom_line(color = "black", linewidth=.5) +                          # Line plot for predicted probabilities
  geom_ribbon(aes(ymin = ci_lower, ymax = ci_upper), alpha = 0.5, fill = "grey") +  # CI ribbon
  labs(x = "x", y = "Predicted Probability", title = "Predicted Probability with 95% CI") +
  theme_minimal() + scale_x_continuous(breaks = 1:11) +
  geom_hline(yintercept = 0.5, linetype = "dashed", color = "red") 

```

Above, we graphed the predicted probability of y = 1 at each value of _x_ from our logistic model. The black line represents the average predicted probability of _y_ at that level of _x_ while the light grey bar reflects the 95% confidence interval. Because we have 11,000 cases in our data, the confidence intervals are really tight (close) around the point estimate. This will not be the case with real-world data that typically has much smaller sample sizes.

#### Evaluating Model Fit \*Note that we will evaluate model fit here on the logistic results but the approach stays identical if you have estimated a probit model.

Unlike with an OLS estimator, the residuals (predicted _y_ - actual _y_), do not give us good information with a binary DV. Instead, we create binned residuals plots to understand if we are systematically over or under-estimating _y_ at different levels of our predictor.

First, let's calculate and graph conventional residuals to understand why they are not informative.

```{r traditional residuals}
# Fitted values
fv.logit <- logit$fitted.values  # Gets fitted values at each X

y.logit <- logit$y # The dependent variable (i.e., actual values)

# Residual plot
plot(fv.logit, y.logit - fv.logit, pch = 19)
abline(h = 0, lwd = 2, lty = 3)
```

Since _Y_ is binary, the traditional residuals are not informative, as they are simply the differences between the average predicted value at each _x_ versus the actual value of _y_ at each _x_. What we really want to know is if we consistently predict _y_ at each level of _x_.

For this, we will use binned residuals plots. Binned residuals divide the data into categories (bins) based on their fitted values then plot the average residual against the average fitted value by bin.

```{r binned residuals}

# Create bins based on the range of x
simulated_data$predicted_probs<-predict(logit, type="response")
simulated_data$residuals <- simulated_data$y - simulated_data$predicted_probs
simulated_data$bins <- cut(simulated_data$x, breaks = c(0:12))

# Summarize the residuals for each bin
binned_data <- simulated_data %>%
  group_by(bins) %>%
  summarize(
    mean_residual = mean(residuals),
    ci_lower = mean_residual - 1.96 * sd(residuals) / sqrt(n()),
    ci_upper = mean_residual + 1.96 * sd(residuals) / sqrt(n())
  )

#Create a plot to visualize the mean residuals and their CIs
ggplot(binned_data, aes(x = bins, y = mean_residual)) +
  geom_point(color = "black", size = 3) +      # Dot graph with points
  geom_errorbar(aes(ymin = ci_lower, ymax = ci_upper), width = 0.2, position = position_dodge(width = 0.5)) +  # CIs
  labs(x = "Bins of x", y = "Avg Binned Residual", title = "Binned Residuals with 95% CI") +
  theme_minimal() + geom_hline(yintercept=0, linetype="dashed",color="red")

```





